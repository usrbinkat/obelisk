{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obelisk","text":"<p>Welcome to Obelisk - a powerful Obsidian vault to MkDocs Material Theme converter with integrated AI capabilities.</p> <p>Categories: documentation \ud83d\udcda \u2022 knowledge base \ud83e\udde0 \u2022 AI integration \ud83e\udd16 \u2022 RAG system \ud83d\udd0d</p>"},{"location":"#overview","title":"Overview","text":"<p>Obelisk transforms Obsidian vaults into beautifully rendered static sites using MkDocs with the Material theme. It offers a complete documentation solution with built-in AI-powered search and chat capabilities through Retrieval Augmented Generation (RAG).</p> <p>Complete Documentation Platform</p> <p>Obelisk is more than just a converter - it's a complete documentation platform with AI assistance built in.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Seamless Obsidian Conversion: Preserves Obsidian's rich features in your MkDocs site</li> <li>Integrated RAG Pipeline: Enables AI to answer questions directly from your documentation</li> <li>Material Theme Integration: Beautiful, responsive, and feature-rich documentation</li> <li>AI Chatbot: Connect with Ollama and Open WebUI for context-aware answers</li> <li>Docker Orchestration: One-command deployment of the entire stack</li> <li>Customization Options: Extensive styling and theming capabilities</li> <li>Python-based Workflow: Managed with Poetry for reproducible builds</li> </ul>"},{"location":"#rag-system","title":"RAG System","text":"<p>Obelisk's RAG (Retrieval Augmented Generation) system connects your documentation directly to AI models:</p> <pre><code>graph TD\n    User[User] --&gt; Query[Query Interface]\n    Query --&gt; RAG[RAG Pipeline]\n    RAG --&gt; VectorDB[(Vector Database)]\n    RAG --&gt; Embeddings[Embedding Generation]\n    RAG --&gt; Context[Context Assembly]\n    Context --&gt; Ollama[Ollama LLM]\n    Ollama --&gt; Response[Enhanced Response]\n    Response --&gt; User</code></pre> <p>The RAG pipeline: 1. Processes your documentation into searchable chunks 2. Generates vector embeddings for semantic search 3. Retrieves relevant content based on user queries 4. Feeds this context to an LLM through Ollama 5. Returns accurate, contextual responses based on your content</p>"},{"location":"#quick-start","title":"Quick Start","text":"Using Docker (Recommended)Using Poetry <pre><code># Clone the repository\ngit clone https://github.com/usrbinkat/obelisk.git\ncd obelisk\n\n# Start all services (docs, Ollama, Open WebUI)\ndocker-compose up\n\n# Access documentation at http://localhost:8000\n# Access AI chatbot at http://localhost:8080\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/usrbinkat/obelisk.git\ncd obelisk\n\n# Install dependencies\npoetry install\n\n# Start the documentation server\npoetry run mkdocs serve\n\n# In a separate terminal, set up the RAG system\npoetry run obelisk-rag index\npoetry run obelisk-rag serve --watch\n</code></pre>"},{"location":"#ai-integration","title":"AI Integration","text":"<p>Obelisk provides a complete AI integration stack:</p> <ol> <li>Ollama: Serves optimized LLMs locally (llama3, mistral, phi, etc.)</li> <li>Open WebUI: Provides a user-friendly chat interface</li> <li>RAG System: Connects your documentation content to AI responses</li> </ol> <p>To get started with the AI features:</p> <pre><code># Start the full stack with Docker\ndocker-compose up\n\n# Pull recommended models\ndocker exec -it ollama ollama pull llama3\ndocker exec -it ollama ollama pull mxbai-embed-large\n\n# Index your documentation\ndocker exec -it obelisk obelisk-rag index\n</code></pre>"},{"location":"#customization","title":"Customization","text":"<p>Obelisk supports extensive customization options:</p> <ul> <li>CSS Styling - Custom themes and styles</li> <li>JavaScript Enhancements - Interactive features</li> <li>HTML Templates - Layout customization</li> <li>Python Extensions - Extend functionality</li> <li>Versioning - Documentation versioning</li> </ul>"},{"location":"#development","title":"Development","text":"<p>Obelisk uses modern development tools and practices:</p> <ul> <li>Task Runner: Simplified commands via Taskfile.yaml</li> <li>Docker Support: Containerized development and deployment</li> <li>EditorConfig: Consistent coding styles</li> <li>Poetry: Dependency management and packaging</li> <li>MkDocs: Documentation generation with Material theme</li> </ul> <p>For more information, see the Development Guide.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore our comprehensive documentation: - Chatbot Integration - AI chat setup and configuration - RAG System - Retrieval Augmented Generation details - Customization - Styling and theming options - Development - Developer guides and references</p>"},{"location":"chatbot/","title":"Chatbot Integration","text":"<p>Obelisk includes integration with Ollama and Open WebUI to provide AI-powered chat capabilities directly within your documentation site. This section describes how to set up and use these features.</p>"},{"location":"chatbot/#overview","title":"Overview","text":"<p>The chatbot integration consists of three key components:</p> <ol> <li>Ollama: A lightweight, local AI model server that runs models like Llama2, Mistral, and others.</li> <li>Open WebUI: A web interface for interacting with the AI models served by Ollama.</li> <li>RAG System: A Retrieval Augmented Generation system that enhances responses with content from your documentation.</li> </ol> <p>Together, these services provide a complete AI chat experience that is directly connected to your documentation content, providing accurate, contextually relevant answers.</p>"},{"location":"chatbot/#how-it-works","title":"How It Works","text":"<p>The chatbot integration uses Docker Compose to orchestrate the services:</p> <pre><code>graph TD\n    User[User] --&gt; WebUI[Open WebUI]\n    WebUI --&gt; Ollama[Ollama Model Server]\n    WebUI --&gt; RAG[RAG System]\n    RAG --&gt; Ollama\n    RAG --&gt; VectorDB[(Vector Database)]\n    Ollama --&gt; Models[(AI Models)]\n    WebUI --&gt; Config[(Configuration)]\n    User --&gt; Obelisk[Obelisk Docs]\n    Obelisk --&gt; DocContent[(Documentation Content)]\n    DocContent --&gt; RAG</code></pre> <ol> <li>Users interact with the Open WebUI interface at <code>http://localhost:8080</code></li> <li>Queries can be processed either directly by Ollama or through the RAG system</li> <li>When using RAG, the system retrieves relevant content from your documentation</li> <li>Ollama loads and runs AI models to generate responses enhanced with your content</li> <li>The Obelisk documentation server runs independently at <code>http://localhost:8000</code></li> </ol>"},{"location":"chatbot/#services-configuration","title":"Services Configuration","text":""},{"location":"chatbot/#ollama-service","title":"Ollama Service","text":"<p>The Ollama service runs the model server with GPU acceleration:</p> <pre><code>ollama:\n  container_name: ollama\n  image: ollama/ollama:latest\n  runtime: nvidia\n  environment:\n    - NVIDIA_VISIBLE_DEVICES=all\n    - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n    - CUDA_VISIBLE_DEVICES=0\n    - LOG_LEVEL=debug\n  deploy:\n    resources:\n      reservations:\n        devices:\n          - driver: nvidia\n            capabilities: [gpu]\n            count: all\n  volumes:\n    - ollama:/root/.ollama\n    - models:/models\n  ports:\n    - \"11434:11434\"\n  networks:\n    - ollama-net\n  restart: unless-stopped\n</code></pre>"},{"location":"chatbot/#open-webui-service","title":"Open WebUI Service","text":"<p>The Open WebUI service provides the chat interface:</p> <pre><code>open-webui:\n  container_name: open-webui\n  image: ghcr.io/open-webui/open-webui:main\n  environment:\n    - MODEL_DOWNLOAD_DIR=/models\n    - OLLAMA_API_BASE_URL=http://ollama:11434\n    - OLLAMA_API_URL=http://ollama:11434\n    - LOG_LEVEL=debug\n  volumes:\n    - data:/data\n    - models:/models\n    - open-webui:/config\n  ports:\n    - \"8080:8080\"\n  depends_on:\n    - ollama\n  networks:\n    - ollama-net\n  restart: unless-stopped\n</code></pre>"},{"location":"chatbot/#getting-started","title":"Getting Started","text":"<p>To start using the chatbot integration:</p> <ol> <li>Ensure you have Docker and Docker Compose installed</li> <li>For GPU acceleration, install the NVIDIA Container Toolkit</li> <li>Start the full stack:</li> </ol> <pre><code>task compose\n</code></pre> <ol> <li>Access the chat interface at <code>http://localhost:8080</code></li> <li>Access your documentation at <code>http://localhost:8000</code></li> </ol>"},{"location":"chatbot/#available-models","title":"Available Models","text":"<p>By default, no models are pre-loaded. You can pull models through the Open WebUI interface or directly via Ollama commands:</p> <pre><code># Connect to the Ollama container\ndocker exec -it ollama bash\n\n# Pull a model (example: mistral)\nollama pull mistral\n</code></pre> <p>Popular models to consider:</p> <ul> <li><code>llama2</code> - Meta's Llama 2 model</li> <li><code>mistral</code> - Mistral AI's 7B model</li> <li><code>phi</code> - Microsoft's Phi model</li> <li><code>gemma</code> - Google's Gemma model</li> </ul>"},{"location":"chatbot/#customizing-the-chat-experience","title":"Customizing the Chat Experience","text":"<p>You can customize the chat experience by:</p> <ol> <li>Configuring Open WebUI settings through the interface</li> <li>Creating custom model configurations</li> <li>Using the RAG system to enhance responses with your documentation</li> <li>Customizing the RAG system parameters for better retrieval</li> </ol> <p>See the Open WebUI documentation, Ollama documentation, and our RAG documentation for more details.</p>"},{"location":"chatbot/#rag-system-integration","title":"RAG System Integration","text":"<p>The Retrieval Augmented Generation (RAG) system enhances your chatbot with knowledge from your documentation:</p> <ol> <li> <p>Index your documentation:    <pre><code>obelisk-rag index\n</code></pre></p> </li> <li> <p>Start the RAG API server:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>In Open WebUI, add a new API-based model:</p> </li> <li>Name: \"Obelisk RAG\"</li> <li>Base URL: \"http://localhost:8000\"</li> <li>API Path: \"/query\"</li> <li>Request Format: <code>{\"query\": \"{prompt}\"}</code></li> <li>Response Path: \"response\"</li> </ol> <p>For detailed instructions on setting up and using the RAG system, see the RAG Getting Started Guide and Using RAG.</p>"},{"location":"chatbot/integration/","title":"Integrating AI Chat with Documentation","text":"<p>Learn how to effectively integrate the AI chatbot capabilities with your Obelisk documentation site.</p>"},{"location":"chatbot/integration/#basic-integration","title":"Basic Integration","text":"<p>The Obelisk stack includes Ollama and Open WebUI running alongside your documentation server. Users can access:</p> <ul> <li>Documentation site: <code>http://localhost:8000</code></li> <li>Chat interface: <code>http://localhost:8080</code></li> </ul> <p>This separation allows flexible deployment options while keeping the services connected through a shared Docker network.</p>"},{"location":"chatbot/integration/#advanced-integration-options","title":"Advanced Integration Options","text":""},{"location":"chatbot/integration/#embedding-chat-in-documentation","title":"Embedding Chat in Documentation","text":"<p>To embed the chat interface directly within your documentation pages:</p> <ol> <li>Create a custom HTML template by modifying <code>vault/overrides/main.html</code>:</li> </ol> <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n  {{ super() }}\n\n  &lt;!-- Chat button in corner --&gt;\n  &lt;div class=\"chat-launcher\"&gt;\n    &lt;button class=\"chat-button\" onclick=\"toggleChat()\"&gt;\n      &lt;span class=\"material-icons\"&gt;chat&lt;/span&gt;\n    &lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;!-- Chat iframe container --&gt;\n  &lt;div id=\"chat-container\" class=\"hidden\"&gt;\n    &lt;div class=\"chat-header\"&gt;\n      &lt;span&gt;Obelisk AI Assistant&lt;/span&gt;\n      &lt;button onclick=\"toggleChat()\"&gt;\u00d7&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;iframe id=\"chat-frame\" src=\"http://localhost:8080\"&gt;&lt;/iframe&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n{% block extrahead %}\n  {{ super() }}\n  &lt;style&gt;\n    .chat-launcher {\n      position: fixed;\n      bottom: 20px;\n      right: 20px;\n      z-index: 999;\n    }\n    .chat-button {\n      background: var(--md-primary-fg-color);\n      color: white;\n      border: none;\n      border-radius: 50%;\n      width: 60px;\n      height: 60px;\n      cursor: pointer;\n      box-shadow: 0 2px 10px rgba(0,0,0,0.2);\n    }\n    #chat-container {\n      position: fixed;\n      bottom: 90px;\n      right: 20px;\n      width: 400px;\n      height: 600px;\n      background: white;\n      border-radius: 10px;\n      box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n      z-index: 1000;\n      display: flex;\n      flex-direction: column;\n    }\n    .hidden {\n      display: none !important;\n    }\n    .chat-header {\n      padding: 10px;\n      background: var(--md-primary-fg-color);\n      color: white;\n      border-radius: 10px 10px 0 0;\n      display: flex;\n      justify-content: space-between;\n    }\n    #chat-frame {\n      flex: 1;\n      border: none;\n      border-radius: 0 0 10px 10px;\n    }\n  &lt;/style&gt;\n  &lt;script&gt;\n    function toggleChat() {\n      const container = document.getElementById('chat-container');\n      container.classList.toggle('hidden');\n    }\n  &lt;/script&gt;\n{% endblock %}\n</code></pre> <ol> <li> <p>Add custom CSS in <code>vault/stylesheets/extra.css</code> if needed</p> </li> <li> <p>Update the JavaScript in <code>vault/javascripts/extra.js</code> to handle chat functionality</p> </li> </ol>"},{"location":"chatbot/integration/#training-on-documentation-content","title":"Training on Documentation Content","text":"<p>For more contextual responses about your documentation:</p> <ol> <li>Extract your documentation content:</li> </ol> <pre><code># Create a training data directory\nmkdir -p ~/obelisk-training-data\n\n# Use a script to extract content from markdown files\nfind /workspaces/obelisk/vault -name \"*.md\" -exec sh -c 'cat \"$1\" &gt;&gt; ~/obelisk-training-data/docs.txt' sh {} \\;\n</code></pre> <ol> <li>Create a new Modelfile with your documentation context:</li> </ol> <pre><code>FROM mistral\n\n# Include documentation context\nSYSTEM You are an AI assistant for the Obelisk documentation system. \nSYSTEM You specialize in helping users understand how to use Obelisk to convert their Obsidian vaults to MkDocs Material Theme sites.\nSYSTEM You should give concise, helpful answers based on the official documentation.\n\n# Reference documentation content\nPARAMETER temperature 0.7\nPARAMETER num_ctx 4096\n</code></pre> <ol> <li>Build a custom model:</li> </ol> <pre><code>docker exec -it ollama ollama create obelisk-docs -f /path/to/Modelfile\n</code></pre>"},{"location":"chatbot/integration/#security-considerations","title":"Security Considerations","text":"<p>When integrating AI chat with your documentation:</p> <ol> <li>Access Control:</li> <li>Consider securing the chat interface with authentication</li> <li> <p>Limit network access to the Ollama API</p> </li> <li> <p>Content Filtering:</p> </li> <li>Configure model parameters to avoid harmful outputs</li> <li> <p>Set appropriate system prompts to guide model behavior</p> </li> <li> <p>Privacy:</p> </li> <li>Be aware that conversations may be stored in the <code>data</code> volume</li> <li> <p>Configure data retention policies in Open WebUI</p> </li> <li> <p>Deployment:</p> </li> <li>For public deployments, consider using a reverse proxy</li> <li>Implement rate limiting to prevent abuse</li> </ol>"},{"location":"chatbot/integration/#best-practices","title":"Best Practices","text":"<p>For effective documentation-chat integration:</p> <ol> <li>Clear Distinction: Make it obvious when users are interacting with AI vs. reading documentation</li> <li>Contextual Linking: Have the AI provide links to relevant documentation pages</li> <li>Feedback Loop: Collect user feedback on AI responses to improve over time</li> <li>Fallbacks: Provide easy ways for users to access human help when AI can't solve a problem</li> <li>Monitoring: Track usage patterns to identify documentation gaps</li> </ol>"},{"location":"chatbot/models/","title":"AI Models Configuration","text":"<p>This guide explains how to configure and use AI models with the Ollama and Open WebUI integration in Obelisk.</p>"},{"location":"chatbot/models/#model-management","title":"Model Management","text":""},{"location":"chatbot/models/#pulling-models","title":"Pulling Models","text":"<p>Models can be pulled through the Open WebUI interface or directly using Ollama:</p> <pre><code># Using Ollama CLI\ndocker exec -it ollama ollama pull mistral\n\n# List available models\ndocker exec -it ollama ollama list\n</code></pre>"},{"location":"chatbot/models/#model-storage","title":"Model Storage","text":"<p>Models are stored in persistent Docker volumes:</p> <ul> <li><code>models</code>: Shared volume for model files</li> <li><code>ollama</code>: Ollama-specific configuration and model registry</li> </ul> <p>This ensures your models persist between container restarts.</p>"},{"location":"chatbot/models/#recommended-models","title":"Recommended Models","text":"<p>Here are some recommended models to use with the Obelisk chatbot integration:</p> Model Size Description Command Llama 2 7B Meta's general purpose model <code>ollama pull llama2</code> Mistral 7B High-performance open model <code>ollama pull mistral</code> Phi-2 2.7B Microsoft's compact model <code>ollama pull phi</code> Gemma 7B Google's lightweight model <code>ollama pull gemma:7b</code> CodeLlama 7B Code-specialized model <code>ollama pull codellama</code> <p>For documentation-specific tasks, consider models that excel at knowledge retrieval and explanation.</p>"},{"location":"chatbot/models/#custom-model-configuration","title":"Custom Model Configuration","text":"<p>You can create custom model configurations using Modelfiles:</p> <ol> <li>Create a Modelfile:</li> </ol> <pre><code>FROM mistral\nSYSTEM You are a helpful documentation assistant for the Obelisk project.\n</code></pre> <ol> <li>Build the custom model:</li> </ol> <pre><code>docker exec -it ollama ollama create obelisk-assistant -f Modelfile\n</code></pre> <ol> <li>Use the custom model in Open WebUI.</li> </ol>"},{"location":"chatbot/models/#hardware-requirements","title":"Hardware Requirements","text":"<p>Model performance depends on available hardware:</p> <ul> <li>7B models: Minimum 8GB VRAM (GPU) or 16GB RAM (CPU)</li> <li>13B models: Minimum 16GB VRAM (GPU) or 32GB RAM (CPU)</li> <li>70B models: Minimum 80GB VRAM (GPU) or distributed setup</li> </ul> <p>For optimal performance, use GPU acceleration with the NVIDIA Container Toolkit.</p>"},{"location":"chatbot/models/#quantization-options","title":"Quantization Options","text":"<p>Ollama supports various quantization levels to balance performance and resource usage:</p> Quantization Quality Memory Usage Example F16 Highest Highest <code>ollama pull mistral:latest</code> Q8_0 High Medium <code>ollama pull mistral:8b-q8_0</code> Q4_K_M Medium Low <code>ollama pull mistral:8b-q4_k_m</code> Q4_0 Lowest Lowest <code>ollama pull mistral:8b-q4_0</code> <p>Choose quantization based on your hardware capabilities and quality requirements.</p>"},{"location":"chatbot/models/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Out of memory errors:</li> <li>Try a smaller model or higher quantization level</li> <li> <p>Reduce context length in Open WebUI settings</p> </li> <li> <p>Slow responses:</p> </li> <li>Ensure GPU acceleration is properly configured</li> <li> <p>Check for other processes using GPU resources</p> </li> <li> <p>Model not found:</p> </li> <li>Verify the model was pulled correctly</li> <li>Check network connectivity to model repositories</li> </ol> <p>For more troubleshooting, consult the Ollama documentation.</p>"},{"location":"chatbot/openwebui/","title":"Open WebUI Configuration","text":"<p>Open WebUI provides a powerful interface for interacting with AI models through Ollama. This guide explains how to configure and customize it for use with Obelisk.</p>"},{"location":"chatbot/openwebui/#basic-configuration","title":"Basic Configuration","text":"<p>Open WebUI is configured through environment variables in the <code>docker-compose.yaml</code> file:</p> <pre><code>open-webui:\n  environment:\n    - MODEL_DOWNLOAD_DIR=/models\n    - OLLAMA_API_BASE_URL=http://ollama:11434\n    - OLLAMA_API_URL=http://ollama:11434\n    - LOG_LEVEL=debug\n</code></pre> <p>These settings establish connection to the Ollama service and configure basic behavior.</p>"},{"location":"chatbot/openwebui/#user-interface-features","title":"User Interface Features","text":"<p>Open WebUI provides several key features:</p>"},{"location":"chatbot/openwebui/#chat-interface","title":"Chat Interface","text":"<p>The main chat interface allows:</p> <ul> <li>Conversational interactions with AI models</li> <li>Code highlighting and formatting</li> <li>File attachment and reference</li> <li>Conversation history and management</li> </ul>"},{"location":"chatbot/openwebui/#model-selection","title":"Model Selection","text":"<p>Users can select from available models with options for:</p> <ul> <li>Parameter adjustment (temperature, top_p, etc.)</li> <li>Context length configuration</li> <li>Model-specific presets</li> </ul>"},{"location":"chatbot/openwebui/#prompt-templates","title":"Prompt Templates","text":"<p>Create and manage prompt templates to:</p> <ul> <li>Define consistent AI behavior</li> <li>Create specialized assistants for different tasks</li> <li>Share templates with your team</li> </ul>"},{"location":"chatbot/openwebui/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"chatbot/openwebui/#custom-branding","title":"Custom Branding","text":"<p>To customize the Open WebUI appearance for your Obelisk deployment:</p> <ol> <li>Mount a custom assets volume:</li> </ol> <pre><code>open-webui:\n  volumes:\n    - ./custom-webui-assets:/app/public/custom\n</code></pre> <ol> <li>Create the following files:</li> <li><code>custom-webui-assets/logo.png</code> - Main logo</li> <li><code>custom-webui-assets/logo-dark.png</code> - Logo for dark mode</li> <li><code>custom-webui-assets/favicon.png</code> - Browser tab icon</li> <li><code>custom-webui-assets/background.png</code> - Login page background</li> </ol>"},{"location":"chatbot/openwebui/#authentication","title":"Authentication","text":"<p>Enable authentication for multi-user setups:</p> <pre><code>open-webui:\n  environment:\n    - ENABLE_USER_AUTH=true\n    - DEFAULT_USER_EMAIL=admin@example.com\n    - DEFAULT_USER_PASSWORD=strongpassword\n</code></pre>"},{"location":"chatbot/openwebui/#api-integration","title":"API Integration","text":"<p>Open WebUI can be integrated with other services via its API:</p> <pre><code>open-webui:\n  environment:\n    - ENABLE_API=true\n    - API_KEY=your-secure-api-key\n</code></pre> <p>This allows programmatic access to model interactions.</p>"},{"location":"chatbot/openwebui/#persistent-data","title":"Persistent Data","text":"<p>Open WebUI stores its data in Docker volumes:</p> <ul> <li><code>data</code>: Conversations, user settings, and app data</li> <li><code>open-webui</code>: Configuration files</li> <li><code>models</code>: Shared with Ollama for model storage</li> </ul> <p>These volumes persist across container restarts and updates.</p>"},{"location":"chatbot/openwebui/#customizing-for-documentation-support","title":"Customizing for Documentation Support","text":"<p>To optimize Open WebUI for documentation support:</p> <ol> <li>Create a specialized preset:</li> <li>Navigate to Settings &gt; Presets</li> <li>Create a new preset named \"Documentation Helper\"</li> <li>Configure with appropriate temperature (0.3-0.5) and parameters</li> <li> <p>Set system prompt to documentation-specific instructions</p> </li> <li> <p>Create documentation-focused prompt templates:</p> </li> <li>\"Explain this concept\"</li> <li>\"How do I configure X\"</li> <li> <p>\"Troubleshoot this error\"</p> </li> <li> <p>Enable RAG (Retrieval Augmented Generation):</p> </li> <li>Upload documentation files through the interface</li> <li>Enable \"Knowledge Base\" feature</li> <li>Configure vector storage settings</li> </ol>"},{"location":"chatbot/openwebui/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Connection errors:</li> <li>Verify network settings in docker-compose</li> <li> <p>Check that Ollama service is running</p> </li> <li> <p>Authentication problems:</p> </li> <li>Reset password using the API</li> <li> <p>Check environment variables for auth settings</p> </li> <li> <p>Performance issues:</p> </li> <li>Adjust interface settings for slower devices</li> <li>Configure page size and context window appropriately</li> </ol>"},{"location":"chatbot/openwebui/#resources","title":"Resources","text":"<ul> <li>Open WebUI GitHub Repository</li> <li>Open WebUI Documentation</li> <li>Ollama Documentation</li> </ul>"},{"location":"chatbot/rag/","title":"Retrieval Augmented Generation (RAG)","text":"<p>This section outlines the RAG pipeline for Obelisk, which enables the chatbot to retrieve and reference content directly from your documentation.</p>"},{"location":"chatbot/rag/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant information retrieved from a knowledge base before generating responses. This approach combines the strengths of:</p> <ol> <li>Retrieval systems - Finding relevant content from your knowledge base</li> <li>Generation capabilities - Using that content to produce accurate, contextual responses</li> </ol> <p>For Obelisk, this means the AI chatbot will be able to answer questions based specifically on your documentation content.</p>"},{"location":"chatbot/rag/#rag-architecture","title":"RAG Architecture","text":"<p>The Obelisk RAG pipeline will consist of several key components:</p> <pre><code>graph TD\n    A[Documentation Files] --&gt;|Extraction| B[Content Processor]\n    B --&gt;|Chunking| C[Text Chunks]\n    C --&gt;|Embedding Generation| D[Vector Database]\n    E[User Query] --&gt;|Query Processing| F[Query Embeddings]\n    D --&gt;|Similarity Search| G[Relevant Chunks]\n    F --&gt;|Search| G\n    G --&gt;|Context Assembly| H[Prompt Assembly]\n    H --&gt;|LLM Input| I[Ollama Model]\n    I --&gt;|Response| J[Enhanced Answer]</code></pre> <ol> <li>Content Processing: Extract content from Markdown files in your Obsidian vault</li> <li>Chunking: Split content into appropriate segments for embedding</li> <li>Embedding Generation: Convert text chunks into vector embeddings</li> <li>Vector Storage: Store embeddings in a vector database for efficient retrieval</li> <li>Query Processing: Process and embed user queries</li> <li>Retrieval: Find the most relevant document chunks</li> <li>Context Assembly: Combine retrieved content into a prompt</li> <li>Response Generation: Generate accurate responses based on retrieved content</li> </ol>"},{"location":"chatbot/rag/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>The RAG pipeline will be implemented in phases:</p> Phase Feature Status 1 Document Processing Pipeline Completed \u2713 2 Vector Database Integration Completed \u2713 3 Query Processing &amp; Retrieval Completed \u2713 4 Integration with Ollama Completed \u2713 5 Web UI Extensions Planned"},{"location":"chatbot/rag/#document-processing","title":"Document Processing","text":""},{"location":"chatbot/rag/#content-extraction","title":"Content Extraction","text":"<p>The RAG pipeline will extract content from your Obsidian vault:</p> <pre><code># Implemented in obelisk/rag/document.py\nclass DocumentProcessor:\n    \"\"\"Process markdown documents for the RAG system.\"\"\"\n\n    def process_directory(self, directory: str = None) -&gt; List[Document]:\n        \"\"\"Process all markdown files in a directory.\"\"\"\n        if directory is None:\n            directory = self.config.get(\"vault_dir\")\n\n        all_chunks = []\n        for md_file in glob.glob(f\"{directory}/**/*.md\", recursive=True):\n            chunks = self.process_file(md_file)\n            all_chunks.extend(chunks)\n\n        return all_chunks\n\n    def process_file(self, file_path: str) -&gt; List[Document]:\n        \"\"\"Process a single markdown file.\"\"\"\n        # Read and process the file\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # Create a Document object with metadata\n        doc = Document(page_content=content, metadata={\"source\": file_path})\n\n        # Extract YAML frontmatter as metadata\n        self._extract_metadata(doc)\n\n        # Split into chunks with overlap\n        chunks = self.text_splitter.split_documents([doc])\n\n        return chunks\n</code></pre>"},{"location":"chatbot/rag/#chunking-strategies","title":"Chunking Strategies","text":"<p>Documents will be split into chunks using various strategies:</p> <ul> <li>Fixed-size chunks: Split by character or token count</li> <li>Semantic chunks: Split by headings or semantic boundaries</li> <li>Overlapping chunks: Create overlapping segments to maintain context</li> </ul>"},{"location":"chatbot/rag/#metadata-enhancement","title":"Metadata Enhancement","text":"<p>Each chunk will be enhanced with metadata:</p> <ul> <li>Source document and location</li> <li>Heading hierarchy</li> <li>Last updated date</li> <li>Tags and categories</li> <li>Related documents</li> </ul>"},{"location":"chatbot/rag/architecture-draft-review/","title":"Retrieval-Augmented Generation (RAG) Architecture for a Local-First Developer Setup","text":""},{"location":"chatbot/rag/architecture-draft-review/#overview-and-key-requirements","title":"Overview and Key Requirements","text":"<p>This design outlines a local-first RAG system for developers that ingest Markdown documents and enables querying via local LLMs. The architecture emphasizes offline operation, dynamic updates from a Git-tracked <code>./vault</code> directory, and compatibility with developer hardware (NVIDIA GPUs and Apple M-series). Key requirements include:</p> <ul> <li>Dynamic Document Ingestion: Automatically detect and process new or updated Markdown files in a vault (Git repository) into the RAG knowledge base.</li> <li>Local Embeddings &amp; Storage: Generate embeddings on-premise (no cloud API) using state-of-the-art open models like <code>mxbai-embed-large</code>, and store vectors in a local vector database.</li> <li>Efficient Retrieval Pipeline: Use a LangChain-based pipeline to retrieve relevant content for a query with modern retrieval techniques (e.g. similarity search, re-ranking) and feed it to a local LLM.</li> <li>Local LLM Integration: Query resolution via local large language models (e.g. LLaMA 3, Phi-4, DeepSeek-R1) served through Ollama, ensuring all inference is offline.</li> <li>Developer-Friendly Tools: Favor easy-to-deploy, open-source components (Milvus Lite, Chroma, or Postgres/pgvector) with minimal overhead and good performance on a single machine.</li> <li>OpenWebUI Compatibility: Optionally integrate with OpenWebUI\u2019s interface for a seamless chat experience, leveraging its knowledge base features for RAG.</li> </ul> <p>By meeting these requirements, the system will function as a self-contained \u201cpersonal ChatGPT\u201d that stays updated with local documentation and runs entirely on a laptop. Below we detail the architecture components and design decisions.</p>"},{"location":"chatbot/rag/architecture-draft-review/#document-ingestion-and-indexing-pipeline","title":"Document Ingestion and Indexing Pipeline","text":"<p>Ingestion pipeline refers to how documents are loaded, parsed, and indexed into the vector store. We use LangChain\u2019s document loaders and text splitters, augmented with a filesystem watcher, to continuously ingest Markdown files from the <code>./vault</code> directory. This ensures that as soon as documentation is added or changed, the vector index is updated.</p> <ul> <li>Markdown Document Loading: LangChain provides Markdown loaders (or generic text loaders) to read <code>.md</code> files. Each file is loaded and its content prepared for splitting. Metadata (such as filename, path, or commit ID) is attached to each document so that the origin of retrieved text is known. This metadata will later help format answers with source references.</li> <li>Dynamic Watcher for Updates: LangChain\u2019s core does not natively monitor filesystem changes in real-time. Instead, we integrate a Python filesystem watcher (e.g. the <code>watchdog</code> library) to track additions, deletions, or modifications in the vault. On detecting a change, the pipeline will load the new/updated file and update the vector store (how to monitoring the new files after directory loader class used \u00b7 Issue #5252 \u00b7 langchain-ai/langchain \u00b7 GitHub). This approach is recommended by LangChain contributors for auto-ingestion of new files (how to monitoring the new files after directory loader class used \u00b7 Issue #5252 \u00b7 langchain-ai/langchain \u00b7 GitHub). Alternatively, a scheduled batch job or Git hook could trigger re-ingestion for new commits.</li> <li>Text Splitting: Each Markdown document is split into smaller chunks (e.g. 500-1000 tokens) using LangChain\u2019s text splitters (such as <code>RecursiveCharacterTextSplitter</code>). Splitting by section or headings is ideal so that each chunk is a semantically coherent unit (e.g. a paragraph, bullet list, or code block). This yields better retrieval granularity and ensures the LLM can be given just the relevant portions of a document. </li> <li>Indexing &amp; Embedding Ingestion: For each chunk, an embedding vector is computed and upserted into the vector database (with the chunk\u2019s text and metadata). If a file was modified or deleted, the pipeline will delete or update the corresponding vectors to keep the index in sync. Using unique IDs (like stable document IDs + chunk index) for each chunk makes it possible to replace or remove them when the source changes.</li> </ul> <p>By maintaining this ingestion pipeline, the \u201cknowledge base\u201d is always up to date with the Markdown content in the Git vault. The pipeline is lightweight enough to run in the background on a developer laptop, thanks to efficient local embedding models and an embedded vector store.</p>"},{"location":"chatbot/rag/architecture-draft-review/#embedding-generation-local-models-and-alternatives","title":"Embedding Generation: Local Models and Alternatives","text":"<p>To convert text into high-dimensional vectors for similarity search, the system uses a local embedding model. We prioritize <code>mxbai-embed-large-v1</code> (by Mixedbread AI) or a similar state-of-the-art open model due to their strong semantic performance. The embedding model choice critically affects downstream retrieval quality, so we consider the latest options in 2024\u20132025:</p> <ul> <li><code>mxbai-embed-large-v1</code>: A 335M parameter English embedding model known for state-of-the-art results among efficiently-sized models (mxbai-embed-large-v1 - Mixedbread). It outperforms OpenAI\u2019s text-embedding-ada-002 in accuracy (mxbai-embed-large-v1 - Mixedbread) and was trained on 700M pairs + 30M triplets with a specialized contrastive loss. It produces 1024-dimensional embeddings and is versatile across domains (tested via MTEB benchmark) (mxbai-embed-large-v1 - Mixedbread). This model is a strong default for our RAG pipeline given its high quality and moderate size.</li> <li>Context Length &amp; Speed: <code>mxbai-embed-large</code> can handle reasonably long inputs (512 tokens recommended (mxbai-embed-large-v1 - Mixedbread)). For very large Markdown files, we may chunk before embedding to avoid truncation. Its 335M size is feasible to run on CPU (with MKL/BLAS optimizations) or modest GPU, but for large volumes of text, batching and possibly 8-bit quantization can be used to speed up embedding generation.</li> <li>Alternative Embedding Models: By 2025, new open-source embedding models have emerged that might offer better performance:</li> <li>Stella (400M &amp; 1.5B): An open model by Dun Zhang that tops MTEB\u2019s retrieval leaderboard (commercial-use allowed). Notably, the 1.5B version provides only marginal gains over the 400M version (The Best Embedding Models for Information Retrieval in 2025 | DataStax), making the smaller Stella-400M very attractive for local use. Stella is praised for its excellent out-of-the-box accuracy (The Best Embedding Models for Information Retrieval in 2025 | DataStax).</li> <li>BGE (BAAI\u2019s model): Models like bge-large or bge-m3 are multi-lingual, multi-domain embedding models (567M params) that perform well in semantic tasks. These appear in Ollama\u2019s model list and offer versatility (e.g. BGE-M3 is noted for multi-granularity embeddings) (Embedding models \u00b7 Ollama Search).</li> <li>Snowflake\u2019s Arctic Embeds: A suite of models (22M\u2013335M and a newer 568M v2) optimized by Snowflake for high performance (Embedding models \u00b7 Ollama Search) (Embedding models \u00b7 Ollama Search). These include multilingual support in v2 (Embedding models \u00b7 Ollama Search).</li> <li>Modern BERT Embed: A 2025-era model from Answer AI/LightOn that aimed to improve BERT-based embeddings. However, evaluations showed it underperformed expectations relative to others (The Best Embedding Models for Information Retrieval in 2025 | DataStax).</li> <li>Nomic\u2019s <code>nomic-embed-text</code>: A high-performing model with a large token window, popular via Ollama (Embedding models \u00b7 Ollama Search). It is efficient (21M pulls indicating popularity) and may allow embedding longer documents in one shot.</li> </ul> <p>In practice, mxbai-embed-large remains a top choice for English text due to its proven balance of accuracy and size. We keep the architecture flexible so the embedding model can be swapped if a new model (e.g. \u201cVoyage-3-embed\u201d or other future release) surpasses it. The embedding generation is done through a local inference pipeline. We can run the model using the Hugging Face Transformers pipeline in Python or via Ollama if the model is packaged there (Ollama supports pulling these embeddings models directly (Embedding models \u00b7 Ollama Search)). For example, one could run <code>ollama pull mxbai-embed-large</code> and have an embedding server locally. </p> <p>Table: Comparison of Candidate Embedding Models (2024\u20132025)</p> Model Dimensions Size (params) Notable Features License mxbai-embed-large 1024 335M Top-tier accuracy, English-only, supports binary embeddings for compression (mxbai-embed-large-v1 - Mixedbread) Apache-2.0 (?) Stella-400M 1024 400M MTEB retrieval leader (open-source), almost on par with 1.5B model ([The Best Embedding Models for Information Retrieval in 2025 DataStax](https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,to%20pay%20for%20more%20throughput)) BGE-m3 (BAAI) 768 or 1024 567M Multi-lingual &amp; multi-domain; good zero-shot performance Apache-2.0 Snowflake Arctic v2 768 568M Multilingual, enterprise-optimized, various sizes available Apache-2.0 ModernBERT Large 1024 ~1B Next-gen BERT-based model by LightOn (underwhelming in 2025 tests ([The Best Embedding Models for Information Retrieval in 2025 DataStax](https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=Zhang%20released%20a%20whitepaper%20in,to%20pay%20for%20more%20throughput))) nomic-embed-text 768 - (transformer) High performance, large context window, very popular for local setups (Embedding models \u00b7 Ollama Search) MIT <p>Note: All above models are offline and can be run locally. We ensure whichever model is used is loaded at startup and kept in memory for throughput. The embedding step is typically fast (&lt;100ms per chunk on GPU for these model sizes, or a bit slower on CPU). If using Apple M-series, models can run via coreml or 4-bit quantization for speed, albeit with slight accuracy trade-offs.</p> <p>After embedding, each text chunk (with its metadata) and vector goes into the vector database for fast similarity search, described next.</p>"},{"location":"chatbot/rag/architecture-draft-review/#vector-database-local-deployment-options","title":"Vector Database: Local Deployment Options","text":"<p>A vector database stores embeddings and supports similarity search (k-NN) efficiently. For a local-first system, the DB must run on a laptop without heavy resource needs or cloud services. We evaluate three popular options \u2013 Milvus (Lite), Chroma, and Postgres (pgvector) \u2013 focusing on their capabilities, limitations, and ease of use for a developer:</p>"},{"location":"chatbot/rag/architecture-draft-review/#milvus-with-milvus-lite","title":"Milvus (with Milvus Lite)","text":"<p>Milvus is an open-source, purpose-built vector database designed for high-performance at scale (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). It supports a wide range of indexing algorithms (IVF, HNSW, DiskANN, etc.) and can handle billion-scale vectors with low latency on distributed clusters (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). For our local setup, we leverage Milvus Lite, a lightweight in-process version introduced in Milvus 2.4.x that is tailored for laptops and small data sizes (milvus-lite \u00b7 PyPI) (milvus-lite \u00b7 PyPI).</p> <ul> <li>Capabilities: Milvus (full) offers extensive index choices and tuning: e.g., HNSW for high recall, IVF for memory/disk trade-offs, GPU-accelerated indexes, and even hybrid search combining vector and scalar filters (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). It ensures high recall and performance even as data scales to millions or more vectors. Milvus supports metadata filtering and hybrid queries (vector + keyword filtering) which can be useful to narrow results by file tags or sections (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog).</li> <li>Milvus Lite for Local Use: Milvus Lite runs as an embedded library (via <code>pymilvus</code> &gt;=2.4.2) without needing a separate server. This drastically lowers the setup complexity \u2013 the vector store is just a local file (e.g. <code>milvus_demo.db</code>) used via the same Milvus API (milvus-lite \u00b7 PyPI) (milvus-lite \u00b7 PyPI). It supports &lt;1 million vectors comfortably in-memory on a single machine (milvus-lite \u00b7 PyPI), aligning with the scale of a personal vault. Importantly, Milvus Lite supports Apple Silicon (M1/M2) and Linux ARM out-of-the-box (milvus-lite \u00b7 PyPI), ensuring compatibility with both MacBooks and typical Linux dev machines.</li> <li>Resource Requirements: A Milvus Lite instance embedded in a Python process will use only the resources needed for its indexes. This could be a few hundred MBs of RAM for, say, 100k vectors with HNSW (exact usage depends on vector dimension and index settings). It has no background daemon or extra services \u2013 unlike full Milvus which requires etcd or Pulsar \u2013 making it lightweight. Milvus is optimized in C++ and can use multiple CPU cores; with GPU, the full Milvus can offload ANN search computations to CUDA (Milvus Lite might not include GPU support, but one can always run a separate Milvus server with GPU if needed).</li> <li>Limitations: The flexibility comes with complexity. While basic use is straightforward, fine-tuning indexes or dealing with schema changes in Milvus may have a learning curve for newcomers (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA). However, for our moderate scale, we can likely stick with default HNSW index (high recall) and not worry about advanced tuning. Another consideration is that Milvus is focused purely on vector similarity; semantic filtering or re-ranking beyond k-NN would have to be handled at the application layer (e.g., by an LLM or cross-encoder re-ranker), whereas a system like Weaviate has built-in semantic search modules (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA) (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA). In our case, the LLM will handle final answer synthesis, so this is acceptable.</li> </ul> <p>Overall, Milvus Lite provides production-grade retrieval performance locally, with the same API as full Milvus if we ever scale up (milvus-lite \u00b7 PyPI). This makes it a strong choice if our vector count grows or we require the fastest possible searches.</p>"},{"location":"chatbot/rag/architecture-draft-review/#chroma","title":"Chroma","text":"<p>Chroma DB is a lightweight embedded vector store aimed at simplicity and developer-friendliness (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). It can be installed via pip (<code>chromadb</code>), and runs within the Python process (using DuckDB or SQLite under the hood for persistence). Chroma prioritizes ease of use \u2013 minimal configuration, an intuitive API, and tight integration with LangChain out-of-the-box.</p> <ul> <li>Capabilities: Chroma provides HNSW-based ANN search (Cosine or Euclidean distance) with optional embeddings persistency on disk. It supports metadata filtering on queries, and recently introduced an HTTP server mode if needed. For a typical use (&lt;1M embeddings), Chroma\u2019s single-node design is sufficient (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). It is very easy to integrate: LangChain\u2019s <code>Chroma</code> vector store can be initialized with a collection name and will handle storage in a local SQL database transparently.</li> <li>Resource Use: As a pure-Python library with a C++ index (Faiss or HNSW in-memory index), Chroma is lightweight. It keeps the entire index in memory, so memory usage scales with number of vectors (~ a few hundred bytes per vector plus metadata overhead for HNSW). The practical limit is around 1 million vectors on a single machine before performance or memory becomes a concern (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). This aligns with Milvus Lite\u2019s sweet spot as well. Chroma does not spawn extra processes; it will create a DuckDB (or SQLite) file on disk to store the vectors and metadata for persistence.</li> <li>Ease of Use: This is Chroma\u2019s strong suit \u2013 no setup required, just <code>pip install chromadb</code>. It\u2019s a good default for local prototypes. Chroma\u2019s Pythonic API and LangChain integration mean a developer can go from raw text to a queryable index in a few lines of code. This makes it very developer-friendly for a local app.</li> <li>Limitations: The simplicity comes at the cost of advanced features. Chroma currently uses a single index type (HNSW) and is limited to a single node (no sharding or replication) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). There\u2019s no role-based access or multi-user security built in (not needed for a single-user dev app) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). While it supports basic filtering and CRUD on vectors, it lacks the rich configuration of Milvus. If our dataset remains moderately sized and our focus is quick development, these limitations are acceptable. Chroma may also have slower performance than Milvus on very large datasets or high concurrent query loads, but in a local scenario queries are single-user and sporadic.</li> </ul> <p>In summary, Chroma offers the simplest path: it\u2019s essentially plug-and-play for a Python application and would work well as long as our vector count is in the thousands or low hundreds of thousands (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog).</p>"},{"location":"chatbot/rag/architecture-draft-review/#postgres-pgvector","title":"Postgres + pgvector","text":"<p>PostgreSQL with the <code>pgvector</code> extension introduces vector search capability into the familiar Postgres database. This option is attractive if we want to reuse a single database for both relational and vector data, or if we prefer the robustness of PostgreSQL for persistence.</p> <ul> <li>Capabilities: pgvector allows storing vector embeddings as a column type and provides indexed approximate search (via IVF indexes) or exact search using built-in indexing. It benefits from all of Postgres\u2019s features \u2013 transactions, durability, scalability via replication \u2013 and can combine vector queries with SQL filters easily. This means we can do hybrid queries (e.g., find similar vectors among documents where <code>category = 'API'</code>) with standard SQL syntax.</li> <li>Ease of Deployment: If a developer already has Postgres running (or is comfortable with Docker), adding pgvector is straightforward (<code>CREATE EXTENSION pgvector</code>). Many developers find this easier than running a separate specialized DB. For a local setup, running Postgres might be heavier than Chroma, but not by much if Postgres is already used for other tasks. There are also lightweight Postgres variants (like Timescale or TDB) that integrate pgvector.</li> <li>Performance and Scale: For moderate-sized data, pgvector performs well (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA). It is known to handle tens of thousands to low millions of vectors fine, especially if using approximate indexes. However, compared to Milvus or Chroma, Postgres may struggle as vector count grows very large or if queries have to scan a lot of data. The trade-off is speed vs familiarity: Milvus is optimized in C++ for vector math, whereas Postgres has overhead from being a general database. In high-throughput or million-scale scenarios, pgvector will be slower and use more memory (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA). For our use (likely a few thousand Q&amp;A pairs or document chunks), pgvector search (with an IVF index) would likely be sub-100ms, which is acceptable.</li> <li>Limitations: Postgres lacks out-of-the-box support for advanced ANN algorithms beyond IVF (which it added recently) and flat scan. It also doesn\u2019t have built-in clustering or sharding for vectors specifically (beyond what Postgres provides generally). So while you can scale it, you might lose some of the performance benefits of a purpose-built DB at large scale (Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA). Additionally, running Postgres just for vectors is arguably more complex than using Chroma which was built for exactly that purpose. If not already needed, introducing a SQL database might add operational overhead (managing a service, tuning Postgres memory, etc.). On the flip side, it reuses known components \u2013 backup, indexing, etc. \u2013 which can be a plus for maintainability. </li> </ul> <p>In our scenario, pgvector is a viable option if we desire strong persistence guarantees and if we might integrate the data with other structured data. But if we purely need a vector store for unstructured docs, dedicated solutions (Milvus/Chroma) may be more efficient.</p>"},{"location":"chatbot/rag/architecture-draft-review/#comparison-of-vector-db-options","title":"Comparison of Vector DB Options","text":"<p>The table below summarizes the three options in terms of key considerations for a local developer setup:</p> Criteria Milvus Lite (Milvus 2.4) Chroma DB Postgres pgvector Setup &amp; Integration Embedded in Python via <code>pymilvus</code> (no separate server when using Lite) (milvus-lite \u00b7 PyPI). LangChain integration available (<code>Milvus.from_documents</code> etc.) ([Milvus \ufe0f LangChain](https://python.langchain.com/v0.1/docs/integrations/vectorstores/milvus/#:~:text=vector_db%20%3D%20Milvus,19530)). Pure Python library (<code>pip install chromadb</code>). LangChain integration native. Easiest setup (no external service). Performance High-performance ANN search, supports billions of vectors on full server (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). Lite mode good up to ~1M vectors (milvus-lite \u00b7 PyPI) with high recall. Many index types (HNSW, IVF, etc.) for tuning (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). Can use GPU. Good performance for up to ~1M vectors (HNSW) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). In-memory index gives low latency for moderate data. Beyond 1M, may hit single-node limits. Decent performance for tens of thousands of vectors; can use IVF index for faster search. May be ~20-30% slower than Milvus for large datasets (Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA). Suitable for moderate scale, but not designed for very high QPS vector-only workloads. Features Rich features: dynamic index selection, advanced filtering, hybrid search (vector + scalar) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog), time-travel consistency, etc. Mature ecosystem and documentation (Vector database : pgvector vs milvus vs weaviate.   : r/LocalLLaMA). Simplicity-focused feature set: HNSW index only (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog), metadata filtering, persistence to disk. Lacks user authentication or sharding (single-node only) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). Leverages SQL features: can do complex filtering/join with vector search in one query. Strong persistence (ACID transactions). Fewer ANN algorithms (flat or IVF). No built-in vector-specific clustering \u2013 relies on Postgres scaling. Resource Needs Lite: runs in-process, memory depends on data (roughly a few hundred bytes per vector plus index). Very low idle footprint. Full: requires etcd and more RAM/CPU (not ideal for laptop). Lite supports ARM (Apple M) natively (milvus-lite \u00b7 PyPI). Very low overhead. Runs in-process, uses DuckDB/SQLite file. Memory usage proportional to vector count (HNSW graph). CPU usage efficient for moderate data; multi-threaded search. Postgres server will use a constant memory overhead (e.g. 100MB+ even if few data) and CPU even when idle. Optimizing Postgres (shared buffers, etc.) may be needed. On Apple M-series, Postgres runs natively via Homebrew. Developer Experience Medium \u2013 some learning curve to understand Milvus concepts and API. Once set up, same code scales from Lite to cluster. Good if future growth expected (milvus-lite \u00b7 PyPI). Easiest \u2013 designed for developers. Minimal configuration, intuitive API. Great for quick iteration and local debugging. High \u2013 Familiar SQL interface. Can use standard Postgres tools (PSQL, DBeaver) to inspect data. But adds SQL overhead for those not used to it. Combines vector search with normal DB operations if needed. <p>Decision: Given the fully offline, single-user context and anticipated data size, Milvus Lite and Chroma are the leading candidates. Chroma offers zero-friction setup and is likely sufficient if the vault has, say, a few thousand markdown chunks. Milvus Lite offers more headroom and advanced indexing if we push towards hundreds of thousands of vectors or want to experiment with index trade-offs. Both integrate with LangChain easily. If we value simplicity, Chroma might be chosen initially, with an option to migrate to Milvus Lite if needed. </p> <p>We will proceed assuming one of these is used (the architecture doesn\u2019t change significantly between them). For completeness, if a team already uses Postgres in their stack, pgvector could be slotted in, but in a local-first scenario it\u2019s an extra component unless already running.</p>"},{"location":"chatbot/rag/architecture-draft-review/#langchain-rag-pipeline-design","title":"LangChain RAG Pipeline Design","text":"<p>With documents indexed into the vector store, the query-time pipeline handles incoming questions by retrieving relevant context and generating answers. We leverage LangChain\u2019s RAG components to construct this pipeline in a modular way. The pipeline consists of:</p> <ol> <li>User Query Intake \u2013 A question from the user (via CLI, API, or OpenWebUI chat) is received.</li> <li>Retriever \u2013 The query is embedded and similar documents are fetched from the vector store.</li> <li>(optional) Reranking or Filtering \u2013 The retrieved chunks may be filtered for relevance or diversity (to avoid redundant answers). This can involve dropping low similarity results or using Maximal Marginal Relevance (MMR) to ensure a variety of topics in the top results.</li> <li>LLM Answer Generation \u2013 The question and retrieved context are fed into a local LLM (via Ollama) using a prompt template that encourages using the provided info to answer.</li> <li>Response Formatting \u2013 The LLM\u2019s output is returned to the user. In a chat UI, this would appear as the assistant\u2019s answer, potentially with citations or code blocks as provided by the model.</li> </ol> <p>Let\u2019s detail some of these stages and the LangChain design patterns used:</p>"},{"location":"chatbot/rag/architecture-draft-review/#retrieval-step-embedding-query-and-similarity-search","title":"Retrieval Step: Embedding Query and Similarity Search","text":"<p>When a user asks a question, we embed the query text using the same embedding model used for documents. This yields a query vector in the same vector space as our document embeddings. We then use the vector database\u2019s k-NN search to find the top k most similar chunks (by cosine similarity or Euclidean distance).</p> <ul> <li>Using LangChain, we encapsulate this in a Retriever object. For instance, if using Chroma, <code>Chroma.as_retriever(search_type=\"mmr\", k=5)</code> could be used to get a retriever with MMR, or for Milvus we might use a <code>VectorStoreRetriever</code> with <code>search_k</code> and <code>fetch_k</code> parameters tuned.</li> <li>We typically retrieve a handful of chunks (e.g., 3\u20135) for the LLM to consider. The exact number can be tuned based on the average size of chunks and the LLM\u2019s context limit. With a modern local LLM (which often have 4K or more tokens context), including ~4 chunks of a few hundred tokens each is reasonable.</li> </ul> <p>Design considerations for retrieval:</p> <ul> <li>Chunk Metadata and Filtering: Because each chunk has metadata (source file, section, etc.), we can apply filters. For example, if the query includes a tag like \u201c#API\u201d, we could filter to only search documents tagged as API-related. LangChain\u2019s retriever interface supports metadata filters (which underlying stores like Milvus or Chroma apply).</li> <li>Diversity (MMR): Instead of plain top-k similarity, using Maximal Marginal Relevance can improve the diversity of retrieved contexts. This prevents getting multiple very similar chunks that all say the same thing. LangChain\u2019s retrievers support MMR, which we can enable for broad queries that might span multiple documents.</li> <li>Cross-Encoder Re-ranking: For maximum accuracy, one could re-rank the top 10\u201320 retrieved chunks using a cross-encoder (a BERT-based model that scores query\u2013chunk relevance) before picking the final few to pass to the LLM. This is a technique to improve precision at the cost of extra computation. Given we aim for fully local solution, an efficient cross-encoder model like MiniLM or E5 could be used if needed. However, as an initial design, we might skip this and rely on the embedding model\u2019s semantic search quality (especially if using a strong model like mxbai or Stella which are quite accurate).</li> </ul>"},{"location":"chatbot/rag/architecture-draft-review/#prompt-construction-and-llm-query","title":"Prompt Construction and LLM Query","text":"<p>Once we have the relevant text chunks, we construct the prompt for the LLM. A common pattern is the \u201cStuffing\u201d approach: concatenate the retrieved chunks and the question into a single prompt that asks the LLM to answer using the provided context. For example:</p> <pre><code>You are an assistant answering questions about the project\u2019s documentation. \nUse the following context to answer the question. If the context does not have the answer, say you don't know.\n\nContext:\n&lt;&lt;DOC 1 TITLE&gt;&gt;:\nDOC 1 CONTENT...\n\n&lt;&lt;DOC 2 TITLE&gt;&gt;:\nDOC 2 CONTENT...\n\nQuestion: &lt;&lt;user question&gt;&gt;\nAnswer:\n</code></pre> <p>The exact prompt can be tuned, but we ensure to include citations markers or section titles so that the LLM can refer to them. Some RAG systems wrap each document in special tokens or XML (OpenWebUI uses <code>&lt;context&gt;&lt;/context&gt;</code> tags around knowledge (How I\u2019ve Optimized Document Interactions with Open WebUI and RAG: A Comprehensive Guide | by Kelvin Campelo | Medium)) \u2013 this can help the model distinguish context from question.</p> <p>We also can include a system message (if using a chat-model format) that instructs the model on how to behave (e.g., be concise, cite sources). LangChain\u2019s <code>RetrievalQA</code> chain or <code>ConversationalRetrievalChain</code> automates some of this prompt assembly. By 2025, LangChain offers robust support for custom prompt templates in retrieval-augmented QA. We will likely use a custom prompt to ensure the format fits our use (especially if we want markdown output with citations).</p> <p>After forming the prompt, we invoke the local LLM through Ollama. This is done via a LangChain LLM wrapper. For example, using the <code>langchain_ollama</code> integration, we initialize a <code>ChatOllama</code> model instance (Build a Local RAG Application | \ufe0f LangChain) pointing to our desired model (like <code>llama3.1:8b</code> or <code>deepseek-r1:Q4_0</code> if available). This wrapper will handle sending the prompt to the Ollama backend (which serves models on <code>localhost:11434</code> by default) (Build a Local RAG Application | \ufe0f LangChain) and receiving the generated answer.</p> <p>Local LLM considerations:</p> <ul> <li>Model Choices: We plan to support multiple local models. For general queries, Meta\u2019s LLaMA 3 (the hypothetical next-gen LLaMA, here presumably version 3.1) is a strong foundation model. For specialized needs, Phi-4 by Microsoft (14B parameters) excels at complex reasoning and math (Introducing Phi-4: Microsoft\u2019s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub), and DeepSeek-R1 (which is an open model focusing on reasoning/code, comparable to OpenAI\u2019s top models (deepseek-ai/DeepSeek-R1 \u00b7 Hugging Face)). Each of these can be obtained as GGUF or similar format and served via Ollama. We can allow the user to choose the model per query or per session in the UI (OpenWebUI makes it easy to switch models mid-chat).</li> <li>Prompt Tuning per Model: Different models may require different prompt formats. For instance, LLaMA-based models often use a system prompt like <code>&lt;s&gt;[INST] ...</code> unless using a chat wrapper that handles formatting. The LangChain <code>ChatModel</code> abstraction can hide these differences. We will test the prompt on each target model to ensure the formatting is correct. (LangChain provides prompt templates for known families \u2013 e.g., it notes inclusion of special tokens might be needed (Build a Local RAG Application | \ufe0f LangChain), but the <code>ChatOllama</code> wrapper likely normalizes this).</li> <li>Context Window: We ensure the total tokens (documents + question + instructions) fit in the model\u2019s context. Many 2025 models have 4k to 16k token contexts. If using a 8k context model, and our docs are large, we may truncate or retrieve fewer chunks to stay within limits. Alternatively, some RAG setups implement iterative answering: e.g., use a Refine Chain where the LLM reads one chunk at a time and builds an answer incrementally (RAG LangChain app using the 3-pipeline design | Decoding ML). This is useful if context window is small. However, given modern models and our moderate chunk sizes, a single-shot prompt with top 3\u20135 chunks should suffice (\u201cStuff\u201d method).</li> <li>Computation Performance: Running a 7B-14B parameter model on a laptop is feasible. On an M2 MacBook, a 7B model (4-bit quantized) can generate ~10 tokens/sec. For a typical answer (~100 tokens) this is under 15 seconds, which is acceptable. Larger models like 30B may be slower (~2-4 tokens/sec), so we might stick to 7B\u201313B models for responsiveness. If a GPU (like an NVIDIA 3080) is available, FP16 inference can be much faster, making even 30B models viable. The architecture allows swapping the model depending on hardware \u2013 e.g., on a desktop with a 24GB GPU, one could run a 34B DeepSeek-R1 for higher quality. On a Macbook with no GPU, one might use LLaMA 3 7B or Phi-4 14B in 4-bit mode.</li> </ul> <p>LangChain\u2019s chain abstraction will tie it together. For example, using a <code>RetrievalQA</code> chain, we set <code>retriever</code> to our vector store retriever and <code>llm</code> to our ChatOllama model. The chain then handles the overall logic: embed query, retrieve docs, format prompt, get LLM answer, and even include source references if our prompt instructs it to.</p>"},{"location":"chatbot/rag/architecture-draft-review/#end-to-end-query-flow","title":"End-to-End Query Flow","text":"<p>Putting it all together, the end-to-end flow for a query is:</p> <ol> <li>User (e.g. via OpenWebUI chat or a CLI) asks: \u201cHow do I set up the API client in this project?\u201d</li> <li>RAG Pipeline:</li> <li>The query is embedded by (for example) mxbai-embed-large, yielding a 1024-d vector.</li> <li>Vector DB (Milvus/Chroma) is queried (k=5, MMR reordering) and returns, say, 3 relevant text chunks from <code>API_Guide.md</code> and <code>Quickstart.md</code>.</li> <li>The pipeline prepares a prompt:      <pre><code>[SYSTEM] You are a helpful assistant for the project docs.\n[CONTEXT] \nAPI_Guide.md:\n\"... snippet about API client initialization...\"\n\nQuickstart.md:\n\"... snippet showing an example usage of the API client...\"\n\n[USER] How do I set up the API client in this project?\n[ASSISTANT]\n</code></pre></li> <li>The local LLM (Phi-4, for instance) receives this prompt and generates an answer, e.g. explaining the steps to set up the client, possibly quoting code from the context. It might produce an answer with citations like \u201c(see API_Guide.md)\u201d.</li> <li>User receives answer. The answer is accurate and based on local docs, with no external calls made.</li> </ol> <p>Throughout this, LangChain\u2019s framework provides observability and modularity. We can log the retrieved docs, model\u2019s output, etc., to a file for debugging (LangChain callbacks or tracing can be used, though we\u2019d use them offline, e.g., LangSmith locally if needed).</p> <p>If the user asks a follow-up question in a chat, we can reuse the chain. If we want to support conversational memory (so the assistant remembers previous Q&amp;A), LangChain\u2019s <code>ConversationalRetrievalChain</code> can maintain chat history context. This essentially treats prior conversation as additional context (or uses a summary of it). OpenWebUI by default shows the entire conversation to the model, so another strategy is to let the UI handle the history and our pipeline just augment the latest question. In practice, for multi-turn Q&amp;A about documents, a common approach is to combine the last user question with a summary of relevant past info to retrieve again. This is an area for extension, but not core to the initial design.</p>"},{"location":"chatbot/rag/architecture-draft-review/#integration-with-openwebui","title":"Integration with OpenWebUI","text":"<p>OpenWebUI is a popular self-hosted chat interface that supports multiple LLM backends (including Ollama) and features like knowledge base RAG integration (Open WebUI: A Powerful, Open Source Interface for LLM \u2013 Both.org). Integrating our RAG system with OpenWebUI can provide a rich UI for the user without custom frontend coding. There are two ways to integrate:</p> <ol> <li>Use OpenWebUI\u2019s Built-in RAG (Knowledge Bases): OpenWebUI allows creating \u201cKnowledge Bases\u201d by uploading documents (it has a UI for adding Markdown/PDFs, etc.) ( Open WebUI RAG Tutorial | Open WebUI) ( Open WebUI RAG Tutorial | Open WebUI). Internally, OpenWebUI will embed these (it likely uses HuggingFace sentence transformers or similar, possibly ones we specify) and store them (possibly in a local SQLite or DuckDB with FAISS index). A custom model can then be created in OpenWebUI that attaches this knowledge base ( Open WebUI RAG Tutorial | Open WebUI). When the user asks a question to that model, OpenWebUI will handle retrieval and will pass the context to the model automatically. Essentially, it implements a RAG pipeline under the hood. This approach means we could bypass our LangChain pipeline for query time and let OpenWebUI do it. However, its ingestion might not be dynamic (you\u2019d have to re-upload or trigger a re-index manually when docs change), and we have less control over embedding models or retrieval settings.</li> <li>Custom Pipeline Integration: We can instead connect our LangChain pipeline to OpenWebUI by treating it as a tool or API. For example, OpenWebUI supports an OpenAI-compatible REST API mode and even pipelines. We could run a local FastAPI server that exposes an endpoint: when a request comes in (with the user query), our server uses the LangChain RAG chain to produce an answer (with citations) and returns it. Then configure OpenWebUI to use that as a \u201cmodel\u201d (it would treat it like an OpenAI chat completion API). In essence, OpenWebUI becomes just the frontend, and our LangChain pipeline is the backend answering. This gives us full control over retrieval settings, embedding model choice, etc., at the cost of a bit more setup (running a separate API server).</li> </ol> <p>Given that OpenWebUI now has official RAG support with knowledge bases (and tutorials for it ( Open WebUI RAG Tutorial | Open WebUI) ( Open WebUI RAG Tutorial | Open WebUI)), we might leverage it for simplicity. We could periodically push our vault documents into OpenWebUI\u2019s knowledge base via its CLI or API (perhaps there\u2019s an API to add docs, or we use the UI occasionally to sync). But for a seamless \u201chot reload\u201d of docs, a custom integration might be superior.</p> <p>OpenWebUI\u2019s advantage is the user experience: it supports chat history, Markdown rendering (important for code snippets in answers), and multi-model selection easily (Open WebUI: A Powerful, Open Source Interface for LLM \u2013 Both.org) (Dave does AI #1 - Self-hosted AI using Ollama + Open WebUI). It\u2019s also offline and lightweight (web app running on localhost). Since our focus is on architecture, we ensure that the core RAG system is decoupled \u2013 it can function via command line or tests without OpenWebUI. The UI integration is an added layer. </p> <p>Plan for integration: Start with our LangChain pipeline accessible through a simple interface (could be a CLI or small Flask app). Then, if using OpenWebUI, create a custom \u201ctool\u201d or model in OpenWebUI that queries this interface. For instance, define a pseudo-model in OpenWebUI that on each user message calls our API to get a response (this might be achieved through OpenWebUI\u2019s \u201cOpenAPI Tool Servers\u201d feature ( Open WebUI RAG Tutorial | Open WebUI) or a pipeline script). Another approach is to use OpenWebUI\u2019s new Pipelines functionality ( Open WebUI RAG Tutorial | Open WebUI), where you might be able to insert a custom Python function in the generation pipeline. If possible, we could insert a hook that intercepts the user message, runs our retrieval, and prepends the context to the prompt, then continues to the LLM. Community discussions (e.g., Reddit posts about RAG in OpenWebUI) indicate users have successfully connected custom knowledge sources (Focused Retrieval on Knowledge Documents : r/OpenWebUI - Reddit) (How I've Optimized Document Interactions with Open WebUI and RAG).</p> <p>In summary, the integration will allow a user to open a browser to OpenWebUI, select (for example) a model called \u201cLocalDocs-GPT\u201d, and chat with it. The answers will come from the LangChain RAG backend, but to the user it feels like a normal chat with an AI assistant that \u201cknows\u201d their documents. This meets the goal of a user-friendly, offline QA system for the developer\u2019s documentation.</p>"},{"location":"chatbot/rag/architecture-draft-review/#performance-and-resource-considerations","title":"Performance and Resource Considerations","text":"<p>Designing for developer hardware means we must be mindful of CPU/GPU usage, memory, and responsiveness:</p> <ul> <li>Embedding Throughput: Using a large embedding model (300M+ params) to index potentially thousands of chunks can be time-consuming. We mitigate this by doing initial ingestion in batch (which is one-time and can be done when the vault is first indexed). For ongoing updates, the volume is presumably low (commits of a few docs at a time). We can further speed up embedding by using batch inference (embedding multiple texts at once if the model and library support it) or by switching to a slightly smaller model if necessary (e.g. the all-MiniLM models are very fast but at some accuracy cost). In practice, mxbai-embed-large is reported to be quite efficient for its size, and on an M1 chip it might take ~50-100 milliseconds per chunk. This is acceptable for &lt;=10k chunks (a few minutes total). If the vault is extremely large (say hundreds of MBs of text), one might do initial embedding on a more powerful machine or overnight. But since the use-case is developer notes/docs, we anticipate manageable data sizes.</li> <li>Vector DB Memory: We will configure the vector store index to balance memory and speed. HNSW (used by both Milvus and Chroma by default) has a controllable memory footprint (via M and ef parameters). We might use a slightly lower M (graph connectivity) if memory is tight, at the cost of some recall drop. That said, for a few thousand vectors, the memory use is trivial. For 100k vectors, HNSW might use a few hundred MB of RAM. Milvus Lite storing data to disk (<code>milvus_demo.db</code> file) ensures persistence without consuming RAM for all vectors at once (it likely memory-maps data as needed). Chroma uses DuckDB which will spill to disk as well. So memory should not be a limiting factor. On a 16GB RAM laptop, dedicating even 1-2GB to the vector index is fine.</li> <li>LLM Model Memory/VRAM: Running a 7B parameter model in 4-bit quantization uses about ~4GB of RAM (or VRAM). A 13-14B model uses ~8GB in 4-bit. This is within reach for an 8GB VRAM GPU or 16GB system RAM (with swap possibly for Mac). For better performance on Mac, using the GPU via Metal acceleration is possible with smaller models (CoreML versions of LLaMA 2 exist; by 2025 possibly LLaMA 3 too). If using an NVIDIA GPU, we can load the model in VRAM fully for faster inference. The design allows configuring the model size/precision to fit the machine. We will provide instructions for using 4-bit quantized models via Ollama for those on CPU-only systems. Ollama itself handles model loading and can use Apple\u2019s neural engine for acceleration where possible. </li> <li>Multi-threading and Concurrency: Since this is a single-user setup, we don\u2019t expect concurrent queries. If multiple queries did happen, both the vector DB and LLM could become bottlenecks. Milvus/Chroma can handle concurrent searches well on multiple threads, but a single LLM on one machine can usually do one response at a time (unless running multiple model instances). If in future one wanted a multi-user setup, they might need to run separate processes or threads for the LLM calls (and ensure the hardware can handle it). Our focus is single-user, which simplifies things.</li> <li> <p>Latency: The steps in the pipeline (embedding the query, vector search, LLM generation) all add some latency. Embedding the query is fast (~20ms). Vector search in a small index is ~10ms. The LLM generation is the dominant cost (a few seconds). End-to-end latency is thus mostly the LLM\u2019s doing. We aim to keep that reasonable by choosing models and context sizes appropriately. If a user asks extremely long questions or if the retrieved context is very large, generation will be slower (more tokens to process). We will document best practices: e.g., if a user asks a extremely detailed multi-part question, it might be better to break it down. But generally, expect answers within 5\u201315 seconds on typical hardware, which is acceptable for an interactive assistant.</p> </li> <li> <p>Accuracy vs Model Size: If the smaller local models sometimes falter in reasoning or correctness, one could swap in a bigger model (like DeepSeek-R1 distilled 32B) for complex questions. The modular design (using Ollama and LangChain) makes this a configuration choice rather than an architectural one. It\u2019s worth noting that open models like DeepSeek-R1 and Phi-4 are closing the gap with larger proprietary models (DeepSeek-R1: Best Open-Source Reasoning LLM Outperforms ...) (Introducing Phi-4: Microsoft\u2019s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub), so we anticipate high-quality answers, especially since the model can directly look up the truth from documentation (which mitigates knowledge gaps and hallucinations).</p> </li> </ul>"},{"location":"chatbot/rag/architecture-draft-review/#implementation-references","title":"Implementation References","text":"<p>To ensure this architecture is implementable with today\u2019s tools, refer to these resources and examples: - LangChain Local RAG Tutorial (2024): Demonstrates running LLaMA 3 locally with Ollama and a local embedding model (Build a Local RAG Application | \ufe0f LangChain) (Build a Local RAG Application | \ufe0f LangChain). It provides a blueprint for setting up the Ollama backend and retrieving from a vector store. - Milvus Lite RAG Example: Milvus documentation and examples show how to use Milvus Lite within a Python app for RAG (milvus-lite \u00b7 PyPI). A specific example is Milvus\u2019s bootcamp demo \u201cbuild_RAG_with_milvus.ipynb\u201d on GitHub, integrating with LangChain. - OpenWebUI RAG Tutorial: The community-contributed tutorial on OpenWebUI\u2019s site ( Open WebUI RAG Tutorial | Open WebUI) ( Open WebUI RAG Tutorial | Open WebUI) is a step-by-step guide to load a set of Markdown files as a knowledge base and query them in the UI. This can be used as a starting point to configure our system in OpenWebUI if we choose that route. - Ollama Documentation: For installing and managing models with Ollama \u2013 e.g., how to quantize models, the command to pull specific versions, etc. (Ollama\u2019s model catalog shows available models like mxbai-embed-large and others (Embedding models \u00b7 Ollama Search)). - Mixedbread\u2019s Embedding Blog: Mixedbread\u2019s blog post \u201cOpen Source Strikes Bread \u2013 New Fluffy Embedding Model\u201d (referenced on their site (mxbai-embed-large-v1 - Mixedbread)) likely details the performance of mxbai-embed-large and how to best use it (e.g., prompting techniques for embeddings, as hinted by the use of an optional prompt for domain (mxbai-embed-large-v1 - Mixedbread)). - Microsoft Phi-4 Technical Report: Provides insight into the capabilities of Phi-4 14B, which might guide how to leverage its strengths (especially if the docs have math or require step-by-step reasoning) (Introducing Phi-4: Microsoft\u2019s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub) (Introducing Phi-4: Microsoft\u2019s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub). - DeepSeek-R1 Paper: Describes the reasoning prowess of DeepSeek-R1 (deepseek-ai/DeepSeek-R1 \u00b7 Hugging Face). It also mentions distilled variants based on Llama and Qwen (some of which might be easier to run locally). These references help justify using these models and can guide fine-tuning or prompting if we ever refine the LLM on our domain.</p> <p>By following this design and utilizing the mentioned tools, a developer can implement a robust RAG system that runs entirely locally, scales to their needs, and provides quick, accurate answers from their Markdown knowledge base. This empowers an \u201coffline ChatGPT\u201d experience tailored to one\u2019s own documentation \u2013 preserving privacy and leveraging the latest open-source AI advances (circa 2025).</p>"},{"location":"chatbot/rag/architecture-draft-review/#conclusion","title":"Conclusion","text":"<p>The proposed architecture combines modern embedding models, a high-performance local vector store, and powerful local LLMs to achieve a self-contained RAG setup. We prioritized components that are community-maintained and cutting-edge in 2024\u20132025, ensuring the system remains relevant and high-quality: - Dynamic ingestion keeps the knowledge updated from a Git vault in real-time. - State-of-the-art embeddings (mxbai-embed-large or its successors) ensure the retrieval step is semantically accurate (mxbai-embed-large-v1 - Mixedbread). - Milvus Lite/Chroma offer fast similarity search on device, with Milvus providing scalability if needed (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog) (Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog). - Local LLMs via Ollama provide the brains for answering questions, with options like LLaMA 3, Phi-4, and DeepSeek-R1 pushing the envelope of what\u2019s possible without any cloud services (Introducing Phi-4: Microsoft\u2019s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub) (deepseek-ai/DeepSeek-R1 \u00b7 Hugging Face). - Integration with OpenWebUI delivers a polished user interface, showing that local AI assistants can be both powerful and user-friendly (Open WebUI: A Powerful, Open Source Interface for LLM \u2013 Both.org).</p> <p>In essence, this design enables developers to harness their private documentation with AI assistance entirely offline. It aligns with the growing trend of privacy-preserving, local AI deployments and leverages community best practices (LangChain patterns, open models, vector DB benchmarks) to ensure it\u2019s both practical and cutting-edge. With this system in place, a developer could query \u201cHow do I deploy our app on Kubernetes?\u201d and get an immediate, accurate answer sourced from their own docs \u2013 all without an internet connection. Such capability underscores the potential of retrieval-augmented generation when thoughtfully applied in a local-first context. </p>"},{"location":"chatbot/rag/architecture-draft/","title":"RAG System Design Document: Markdown-Based Knowledge Retrieval","text":""},{"location":"chatbot/rag/architecture-draft/#executive-summary","title":"Executive Summary","text":"<p>This document outlines a comprehensive design for a retrieval-augmented generation (RAG) system that processes markdown documents from a monitored directory, creates vector embeddings, stores them in a Milvus database, and leverages LangChain to create a pipeline that connects with Ollama-hosted LLMs for intelligent querying and response generation. The design emphasizes: - Real-time document processing with change detection - State-of-the-art embedding models for optimal semantic understanding - Horizontally scalable vector storage with Milvus - Modular pipeline architecture for extensibility - Comprehensive evaluation metrics for continuous improvement</p>"},{"location":"chatbot/rag/architecture-draft/#1-system-architecture-overview","title":"1. System Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              \u2502    \u2502              \u2502    \u2502              \u2502    \u2502              \u2502\n\u2502   Document   \u2502    \u2502  Embedding   \u2502    \u2502    Vector    \u2502    \u2502   Response   \u2502\n\u2502  Processing  \u2502\u2500\u2500\u2500\u25b6\u2502  Generation  \u2502\u2500\u2500\u2500\u25b6\u2502    Store     \u2502\u2500\u2500\u2500\u25b6\u2502  Generation  \u2502\n\u2502              \u2502    \u2502              \u2502    \u2502              \u2502    \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                                       \u25b2                    \u2502\n       \u2502                                       \u2502                    \u2502\n       \u2502                                       \u2502                    \u2502\n       \u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502                    \u2502\n       \u2502                 \u2502              \u2502      \u2502                    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Metadata   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n                         \u2502    Store     \u2502                           \u2502\n                         \u2502              \u2502                           \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n                                                                    \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n                         \u2502              \u2502                           \u2502\n                         \u2502     User     \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502  Interface   \u2502\n                         \u2502              \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"chatbot/rag/architecture-draft/#2-component-design","title":"2. Component Design","text":""},{"location":"chatbot/rag/architecture-draft/#21-document-processing-pipeline","title":"2.1 Document Processing Pipeline","text":""},{"location":"chatbot/rag/architecture-draft/#directory-watcher-service","title":"Directory Watcher Service","text":"<p><pre><code>from watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nimport os\nclass MarkdownWatcher(FileSystemEventHandler):\n    def init(self, processor):\n        self.processor = processor\n    def on_created(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.process_file(event.src_path)\n    def on_modified(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.process_file(event.src_path)\n    def on_deleted(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.delete_file(event.src_path)\nclass DirectoryWatcherService:\n    def init(self, directory_path, processor):\n        self.observer = Observer()\n        self.directory_path = directory_path\n        self.event_handler = MarkdownWatcher(processor)\n    def start(self):\n        self.observer.schedule(self.event_handler, self.directory_path, recursive=True)\n        self.observer.start()\n    def stop(self):\n        self.observer.stop()\n        self.observer.join()\n</code></pre> The directory watcher service uses the Watchdog library to monitor the ./vault directory for changes to markdown files. When changes are detected, the appropriate processing function is called.</p>"},{"location":"chatbot/rag/architecture-draft/#document-processor","title":"Document Processor","text":"<p><pre><code>from langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\nimport frontmatter\nimport hashlib\nclass DocumentProcessor:\n    def init(self, embedding_service, vectorstore_service, metadata_store):\n        self.embedding_service = embedding_service\n        self.vectorstore_service = vectorstore_service\n        self.metadata_store = metadata_store\n        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=512,\n            chunk_overlap=50,\n            separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n        )\n    def process_file(self, file_path):\n        # Generate document hash for tracking changes\n        file_hash = self._hash_file(file_path)\n        # Check if document has changed\n        existing_hash = self.metadata_store.get_document_hash(file_path)\n        if existing_hash == file_hash:\n            print(f\"No changes detected for {file_path}\")\n            return\n        # Extract content and metadata\n        with open(file_path, 'r') as f:\n            content = f.read()\n        try:\n            post = frontmatter.loads(content)\n            metadata = post.metadata\n            content_text = post.content\n        except:\n            metadata = {}\n            content_text = content\n        # Add file path and hash to metadata\n        metadata['source'] = file_path\n        metadata['file_hash'] = file_hash\n        # Split the document\n        docs = self.text_splitter.create_documents([content_text], [metadata])\n        # Delete old vectors if they exist\n        if existing_hash:\n            self.vectorstore_service.delete_document(file_path)\n        # Generate embeddings and store\n        self.embedding_service.embed_documents(docs)\n        self.vectorstore_service.add_documents(docs)\n        # Update metadata store\n        self.metadata_store.update_document_metadata(file_path, metadata, file_hash)\n    def delete_file(self, file_path):\n        self.vectorstore_service.delete_document(file_path)\n        self.metadata_store.delete_document(file_path)\n    def process_all_files(self, directory_path):\n        for root, , files in os.walk(directorypath):\n            for file in files:\n                if file.endswith('.md'):\n                    file_path = os.path.join(root, file)\n                    self.process_file(file_path)\n    def hashfile(self, file_path):\n        with open(file_path, 'rb') as f:\n            return hashlib.md5(f.read()).hexdigest()\n</code></pre> The Document Processor handles loading, parsing, and processing markdown files. It uses frontmatter for parsing YAML frontmatter in markdown files, and implements a hashing mechanism to track changes and avoid redundant processing.</p>"},{"location":"chatbot/rag/architecture-draft/#22-embedding-generation","title":"2.2 Embedding Generation","text":"<p><pre><code>from langchain_ollama import OllamaEmbeddings\nimport numpy as np\nclass EmbeddingService:\n    def init(self, model_name=\"mxbai-embed-large\", batch_size=32):\n        self.embeddings_model = OllamaEmbeddings(\n            model=model_name,\n            base_url=\"http://localhost:11434\"\n        )\n        self.batch_size = batch_size\n    def embed_documents(self, documents):\n        \"\"\"Generate embeddings for a list of documents\"\"\"\n        texts = [doc.page_content for doc in documents]\n        # Process in batches to avoid memory issues\n        all_embeddings = []\n        for i in range(0, len(texts), self.batch_size):\n            batch_texts = texts[i:i+self.batch_size]\n            batch_embeddings = self.embeddings_model.embed_documents(batch_texts)\n            all_embeddings.extend(batch_embeddings)\n        # Add embeddings to document objects\n        for i, doc in enumerate(documents):\n            doc.embedding = all_embeddings[i]\n        return documents\n    def embed_query(self, query):\n        \"\"\"Generate embedding for a query string\"\"\"\n        return self.embeddings_model.embed_query(query)\n</code></pre> The Embedding Service leverages Ollama to generate high-quality embeddings using the mxbai-embed-large model. It includes batch processing to handle large document sets efficiently.</p>"},{"location":"chatbot/rag/architecture-draft/#23-vector-storage-with-milvus","title":"2.3 Vector Storage with Milvus","text":"<p><pre><code>from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType\nimport numpy as np\nimport uuid\nclass MilvusVectorStore:\n    def init(self, host=\"localhost\", port=\"19530\", embedding_dim=1024):\n        self.embedding_dim = embedding_dim\n        self.collection_name = \"markdown_documents\"\n        # Connect to Milvus\n        connections.connect(host=host, port=port)\n        # Create collection if it doesn't exist\n        if not utility.has_collection(self.collection_name):\n            self._create_collection()\n    def createcollection(self):\n        fields = [\n            FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=100),\n            FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n            FieldSchema(name=\"chunk_id\", dtype=DataType.INT64),\n            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n            FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1000),\n            FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim)\n        ]\n        schema = CollectionSchema(fields=fields)\n        collection = Collection(name=self.collection_name, schema=schema)\n        # Create index for fast retrieval\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 200}\n        }\n        collection.create_index(field_name=\"embedding\", index_params=index_params)\n        print(f\"Created Milvus collection: {self.collection_name}\")\n    def add_documents(self, documents):\n        collection = Collection(self.collection_name)\n        entities = []\n        for i, doc in enumerate(documents):\n            # Generate document ID if needed\n            doc_id = doc.metadata.get(\"source\", str(uuid.uuid4()))\n            # Prepare entity\n            entity = {\n                \"id\": f\"{doc_id}_{i}\",\n                \"document_id\": doc_id,\n                \"chunk_id\": i,\n                \"text\": doc.page_content,\n                \"source\": doc.metadata.get(\"source\", \"\"),\n                \"metadata\": doc.metadata,\n                \"embedding\": doc.embedding\n            }\n            entities.append(entity)\n        # Insert data in batches\n        collection.insert(entities)\n        collection.flush()\n        print(f\"Added {len(documents)} documents to Milvus\")\n    def search(self, query_embedding, top_k=5, filter=None):\n        collection = Collection(self.collection_name)\n        collection.load()\n        search_params = {\"metric_type\": \"L2\", \"params\": {\"ef\": 128}}\n        results = collection.search(\n            data=[query_embedding],\n            anns_field=\"embedding\",\n            param=search_params,\n            limit=top_k,\n            expr=filter,\n            output_fields=[\"text\", \"source\", \"metadata\"]\n        )\n        matches = []\n        for hits in results:\n            for hit in hits:\n                matches.append({\n                    \"id\": hit.id,\n                    \"score\": hit.score,\n                    \"text\": hit.entity.get(\"text\"),\n                    \"source\": hit.entity.get(\"source\"),\n                    \"metadata\": hit.entity.get(\"metadata\")\n                })\n        collection.release()\n        return matches\n    def delete_document(self, document_id):\n        collection = Collection(self.collection_name)\n        expr = f'document_id == \"{document_id}\"'\n        collection.delete(expr)\n        print(f\"Deleted document: {document_id}\")\n</code></pre> The Milvus Vector Store service provides efficient storage and retrieval of document embeddings using Milvus's HNSW indexing for fast similarity search. It handles document addition, deletion, and searching with metadata filtering capabilities.</p>"},{"location":"chatbot/rag/architecture-draft/#24-metadata-store","title":"2.4 Metadata Store","text":"<p><pre><code>import sqlite3\nimport json\nclass SQLiteMetadataStore:\n    def init(self, db_path=\"metadata.db\"):\n        self.db_path = db_path\n        self._create_tables()\n    def createtables(self):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        # Create documents table\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS documents (\n            id TEXT PRIMARY KEY,\n            title TEXT,\n            hash TEXT,\n            metadata TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n        ''')\n        conn.commit()\n        conn.close()\n    def update_document_metadata(self, document_id, metadata, document_hash):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata)\n        # Check if document exists\n        cursor.execute(\"SELECT id FROM documents WHERE id = ?\", (document_id,))\n        exists = cursor.fetchone()\n        if exists:\n            cursor.execute('''\n            UPDATE documents SET\n                title = ?,\n                hash = ?,\n                metadata = ?,\n                updated_at = CURRENT_TIMESTAMP\n            WHERE id = ?\n            ''', (metadata.get('title', document_id), document_hash, metadata_json, document_id))\n        else:\n            cursor.execute('''\n            INSERT INTO documents (id, title, hash, metadata)\n            VALUES (?, ?, ?, ?)\n            ''', (document_id, metadata.get('title', document_id), document_hash, metadata_json))\n        conn.commit()\n        conn.close()\n    def get_document_hash(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT hash FROM documents WHERE id = ?\", (document_id,))\n        result = cursor.fetchone()\n        conn.close()\n        if result:\n            return result[0]\n        return None\n    def get_document_metadata(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT metadata FROM documents WHERE id = ?\", (document_id,))\n        result = cursor.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        return None\n    def delete_document(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"DELETE FROM documents WHERE id = ?\", (document_id,))\n        conn.commit()\n        conn.close()\n    def list_all_documents(self):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT id, title, updated_at FROM documents ORDER BY updated_at DESC\")\n        results = cursor.fetchall()\n        conn.close()\n        return [{\"id\": r[0], \"title\": r[1], \"updated_at\": r[2]} for r in results]\n</code></pre> The SQLite Metadata Store provides efficient storage and retrieval of document metadata, including tracking document hashes to detect changes and avoid redundant processing.</p>"},{"location":"chatbot/rag/architecture-draft/#25-langchain-rag-pipeline","title":"2.5 LangChain RAG Pipeline","text":"<p><pre><code>from langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_ollama import Ollama\nfrom typing import List, Dict, Any\nclass RAGPipeline:\n    def init(\n        self,\n        embedding_service,\n        vectorstore_service,\n        metadata_store,\n        model_name=\"llama3\",\n        temperature=0.1,\n        top_k=5\n    ):\n        self.embedding_service = embedding_service\n        self.vectorstore_service = vectorstore_service\n        self.metadata_store = metadata_store\n        self.model_name = model_name\n        self.temperature = temperature\n        self.top_k = top_k\n        # Initialize LLM\n        self.llm = Ollama(\n            model=model_name,\n            temperature=temperature\n        )\n        # Create QA prompt\n        self.qa_prompt = PromptTemplate(\n            template=\"\"\"You are an assistant that helps users find information in a collection of markdown documents.\n            Answer the question based on the following context:\n            {context}\n            Question: {question}\n            Provide a comprehensive answer. If the answer cannot be found in the context, say so clearly.\n            Include relevant source information when possible.\n            \"\"\",\n            input_variables=[\"context\", \"question\"]\n        )\n    def query(self, query_text, filter_metadata=None):\n        # Embed the query\n        query_embedding = self.embedding_service.embed_query(query_text)\n        # Construct filter expression if needed\n        filter_expr = None\n        if filter_metadata:\n            filter_parts = []\n            for key, value in filter_metadata.items():\n                filter_parts.append(f'metadata[\"{key}\"] == \"{value}\"')\n            filter_expr = \" &amp;&amp; \".join(filter_parts)\n        # Retrieve relevant documents\n        results = self.vectorstore_service.search(\n            query_embedding=query_embedding,\n            top_k=self.top_k,\n            filter=filter_expr\n        )\n        # Construct context from results\n        context_texts = []\n        sources = []\n        for result in results:\n            context_texts.append(result[\"text\"])\n            if result[\"source\"]:\n                sources.append(result[\"source\"])\n        context = \"\\n\\n\".join(context_texts)\n        # Generate response with LLM\n        prompt = self.qa_prompt.format(context=context, question=query_text)\n        response = self.llm.invoke(prompt)\n        return {\n            \"query\": query_text,\n            \"response\": response,\n            \"sources\": list(set(sources)),\n            \"results\": results\n        }\n</code></pre> The RAG Pipeline brings together the embedding service, vector store, and LLM to create a complete retrieval-augmented generation system. It handles query embedding, retrieval of relevant documents, and generation of responses using the selected Ollama model.</p>"},{"location":"chatbot/rag/architecture-draft/#26-api-and-interface","title":"2.6 API and Interface","text":"<p><pre><code>from fastapi import FastAPI, HTTPException, Body\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional, Any\nimport uvicorn\nimport os\napp = FastAPI(title=\"Markdown RAG API\")\n# Initialize services\nembedding_service = EmbeddingService(model_name=\"mxbai-embed-large\")\nmetadata_store = SQLiteMetadataStore(db_path=\"metadata.db\")\nvectorstore_service = MilvusVectorStore(\n    host=os.environ.get(\"MILVUS_HOST\", \"localhost\"),\n    port=os.environ.get(\"MILVUS_PORT\", \"19530\")\n)\ndocument_processor = DocumentProcessor(\n    embedding_service=embedding_service,\n    vectorstore_service=vectorstore_service,\n    metadata_store=metadata_store\n)\nrag_pipeline = RAGPipeline(\n    embedding_service=embedding_service,\n    vectorstore_service=vectorstore_service,\n    metadata_store=metadata_store,\n    model_name=os.environ.get(\"OLLAMA_MODEL\", \"llama3\")\n)\n# Initialize directory watcher\nwatcher_service = DirectoryWatcherService(\n    directory_path=os.environ.get(\"VAULT_DIR\", \"./vault\"),\n    processor=document_processor\n)\n# Start directory watcher on startup\n@app.on_event(\"startup\")\nasync def startup_event():\n    # Process all existing files first\n    print(f\"Processing existing files in {os.environ.get('VAULT_DIR', './vault')}\")\n    document_processor.process_all_files(os.environ.get(\"VAULT_DIR\", \"./vault\"))\n    # Start watching for changes\n    print(\"Starting directory watcher\")\n    watcher_service.start()\n# Shutdown directory watcher on shutdown\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    print(\"Stopping directory watcher\")\n    watcher_service.stop()\n# Define API models\nclass QueryRequest(BaseModel):\n    query: str\n    filter_metadata: Optional[Dict[str, Any]] = None\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    top_k: Optional[int] = None\nclass QueryResponse(BaseModel):\n    query: str\n    response: str\n    sources: List[str]\n# Define API endpoints\n@app.post(\"/api/query\", response_model=QueryResponse)\nasync def query(request: QueryRequest = Body(...)):\n    try:\n        # Use provided model params or defaults\n        model = request.model or rag_pipeline.model_name\n        temperature = request.temperature or rag_pipeline.temperature\n        top_k = request.top_k or rag_pipeline.top_k\n        # Create a customized pipeline if needed\n        if model != rag_pipeline.model_name or temperature != rag_pipeline.temperature or top_k != rag_pipeline.top_k:\n            custom_pipeline = RAGPipeline(\n                embedding_service=embedding_service,\n                vectorstore_service=vectorstore_service,\n                metadata_store=metadata_store,\n                model_name=model,\n                temperature=temperature,\n                top_k=top_k\n            )\n            result = custom_pipeline.query(request.query, request.filter_metadata)\n        else:\n            result = rag_pipeline.query(request.query, request.filter_metadata)\n        return {\n            \"query\": result[\"query\"],\n            \"response\": result[\"response\"],\n            \"sources\": result[\"sources\"]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n@app.get(\"/api/documents\")\nasync def list_documents():\n    try:\n        documents = metadata_store.list_all_documents()\n        return documents\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n@app.post(\"/api/reindex\")\nasync def reindex():\n    try:\n        document_processor.process_all_files(os.environ.get(\"VAULT_DIR\", \"./vault\"))\n        return {\"status\": \"success\", \"message\": \"Reindexing complete\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n# Run the API server\nif name == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> The API provides RESTful endpoints for querying the RAG system, listing indexed documents, and triggering reindexing. It uses FastAPI for high performance and includes Pydantic models for request/response validation.</p>"},{"location":"chatbot/rag/architecture-draft/#3-docker-deployment-configuration","title":"3. Docker Deployment Configuration","text":"<pre><code>version: '3.8'\nservices:\n  # LLM service\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n    restart: unless-stopped\n  # Web UI for Ollama\n  openwebui:\n    image: ghcr.io/open-webui/open-webui:latest\n    depends_on:\n      - ollama\n    ports:\n      - \"8080:8080\"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    restart: unless-stopped\n  # Vector database\n  milvus:\n    image: milvusdb/milvus:v2.3.2\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    environment:\n      - ETCD_ENDPOINTS=etcd:2379\n      - MINIO_ADDRESS=minio:9000\n    volumes:\n      - milvus_data:/var/lib/milvus\n    restart: unless-stopped\n    depends_on:\n      - etcd\n      - minio\n  etcd:\n    image: quay.io/coreos/etcd:v3.5.5\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n      - ETCD_SNAPSHOT_COUNT=50000\n    volumes:\n      - etcd_data:/etcd\n    command: etcd --advertise-client-urls=http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n    restart: unless-stopped\n  minio:\n    image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n    environment:\n      - MINIO_ACCESS_KEY=minioadmin\n      - MINIO_SECRET_KEY=minioadmin\n    ports:\n      - \"9000:9000\"\n    volumes:\n      - minio_data:/data\n    command: minio server /data\n    restart: unless-stopped\n  # RAG API service\n  rag-api:\n    build:\n      context: .\n      dockerfile: Dockerfile.rag\n    ports:\n      - \"8000:8000\"\n    environment:\n      - VAULT_DIR=/vault\n      - MILVUS_HOST=milvus\n      - MILVUS_PORT=19530\n      - OLLAMA_URL=http://ollama:11434\n      - OLLAMA_MODEL=llama3\n    volumes:\n      - ./vault:/vault\n      - rag_data:/app/data\n    depends_on:\n      - ollama\n      - milvus\n    restart: unless-stopped\n  # Minimal UI for RAG testing\n  streamlit-ui:\n    build:\n      context: .\n      dockerfile: Dockerfile.ui\n    ports:\n      - \"8501:8501\"\n    environment:\n      - RAG_API_URL=http://rag-api:8000\n    depends_on:\n      - rag-api\n    restart: unless-stopped\nvolumes:\n  ollama_data:\n  milvus_data:\n  etcd_data:\n  minio_data:\n  rag_data:\n</code></pre>"},{"location":"chatbot/rag/architecture-draft/#4-implementation-details-and-best-practices","title":"4. Implementation Details and Best Practices","text":""},{"location":"chatbot/rag/architecture-draft/#41-markdown-processing-strategies","title":"4.1 Markdown Processing Strategies","text":"<p>For optimal handling of markdown files, we implement: 1. Hierarchical Chunking: Split documents based on heading structure to maintain context. 2. Metadata Extraction: Parse YAML frontmatter for enhanced filtering and context. 3. Link Resolution: Handle internal links (e.g., [[wiki-style]] links) to maintain cross-references. 4. Code Block Handling: Special processing for code blocks to preserve formatting and syntax.</p>"},{"location":"chatbot/rag/architecture-draft/#42-embedding-model-selection-and-optimization","title":"4.2 Embedding Model Selection and Optimization","text":"<p>The mxbai-embed-large model provides excellent performance for semantic understanding of technical content. Key considerations: 1. Dimension Reduction: Consider implementing PCA for large collections to reduce storage requirements. 2. Batch Processing: Process embeddings in batches to optimize throughput. 3. Caching: Implement caching for frequently accessed embeddings. 4. Quantization: For larger collections, consider quantizing embeddings to reduce storage and memory footprint.</p>"},{"location":"chatbot/rag/architecture-draft/#43-milvus-configuration-and-optimization","title":"4.3 Milvus Configuration and Optimization","text":"<p>For optimal Milvus performance: 1. Index Selection: HNSW (Hierarchical Navigable Small World) provides the best balance of accuracy and performance. 2. Parameter Tuning:    - M: Controls the maximum number of connections per node (8-16 recommended)    - efConstruction: Controls index build quality (100-200 recommended)    - ef: Controls search accuracy (50-150 recommended) 3. Resource Allocation: Configure adequate memory for Milvus, especially for the index. 4. Collection Design: Use partitioning for larger collections to improve query performance.</p>"},{"location":"chatbot/rag/architecture-draft/#44-advanced-rag-techniques","title":"4.4 Advanced RAG Techniques","text":"<p>To enhance RAG performance: 1. Query Reformulation: Process user queries to improve retrieval effectiveness:    <pre><code>def preprocess_query(query):\n    # Expand acronyms, handle synonyms, etc.\n    # ...\n    return processed_query\n</code></pre> 2. Hybrid Search: Combine vector similarity with keyword search for improved recall:    <pre><code>def hybrid_search(query, vectorstore, metadata_store):\n    # Vector search\n    vector_results = vectorstore.search(query_embedding)\n    # Keyword search\n    keyword_results = metadata_store.keyword_search(query)\n    # Combine results with appropriate weighting\n    combined_results = combine_search_results(vector_results, keyword_results)\n    return combined_results\n</code></pre> 3. Reranking: Implement a two-stage retrieval process to refine results:    <pre><code>def rerank_results(query, initial_results, reranker_model):\n    query_doc_pairs = [(query, result[\"text\"]) for result in initial_results]\n    scores = reranker_model.compute_scores(query_doc_pairs)\n    # Sort by reranker scores\n    reranked_results = [\n        (initial_results[i], scores[i])\n        for i in range(len(initial_results))\n    ]\n    reranked_results.sort(key=lambda x: x[1], reverse=True)\n    return [item[0] for item in reranked_results]\n</code></pre> 4. LLM Agents for Query Planning:    <pre><code>def agent_based_query(query, rag_pipeline):\n    # Analyze query to determine approach\n    query_plan = rag_pipeline.llm.invoke(f\"\"\"\n    Analyze this query and create a search plan:\n    Query: {query}\n    What kind of information is needed? Should I:\n    1. Perform a direct search\n    2. Break this into sub-questions\n    3. Filter by specific metadata\n    Plan:\n    \"\"\")\n    # Execute the plan\n    if \"sub-questions\" in query_plan:\n        # Handle multi-hop retrieval\n        # ...\n    else:\n        # Direct retrieval\n        return rag_pipeline.query(query)\n</code></pre></p>"},{"location":"chatbot/rag/architecture-draft/#45-evaluation-and-monitoring","title":"4.5 Evaluation and Monitoring","text":"<p>Implement comprehensive evaluation metrics: 1. Retrieval Evaluation:    - Mean Reciprocal Rank (MRR)    - Normalized Discounted Cumulative Gain (NDCG)    - Precision@K and Recall@K 2. Response Quality Evaluation:    - Factual Accuracy    - Answer Relevance    - Citation Accuracy 3. System Monitoring:    - Query latency    - Embedding generation throughput    - Vector store query performance    - LLM response time</p>"},{"location":"chatbot/rag/architecture-draft/#5-extension-points","title":"5. Extension Points","text":"<p>The modular design allows for several extensions: 1. Multi-Modal Support: Extend to handle images and other media in markdown. 2. Semantic Caching: Implement a semantic cache for similar queries. 3. Custom Embedding Models: Allow customization of embedding models based on domain. 4. User Feedback Integration: Capture user feedback to improve retrieval and generation. 5. Self-Critique and Refinement: Implement self-evaluation and refinement of responses.</p>"},{"location":"chatbot/rag/architecture-draft/#6-testing-strategy","title":"6. Testing Strategy","text":"<p>Comprehensive testing includes: 1. Unit Tests: For individual components. 2. Integration Tests: For component interactions. 3. End-to-End Tests: Using a test corpus of markdown documents. 4. Performance Testing: Under various loads and document sizes. 5. Regression Testing: To ensure continued quality as the system evolves.</p>"},{"location":"chatbot/rag/architecture-draft/#7-appendix","title":"7. Appendix","text":""},{"location":"chatbot/rag/architecture-draft/#71-installation-instructions","title":"7.1 Installation Instructions","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/markdown-rag.git\ncd markdown-rag\n# Build and start services\ndocker-compose up -d\n# Download required models\ncurl -X POST http://localhost:11434/api/pull -d '{\"name\": \"mxbai-embed-large\"}'\ncurl -X POST http://localhost:11434/api/pull -d '{\"name\": \"llama3\"}'\n# Verify installation\ncurl http://localhost:8000/api/documents\n</code></pre>"},{"location":"chatbot/rag/architecture-draft/#72-api-documentation","title":"7.2 API Documentation","text":"<p>The API is documented using OpenAPI and accessible at http://localhost:8000/docs when the service is running.</p>"},{"location":"chatbot/rag/architecture-draft/#73-performance-benchmarks","title":"7.3 Performance Benchmarks","text":"<p>Initial benchmarks with a corpus of 1,000 markdown files (~10MB total): - Document processing: ~5 documents/second - Query latency: ~500ms (including embedding generation and retrieval) - Memory usage: ~2GB (Milvus) + ~1GB (Python services)</p>"},{"location":"chatbot/rag/evaluation/","title":"RAG Pipeline Evaluation","text":"<p>This page outlines how to evaluate and optimize the performance of the Obelisk RAG system.</p>"},{"location":"chatbot/rag/evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The RAG pipeline will be evaluated using several key metrics:</p>"},{"location":"chatbot/rag/evaluation/#retrieval-metrics","title":"Retrieval Metrics","text":"<ul> <li>Precision@k: Proportion of relevant documents in the top k results</li> <li>Recall@k: Proportion of all relevant documents that appear in the top k</li> <li>Mean Reciprocal Rank (MRR): Average position of the first relevant document</li> <li>Normalized Discounted Cumulative Gain (NDCG): Measures ranking quality</li> </ul>"},{"location":"chatbot/rag/evaluation/#generation-metrics","title":"Generation Metrics","text":"<ul> <li>Answer Relevance: How relevant the answer is to the question</li> <li>Factual Correctness: Whether the answer contains factual errors</li> <li>Hallucination Rate: Proportion of generated content not supported by context</li> <li>Citation Accuracy: Whether sources are accurately cited</li> <li>Completeness: Whether the answer fully addresses the question</li> </ul>"},{"location":"chatbot/rag/evaluation/#system-metrics","title":"System Metrics","text":"<ul> <li>Latency: End-to-end response time</li> <li>Token Efficiency: Number of tokens used vs. information conveyed</li> <li>Resource Usage: Memory and CPU consumption</li> <li>Throughput: Queries processed per second</li> </ul>"},{"location":"chatbot/rag/evaluation/#evaluation-framework","title":"Evaluation Framework","text":"<p>The RAG pipeline will include a built-in evaluation framework:</p> <pre><code># Future implementation example\nclass RAGEvaluator:\n    def __init__(self, config):\n        self.config = config\n        self.test_cases = self._load_test_cases()\n\n    def _load_test_cases(self):\n        \"\"\"Load test cases from configuration.\"\"\"\n        # Implementation details\n\n    def evaluate_retrieval(self, query_processor):\n        \"\"\"Evaluate retrieval performance.\"\"\"\n        results = {}\n\n        for test_case in self.test_cases:\n            query = test_case[\"query\"]\n            relevant_docs = test_case[\"relevant_docs\"]\n\n            retrieved = query_processor.process_query(query)\n            retrieved_ids = [doc[\"id\"] for doc in retrieved[\"retrieved_chunks\"]]\n\n            results[query] = {\n                \"precision\": self._calculate_precision(retrieved_ids, relevant_docs),\n                \"recall\": self._calculate_recall(retrieved_ids, relevant_docs),\n                \"mrr\": self._calculate_mrr(retrieved_ids, relevant_docs),\n                \"ndcg\": self._calculate_ndcg(retrieved_ids, relevant_docs)\n            }\n\n        return results\n\n    def evaluate_generation(self, rag_pipeline):\n        \"\"\"Evaluate generation quality.\"\"\"\n        # Implementation details\n\n    def evaluate_system(self, rag_pipeline):\n        \"\"\"Evaluate system performance.\"\"\"\n        # Implementation details\n\n    def generate_report(self, results):\n        \"\"\"Generate evaluation report.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/evaluation/#synthetic-test-suite","title":"Synthetic Test Suite","text":"<p>The evaluation framework will include a synthetic test suite:</p> <ol> <li>Query Generation: Generate realistic user queries</li> <li>Expected Answer Creation: Create expected answers</li> <li>Document Tagging: Tag documents for relevance</li> <li>Test Case Assembly: Create complete test cases</li> </ol> <p>Example test case:</p> <pre><code>{\n  \"query\": \"How do I configure the Ollama service in Docker?\",\n  \"query_type\": \"how-to\",\n  \"relevant_docs\": [\n    \"development/docker.md#ollama-service\",\n    \"chatbot/index.md#services-configuration\"\n  ],\n  \"expected_answer_elements\": [\n    \"Ollama service configuration in docker-compose.yaml\",\n    \"Environment variables for GPU acceleration\",\n    \"Volume mounts for model storage\"\n  ],\n  \"difficulty\": \"medium\"\n}\n</code></pre>"},{"location":"chatbot/rag/evaluation/#human-evaluation","title":"Human Evaluation","text":"<p>In addition to automated metrics, human evaluation will be critical:</p> <ol> <li>Side-by-side comparisons: Compare RAG vs. non-RAG responses</li> <li>Blind evaluation: Rate responses without knowing the source</li> <li>Expert review: Domain experts evaluate factual correctness</li> <li>User feedback collection: Gather feedback from real users</li> </ol>"},{"location":"chatbot/rag/evaluation/#evaluation-dashboard","title":"Evaluation Dashboard","text":"<p>The RAG pipeline will include a visual dashboard for evaluation:</p> <pre><code>graph TD\n    A[Evaluation Runner] --&gt;|Executes Tests| B[Test Suite]\n    B --&gt;|Generates Metrics| C[Metrics Store]\n    C --&gt;|Visualizes Results| D[Dashboard]\n    D --&gt;|Precision| E[Retrieval Metrics]\n    D --&gt;|Correctness| F[Generation Metrics]\n    D --&gt;|Performance| G[System Metrics]\n    D --&gt;|Overall| H[Combined Score]</code></pre>"},{"location":"chatbot/rag/evaluation/#continuous-improvement","title":"Continuous Improvement","text":"<p>The evaluation system will enable continuous improvement:</p>"},{"location":"chatbot/rag/evaluation/#error-analysis","title":"Error Analysis","text":"<p>Categorizing and tracking error types:</p> <ul> <li>Retrieval failures: Relevant content not retrieved</li> <li>Context utilization errors: Context ignored or misinterpreted</li> <li>Hallucination instances: Information not grounded in context</li> <li>Citation errors: Missing or incorrect citations</li> </ul>"},{"location":"chatbot/rag/evaluation/#optimization-process","title":"Optimization Process","text":"<p>A systematic approach to RAG optimization:</p> <ol> <li>Baseline establishment: Measure initial performance</li> <li>Component isolation: Test each component independently</li> <li>Ablation studies: Remove components to measure impact</li> <li>Parameter tuning: Optimize configuration parameters</li> <li>A/B testing: Compare variations with real users</li> </ol>"},{"location":"chatbot/rag/evaluation/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>The evaluation system will be implemented in phases:</p> Phase Feature Description 1 Basic Metrics Implement core precision/recall metrics 2 Automated Test Suite Create synthetic test cases 3 Human Evaluation Tools Build tools for human feedback 4 Dashboard Create visualization dashboard 5 Continuous Monitoring Implement ongoing evaluation"},{"location":"chatbot/rag/evaluation/#best-practices","title":"Best Practices","text":"<p>Recommendations for effective RAG evaluation:</p> <ol> <li>Diverse test cases: Include varied query types and difficulty levels</li> <li>Regular re-evaluation: Test after each significant change</li> <li>User-focused metrics: Prioritize metrics aligned with user satisfaction</li> <li>Documentation-specific evaluation: Create tests specific to documentation use cases</li> <li>Comparative analysis: Benchmark against similar systems</li> </ol>"},{"location":"chatbot/rag/getting-started/","title":"Getting Started with Obelisk RAG","text":"<p>This guide will help you get started with the Retrieval Augmented Generation (RAG) system in Obelisk. We'll walk through the process of setting up your environment, initializing the system, and making your first query.</p>"},{"location":"chatbot/rag/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you start, ensure you have:</p> <ol> <li>Obelisk installed: The RAG system is part of Obelisk</li> <li>Ollama running: The system requires Ollama for LLMs and embeddings</li> <li>Documentation in your vault: Some markdown files to index</li> </ol>"},{"location":"chatbot/rag/getting-started/#step-1-start-ollama","title":"Step 1: Start Ollama","text":"<p>The RAG system requires Ollama for generating embeddings and providing LLM capabilities. You can run Ollama using Docker:</p> <pre><code>docker-compose up ollama -d\n</code></pre> <p>Wait for Ollama to start up (this might take a minute).</p>"},{"location":"chatbot/rag/getting-started/#step-2-pull-required-models","title":"Step 2: Pull Required Models","text":"<p>The RAG system needs models for embedding generation and text generation. You can pull them using:</p> <pre><code># Pull the LLM model (llama3 is recommended)\ndocker exec -it ollama ollama pull llama3\n\n# Pull the embedding model\ndocker exec -it ollama ollama pull mxbai-embed-large\n</code></pre> <p>This step will download the required models. The embedding model is optimized for generating high-quality embeddings for document retrieval.</p>"},{"location":"chatbot/rag/getting-started/#step-3-configure-the-rag-system","title":"Step 3: Configure the RAG System","text":"<p>The default configuration should work for most users, but you can customize it if needed:</p> <pre><code># View current configuration\nobelisk-rag config --show\n\n# Set a different vault directory if needed\nobelisk-rag config --set \"vault_dir=/path/to/your/docs\"\n\n# Set different Ollama URL if needed\nobelisk-rag config --set \"ollama_url=http://ollama:11434\"\n</code></pre>"},{"location":"chatbot/rag/getting-started/#step-4-index-your-documentation","title":"Step 4: Index Your Documentation","text":"<p>Before you can query your documentation, you need to index it:</p> <pre><code>obelisk-rag index\n</code></pre> <p>This process will: 1. Read all markdown files in your vault 2. Extract the content and metadata 3. Split the content into chunks 4. Generate embeddings for each chunk 5. Store everything in a vector database</p> <p>You should see a progress report in the console as files are processed.</p>"},{"location":"chatbot/rag/getting-started/#step-5-make-your-first-query","title":"Step 5: Make Your First Query","text":"<p>Now you can query your documentation:</p> <pre><code>obelisk-rag query \"What is Obelisk?\"\n</code></pre> <p>The system will: 1. Convert your query to an embedding 2. Find the most relevant document chunks 3. Use those chunks as context for the LLM 4. Generate a response based on your documentation</p> <p>You should see a response that's specifically informed by your documentation.</p>"},{"location":"chatbot/rag/getting-started/#step-6-start-the-api-server-optional","title":"Step 6: Start the API Server (Optional)","text":"<p>If you want to integrate with other applications or want the real-time document watching feature, you can start the API server:</p> <pre><code>obelisk-rag serve --watch\n</code></pre> <p>This will: 1. Start a REST API server (default: http://0.0.0.0:8000) 2. Provide endpoints for querying and stats 3. Watch for changes to documentation files and update the index automatically</p>"},{"location":"chatbot/rag/getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chatbot/rag/getting-started/#common-issues","title":"Common Issues","text":"<ol> <li>Connection errors with Ollama:</li> </ol> <pre><code>Error: Failed to connect to Ollama service at http://localhost:11434\n</code></pre> <p>Ensure Ollama is running and accessible at the configured URL. You may need to adjust the URL with:</p> <pre><code>obelisk-rag config --set \"ollama_url=http://ollama:11434\"\n</code></pre> <ol> <li>No results when querying:</li> </ol> <pre><code>No documents found for query: What is Obelisk?\n</code></pre> <p>Check that your documentation has been indexed successfully. Run <code>obelisk-rag stats</code> to see how many documents are in the database.</p> <ol> <li>Model not found errors:</li> </ol> <pre><code>Error: Model 'llama3' not found\n</code></pre> <p>Ensure you have pulled the required models using Ollama.</p>"},{"location":"chatbot/rag/getting-started/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>If you're encountering issues, you can enable debug mode for more detailed logs:</p> <pre><code>export RAG_DEBUG=1\nobelisk-rag query \"What is Obelisk?\"\n</code></pre>"},{"location":"chatbot/rag/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have the RAG system up and running, you can:</p> <ol> <li>Learn about advanced configuration options</li> <li>Integrate with Open WebUI for a chat interface</li> <li>Explore the RAG architecture in depth</li> <li>Read about the implementation details if you want to customize the system</li> <li>Review evaluation techniques to measure and improve performance</li> </ol> <p>For more detailed usage information, see Using the RAG System.</p>"},{"location":"chatbot/rag/implementation/","title":"Implementation Guide","text":"<p>This guide provides a roadmap for implementing the RAG pipeline in Obelisk, covering integration with the existing codebase and deployment considerations.</p>"},{"location":"chatbot/rag/implementation/#architecture-integration","title":"Architecture Integration","text":"<p>The RAG pipeline will be integrated into Obelisk's architecture:</p> <pre><code>graph TD\n    A[Obsidian Vault] --&gt;|Convert| B[MkDocs Site]\n    A --&gt;|Process| C[Document Processor]\n    C --&gt;|Embed| D[Vector Database]\n    E[User Query] --&gt;|via WebUI| F[Query Processor]\n    F --&gt;|Search| D\n    D --&gt;|Retrieved Chunks| G[Context Builder]\n    G --&gt;|Enhanced Prompt| H[Ollama API]\n    H --&gt;|Response| I[Web Interface]</code></pre>"},{"location":"chatbot/rag/implementation/#core-components","title":"Core Components","text":""},{"location":"chatbot/rag/implementation/#1-document-processor","title":"1. Document Processor","text":"<p>Responsible for parsing Markdown files and creating embeddings:</p> <pre><code># Future implementation example\nclass DocumentProcessor:\n    def __init__(self, config):\n        self.config = config\n        self.embedding_model = self._load_embedding_model()\n\n    def process_vault(self, vault_path):\n        \"\"\"Process all documents in the vault.\"\"\"\n        documents = self._collect_documents(vault_path)\n        chunks = self._chunk_documents(documents)\n        embeddings = self._create_embeddings(chunks)\n        self._store_in_vectordb(chunks, embeddings)\n\n    def _load_embedding_model(self):\n        \"\"\"Load the configured embedding model.\"\"\"\n        # Implementation details\n\n    def _collect_documents(self, path):\n        \"\"\"Collect all markdown documents.\"\"\"\n        # Implementation details\n\n    def _chunk_documents(self, documents):\n        \"\"\"Split documents into appropriate chunks.\"\"\"\n        # Implementation details\n\n    def _create_embeddings(self, chunks):\n        \"\"\"Generate embeddings for all chunks.\"\"\"\n        # Implementation details\n\n    def _store_in_vectordb(self, chunks, embeddings):\n        \"\"\"Store chunks and embeddings in vector database.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#2-vector-database-manager","title":"2. Vector Database Manager","text":"<p>Interface for vector database operations:</p> <pre><code># Future implementation example\nclass VectorDBManager:\n    def __init__(self, config):\n        self.config = config\n        self.db = self._initialize_db()\n\n    def _initialize_db(self):\n        \"\"\"Initialize the vector database based on configuration.\"\"\"\n        db_type = self.config.get(\"vector_db\", \"chroma\")\n        if db_type == \"chroma\":\n            return self._init_chroma()\n        elif db_type == \"faiss\":\n            return self._init_faiss()\n        # Other implementations\n\n    def add_documents(self, chunks, embeddings, metadata):\n        \"\"\"Add document chunks to the database.\"\"\"\n        # Implementation details\n\n    def search(self, query_embedding, filters=None, k=5):\n        \"\"\"Search for similar documents.\"\"\"\n        # Implementation details\n\n    def update_document(self, doc_id, new_embedding=None, new_metadata=None):\n        \"\"\"Update an existing document.\"\"\"\n        # Implementation details\n\n    def delete_document(self, doc_id):\n        \"\"\"Remove a document from the database.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#3-query-processor","title":"3. Query Processor","text":"<p>Handles user queries and retrieval:</p> <pre><code># Future implementation example\nclass QueryProcessor:\n    def __init__(self, vector_db, embedding_model, config):\n        self.vector_db = vector_db\n        self.embedding_model = embedding_model\n        self.config = config\n\n    async def process_query(self, query_text):\n        \"\"\"Process a user query and retrieve relevant context.\"\"\"\n        # Preprocess query\n        processed_query = self._preprocess_query(query_text)\n\n        # Generate embedding\n        query_embedding = self.embedding_model.embed(processed_query)\n\n        # Retrieve relevant chunks\n        results = self.vector_db.search(\n            query_embedding,\n            filters=processed_query.get(\"filters\"),\n            k=self.config.get(\"retrieve_top_k\", 5)\n        )\n\n        # Assemble context\n        context = self._assemble_context(results)\n\n        return {\n            \"original_query\": query_text,\n            \"processed_query\": processed_query,\n            \"retrieved_chunks\": results,\n            \"assembled_context\": context\n        }\n</code></pre>"},{"location":"chatbot/rag/implementation/#4-prompt-manager","title":"4. Prompt Manager","text":"<p>Handles prompt assembly and model interaction:</p> <pre><code># Future implementation example\nclass PromptManager:\n    def __init__(self, config):\n        self.config = config\n        self.templates = self._load_templates()\n\n    def _load_templates(self):\n        \"\"\"Load prompt templates from configuration.\"\"\"\n        # Implementation details\n\n    def create_prompt(self, query, context):\n        \"\"\"Create a prompt with retrieved context.\"\"\"\n        template = self.templates.get(\"default_rag\")\n        return template.format(\n            retrieved_context=self._format_context(context),\n            user_question=query[\"original_query\"]\n        )\n\n    def _format_context(self, context_items):\n        \"\"\"Format retrieved context items for the prompt.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#integration-with-ollama","title":"Integration with Ollama","text":"<p>The RAG pipeline will integrate with Ollama:</p> <pre><code># Future implementation example\nclass OllamaIntegration:\n    def __init__(self, config):\n        self.api_base = config.get(\"ollama_api_url\", \"http://localhost:11434\")\n        self.model = config.get(\"ollama_model\", \"mistral\")\n\n    async def generate_response(self, prompt, params=None):\n        \"\"\"Generate a response from Ollama.\"\"\"\n        default_params = {\n            \"temperature\": 0.7,\n            \"top_p\": 0.9,\n            \"max_tokens\": 1024\n        }\n        params = {**default_params, **(params or {})}\n\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.api_base}/api/generate\",\n                json={\"model\": self.model, \"prompt\": prompt, **params}\n            ) as response:\n                return await response.json()\n</code></pre>"},{"location":"chatbot/rag/implementation/#web-ui-integration","title":"Web UI Integration","text":"<p>Connection to the Open WebUI interface:</p> <pre><code># Future implementation example\nclass WebUIIntegration:\n    def __init__(self, config):\n        self.config = config\n\n    def register_endpoints(self, app):\n        \"\"\"Register RAG endpoints with the web application.\"\"\"\n        app.add_route(\"/api/rag/query\", self.handle_query)\n\n    async def handle_query(self, request):\n        \"\"\"Handle RAG query requests.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#configuration-system","title":"Configuration System","text":"<p>RAG features will be configurable through MkDocs configuration:</p> <pre><code># Example future configuration\nplugins:\n  - obelisk-rag:\n      # Document processing\n      chunk_size: 512\n      chunk_overlap: 50\n      chunk_strategy: \"fixed\"  # fixed, semantic, recursive\n\n      # Embedding\n      embedding_model: \"nomic-embed-text\"\n      embedding_dimension: 768\n\n      # Vector database\n      vector_db: \"chroma\"\n      vector_db_path: \"./.obelisk/vectordb\"\n\n      # Query processing\n      retrieve_top_k: 5\n      reranking_enabled: true\n      hybrid_search: true\n\n      # Integration\n      ollama_api_url: \"http://ollama:11434\"\n      ollama_model: \"mistral\"\n\n      # Templates\n      prompt_template: \"default_rag\"\n      custom_templates:\n        my_template: \"path/to/template.txt\"\n</code></pre>"},{"location":"chatbot/rag/implementation/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"chatbot/rag/implementation/#resource-requirements","title":"Resource Requirements","text":"Deployment Size Documents Vector DB Size RAM Storage Small (&lt;100 docs) &lt;1,000 chunks ~100MB 2GB 1GB Medium (~500 docs) ~5,000 chunks ~500MB 4GB 5GB Large (1000+ docs) 10,000+ chunks 1GB+ 8GB+ 10GB+"},{"location":"chatbot/rag/implementation/#docker-configuration","title":"Docker Configuration","text":"<p>Additional container configuration for RAG:</p> <pre><code># Future docker-compose additions\nservices:\n  obelisk:\n    # Existing configuration...\n    environment:\n      - OBELISK_RAG_ENABLED=true\n      - OBELISK_VECTOR_DB_PATH=/data/vectordb\n    volumes:\n      - vectordb_data:/data/vectordb\n\nvolumes:\n  vectordb_data:\n</code></pre>"},{"location":"chatbot/rag/implementation/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<p>The RAG system will include:</p> <ol> <li>Embedding updates: Trigger on content changes</li> <li>Index optimization: Scheduled maintenance tasks</li> <li>Performance metrics: Track latency and quality</li> <li>Usage statistics: Monitor query patterns</li> <li>Content gap analysis: Identify missing documentation</li> </ol>"},{"location":"chatbot/rag/mvp-status/","title":"RAG MVP Status","text":"<p>The Retrieval Augmented Generation (RAG) system has reached Minimum Viable Product (MVP) status! This document summarizes what has been implemented and what's planned for future iterations.</p>"},{"location":"chatbot/rag/mvp-status/#mvp-features-implemented","title":"MVP Features Implemented","text":"<p>\u2705 Document Processing: - Markdown document loading from vault - YAML frontmatter extraction - Text chunking with configurable parameters - File system watching for real-time updates</p> <p>\u2705 Embedding Generation: - Integration with Ollama for embeddings - Document and query embedding generation - Error handling and logging</p> <p>\u2705 Vector Storage: - ChromaDB integration for vector storage - Document storage and retrieval - Similarity search with configurable k parameter</p> <p>\u2705 RAG Service: - Integration of all components - Context augmentation for LLM prompts - Proper prompt engineering for effective responses - Fallback handling for no-context scenarios</p> <p>\u2705 Command-Line Interface: - Document indexing - Query processing - Configuration management - System statistics</p> <p>\u2705 API Server: - REST API for integration - Query endpoint - Statistics endpoint - Real-time document watching</p>"},{"location":"chatbot/rag/mvp-status/#whats-working-now","title":"What's Working Now","text":"<p>With the current implementation, you can:</p> <ol> <li> <p>Index your documentation:    <pre><code>obelisk-rag index\n</code></pre></p> </li> <li> <p>Query your documentation:    <pre><code>obelisk-rag query \"What is Obelisk?\"\n</code></pre></p> </li> <li> <p>Start the API server:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>Configure the system:    <pre><code>obelisk-rag config --set \"retrieve_top_k=5\"\n</code></pre></p> </li> <li> <p>View system statistics:    <pre><code>obelisk-rag stats\n</code></pre></p> </li> </ol>"},{"location":"chatbot/rag/mvp-status/#engineering-notes-and-technical-achievements","title":"Engineering Notes and Technical Achievements","text":"<p>Several key engineering challenges were addressed during the MVP implementation:</p> <p>\u2705 Configuration Management: - Created a unified configuration system using environment variables with OBELISK_ prefix - Implemented config file persistence as JSON - Added validation with proper error messages - Created CLI-based configuration management</p> <p>\u2705 Error Handling and Resilience: - Added comprehensive error handling throughout the codebase - Implemented connection retry mechanisms for Ollama services - Added proper logging with configurable levels - Created meaningful error messages for users</p> <p>\u2705 Metadata Processing: - Solved YAML frontmatter extraction and parsing issues - Fixed serialization problems with complex data types in metadata - Implemented proper date handling in document metadata - Created metadata filtering for vector storage</p> <p>\u2705 Performance Considerations: - Optimized document chunking for better retrieval results - Improved embedding generation with batch processing - Added efficient file watching with debouncing - Implemented multi-threaded processing where appropriate</p>"},{"location":"chatbot/rag/mvp-status/#areas-for-future-enhancement","title":"Areas for Future Enhancement","text":"<p>While the MVP is functional and production-ready, several areas could be enhanced in future iterations:</p> <p>\ud83d\udd04 Advanced Chunking: - Semantic chunking based on content meaning - Heading-based chunking - Improved handling of code blocks and tables</p> <p>\ud83d\udd04 Enhanced Retrieval: - Hybrid retrieval (keywords + vectors) - Re-ranking of retrieved documents - Additional filtering options based on metadata</p> <p>\ud83d\udd04 Advanced LLM Integration: - Support for more models - Improved streaming responses - Model parameter customization through UI</p> <p>\ud83d\udd04 Web UI Integration: - Dedicated Web UI components - Visualization of retrieved contexts - Search highlighting</p> <p>\ud83d\udd04 Performance Optimization: - Caching for frequent queries - Additional batch processing optimizations - Benchmarking and optimization</p>"},{"location":"chatbot/rag/mvp-status/#next-steps","title":"Next Steps","text":"<p>The next development priorities are:</p> <ol> <li>Web UI Integration: Create tight integration with Open WebUI</li> <li>Develop custom plugin for OpenWebUI integration</li> <li>Add document source display in responses</li> <li> <p>Create admin interface for monitoring and management</p> </li> <li> <p>Enhanced Evaluation: Implement evaluation tools for measuring RAG quality</p> </li> <li>Develop benchmark datasets for testing retrieval quality</li> <li>Add automated testing framework for RAG metrics</li> <li> <p>Create evaluation dashboard for monitoring performance</p> </li> <li> <p>Advanced Retrieval: Add re-ranking and hybrid retrieval capabilities</p> </li> <li>Implement hybrid search with keywords and vectors</li> <li>Add re-ranking with cross-encoders for improved relevance</li> <li> <p>Create filtering mechanisms based on document metadata</p> </li> <li> <p>User Feedback Loop: Add mechanisms to incorporate user feedback</p> </li> <li>Implement thumbs up/down feedback collection</li> <li>Create feedback database for training improvements</li> <li>Develop tools for analyzing feedback patterns</li> </ol>"},{"location":"chatbot/rag/mvp-status/#conclusion","title":"Conclusion","text":"<p>The RAG MVP has been successfully implemented and is now production-ready! All core components are functioning as expected:</p> <ul> <li>\u2705 Document processing pipeline with YAML frontmatter handling</li> <li>\u2705 Embedding generation using Ollama and mxbai-embed-large</li> <li>\u2705 Vector storage with ChromaDB</li> <li>\u2705 RAG integration with context augmentation</li> <li>\u2705 CLI and API interfaces for user interaction</li> <li>\u2705 Docker containerization and integration</li> </ul> <p>The implementation provides a solid foundation for document retrieval and generation in Obelisk. It enables users to interact with their documentation through natural language queries and receive contextually relevant responses using their own local infrastructure.</p> <p>We've overcome several technical challenges related to metadata handling, error resilience, and system integration. The result is a robust system that can be easily deployed and used in production environments.</p> <p>As we move forward, we'll continue to enhance and expand the RAG capabilities based on user feedback and emerging best practices in the field of retrieval augmented generation. The modular architecture we've established provides a solid foundation for future improvements without requiring significant refactoring.</p>"},{"location":"chatbot/rag/mvp/","title":"RAG System MVP Implementation Roadmap","text":"<p>Based on our research and requirements, this document outlines a focused MVP roadmap for building a local-first RAG system that integrates with Obsidian vault and Ollama setup. This approach prioritizes key functionality while setting a foundation for future enhancements.</p>"},{"location":"chatbot/rag/mvp/#development-checklist","title":"Development Checklist","text":""},{"location":"chatbot/rag/mvp/#prerequisites","title":"Prerequisites","text":"<ul> <li> Pull required embedding and LLM models:   <pre><code>ollama pull llama3\nollama pull mxbai-embed-large\n</code></pre> <p>Completed on 2025-04-11. Models are available via Ollama Docker container. The embedding model is 669MB, and the LLM is 4.7GB.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-1-project-setup-dependencies","title":"Phase 1: Project Setup &amp; Dependencies","text":"<ul> <li> Create module structure in <code>obelisk/rag/</code> <p>Created and implemented complete file structure on 2025-04-11 with all required modules.</p> </li> <li> Update <code>pyproject.toml</code> with RAG dependencies <p>Added dependencies on 2025-04-11 and updated with Poetry. Successfully installed langchain, langchain-community, langchain-ollama, chromadb, watchdog, fastapi, uvicorn, and pydantic.</p> </li> <li> Create basic configuration system for RAG settings <p>Implemented robust configuration system with environment variable support, defaults, and validation. Configuration can be modified via CLI and serialized to JSON.</p> </li> <li> Add initial unit tests structure <p>Implemented comprehensive test suite covering all RAG components with both unit and integration tests.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-2-document-processing-pipeline","title":"Phase 2: Document Processing Pipeline","text":"<ul> <li> Implement document loader for Markdown files <p>Created robust DocumentProcessor class that handles Markdown files with proper error handling and logging.</p> </li> <li> Create text splitter with appropriate chunk sizing <p>Implemented RecursiveCharacterTextSplitter with configurable chunk size and overlap parameters.</p> </li> <li> Develop file change monitoring system <p>Added real-time file watching using Watchdog with event handlers for file creation and modification.</p> </li> <li> Set up metadata extraction from documents <p>Implemented YAML frontmatter extraction with proper error handling and metadata filtering.</p> </li> <li> Test document processing with sample files <p>Validated document processing with real Obelisk documentation files, ensuring proper chunking and metadata extraction.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-3-embedding-vector-storage","title":"Phase 3: Embedding &amp; Vector Storage","text":"<ul> <li> Implement Ollama embedding integration <p>Successfully integrated Ollama embedding service using the mxbai-embed-large model with optimized error handling.</p> </li> <li> Configure ChromaDB for vector storage <p>Configured ChromaDB with proper persistence, filtering, and retrieval mechanisms.</p> </li> <li> Create persistence mechanism for embeddings <p>Implemented persistence to disk with configurable directory location and automatic backup.</p> </li> <li> Develop document indexing pipeline <p>Created efficient indexing pipeline with progress reporting and multi-threaded processing.</p> </li> <li> Build retrieval system for querying vectors <p>Implemented similarity search with configurable k parameter and metadata filtering capabilities.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-4-rag-pipeline-llm-integration","title":"Phase 4: RAG Pipeline &amp; LLM Integration","text":"<ul> <li> Create prompt templates for RAG <p>Developed optimized prompt templates for context insertion with proper formatting and instructions.</p> </li> <li> Implement Ollama LLM integration <p>Integrated Ollama LLM service with proper connection handling, retry mechanisms, and configurable parameters.</p> </li> <li> Develop RAG chain with context injection <p>Created RAG service that properly retrieves context and injects it into prompts for enhanced responses.</p> </li> <li> Add configuration options for the pipeline <p>Implemented comprehensive configuration options for all aspects of the RAG pipeline, including model parameters.</p> </li> <li> Test end-to-end query with retrieved context <p>Successfully tested end-to-end query processing with real documentation, validating context retrieval and response quality.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-5-user-interfaces","title":"Phase 5: User Interfaces","text":"<ul> <li> Build command-line interface <p>Implemented comprehensive CLI with commands for indexing, querying, configuration, and statistics.</p> </li> <li> Develop simple API with FastAPI <p>Created FastAPI application with proper endpoint definitions, validation, and error handling.</p> </li> <li> Create basic documentation for usage <p>Wrote detailed usage documentation for both CLI and API interfaces with examples.</p> </li> <li> Implement endpoints for querying and reindexing <p>Added endpoints for querying, reindexing, file watching, and system statistics.</p> </li> <li> Test interfaces with real documents <p>Validated both interfaces with real-world usage scenarios and sample queries.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#phase-6-docker-integration","title":"Phase 6: Docker &amp; Integration","text":"<ul> <li> Create Dockerfile for RAG service <p>Developed optimized Dockerfile with proper layer caching and minimal dependencies.</p> </li> <li> Update docker-compose.yml to include RAG service <p>Updated docker-compose configuration to include the RAG service with proper dependencies.</p> </li> <li> Configure volumes and environment variables <p>Set up appropriate volume mounts for data persistence and environment variables for configuration.</p> </li> <li> Test integration with existing Obelisk services <p>Verified integration with Ollama and OpenWebUI, ensuring proper communication between services.</p> </li> <li> Verify end-to-end functionality in containers <p>Successfully tested complete end-to-end functionality in containerized environment.</p> </li> </ul>"},{"location":"chatbot/rag/mvp/#implementation-order","title":"Implementation Order","text":"<ol> <li>First Implementation Tasks:</li> <li> Project structure setup</li> <li> Configuration system</li> <li> <p> Basic document processing</p> </li> <li> <p>Second Implementation Tasks:</p> </li> <li> Embedding generation with Ollama</li> <li> Vector storage with ChromaDB</li> <li> <p> Simple retrieval mechanism</p> </li> <li> <p>Third Implementation Tasks:</p> </li> <li> RAG pipeline with LangChain</li> <li> LLM integration with Ollama</li> <li> <p> Testing with sample questions</p> </li> <li> <p>Final Implementation Tasks:</p> </li> <li> CLI interface</li> <li> API endpoints</li> <li> Docker container setup</li> <li> Integration with existing services</li> </ol>"},{"location":"chatbot/rag/mvp/#mvp-core-components","title":"MVP Core Components","text":""},{"location":"chatbot/rag/mvp/#1-document-processing-pipeline","title":"1. Document Processing Pipeline","text":"<p>MVP Implementation: - [x] Simple directory monitor for the <code>./vault</code> folder using Watchdog - [x] LangChain markdown loader for processing files - [x] Basic chunking strategy with RecursiveCharacterTextSplitter</p> <p>Engineering Notes: - Fixed YAML frontmatter extraction with proper error handling - Implemented metadata filtering to handle complex types - Added configurable chunking parameters - Created robust file watching with debounced event handling</p> <pre><code>from watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nclass MarkdownHandler(FileSystemEventHandler):\n    def on_modified(self, event):\n        if event.src_path.endswith('.md'):\n            process_file(event.src_path)\n\ndef process_file(file_path):\n    loader = TextLoader(file_path)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    chunks = text_splitter.split_documents(documents)\n    # Send to embedding pipeline\n</code></pre> <p>Future Enhancements: - YAML frontmatter parsing - Smart chunking based on markdown structure - Git integration for version-aware processing</p>"},{"location":"chatbot/rag/mvp/#2-embedding-generation","title":"2. Embedding Generation","text":"<p>MVP Implementation: - [x] Use Ollama with mxbai-embed-large (already compatible) - [x] Simple sequential embedding of document chunks</p> <p>Engineering Notes: - Implemented proper error handling for Ollama connection issues - Added configurable embedding model selection - Created optimized batching for efficient processing - Implemented retry mechanism for transient failures</p> <pre><code>from langchain_ollama import OllamaEmbeddings\n\nembeddings = OllamaEmbeddings(\n    model=\"mxbai-embed-large\",\n    base_url=\"http://localhost:11434\"\n)\n\ndef embed_documents(chunks):\n    for chunk in chunks:\n        embedding = embeddings.embed_documents([chunk.page_content])[0]\n        # Store embedding with document in vector store\n</code></pre> <p>Future Enhancements: - Batch processing for performance - Caching to avoid redundant embedding - Alternative model support (Stella, BGE, etc.)</p>"},{"location":"chatbot/rag/mvp/#3-vector-storage","title":"3. Vector Storage","text":"<p>MVP Decision: Chroma - [x] Set up Chroma for simple vector storage - [x] Configure persistence to disk with minimal setup - [x] Implement basic querying functionality</p> <p>Engineering Notes: - Updated from deprecated langchain_community.vectorstores to langchain_chroma - Solved metadata serialization issues by filtering complex types - Implemented backup mechanism for vector database - Added configuration for persistence directory</p> <pre><code>from langchain_community.vectorstores import Chroma\n\ndef store_embeddings(chunks, embeddings_model):\n    vector_store = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings_model,\n        persist_directory=\"./chroma_db\"\n    )\n    vector_store.persist()\n    return vector_store\n</code></pre> <p>Future Enhancement Path: - Migration to Milvus Lite if scale becomes an issue - Advanced indexing configurations - Metadata filtering capabilities</p>"},{"location":"chatbot/rag/mvp/#4-rag-pipeline","title":"4. RAG Pipeline","text":"<p>MVP Implementation: - [x] Simple RetrievalQA chain with LangChain - [x] Basic prompt template for context insertion</p> <p>Engineering Notes: - Optimized prompt engineering for better context utilization - Implemented proper error handling for LLM communication - Added configurable retrieval parameters (k, score threshold) - Created comprehensive logging for tracking query performance</p> <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"\"\"You are an assistant helping with questions about Obelisk documentation.\nUse the following context to answer the question. If you don't know, say so.\n\nContext:\n{context}\n\nQuestion: {question}\nAnswer: \"\"\"\n\nprompt = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\ndef create_qa_chain(vector_store, llm):\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        chain_type_kwargs={\"prompt\": prompt}\n    )\n    return qa_chain\n</code></pre> <p>Future Enhancements: - Maximal Marginal Relevance retrieval - Multi-query retrieval strategies - Query reformulation techniques - Citation and source tracking</p>"},{"location":"chatbot/rag/mvp/#5-llm-integration","title":"5. LLM Integration","text":"<p>MVP Implementation: - [x] Ollama with Llama3 (8B or smaller quantized model) - [x] Simple streaming interface for responses</p> <p>Engineering Notes: - Implemented proper connection handling with Ollama API - Added configurable model parameters (temperature, top_p, etc.) - Created robust error handling for LLM timeout and failure cases - Implemented streaming response capability for both CLI and API</p> <pre><code>from langchain_ollama import Ollama\n\ndef setup_llm():\n    llm = Ollama(\n        model=\"llama3\",  # or llama3:8b\n        temperature=0.1,\n        base_url=\"http://localhost:11434\"\n    )\n    return llm\n</code></pre> <p>Future Enhancements: - Model switching capability (Phi-4, DeepSeek-R1) - Performance optimizations based on hardware - Custom prompt templates per model</p>"},{"location":"chatbot/rag/mvp/#6-user-interface","title":"6. User Interface","text":"<p>MVP Implementation: - [x] Simple CLI interface for direct interaction - [x] Basic FastAPI endpoint for integration with tools</p> <p>Engineering Notes: - Implemented comprehensive CLI commands with argparse - Created well-documented FastAPI endpoints with proper validation - Added JSON output format option for CLI - Implemented detailed error messages and logging - Created helpful usage examples in documentation</p> <pre><code>import argparse\nfrom fastapi import FastAPI, Body\nfrom pydantic import BaseModel\n\n# CLI\ndef setup_cli():\n    parser = argparse.ArgumentParser(description=\"Query your documentation\")\n    parser.add_argument(\"question\", help=\"Your question about the documentation\")\n    return parser\n\n# API\napp = FastAPI()\n\nclass Query(BaseModel):\n    question: str\n\n@app.post(\"/api/query\")\nasync def query_docs(query: Query):\n    response = qa_chain.run(query.question)\n    return {\"response\": response}\n</code></pre> <p>Future Enhancement: - OpenWebUI integration  - Web interface with source citations - Chat history and conversation state</p>"},{"location":"chatbot/rag/mvp/#technical-decisions-for-mvp","title":"Technical Decisions for MVP","text":"<ol> <li>Embedding Model: mxbai-embed-large via Ollama</li> <li>Rationale: Already integrated with Ollama, good performance, simple setup</li> <li> <p>Implementation Note: Successfully integrated with 768-dimensional embeddings, handling ~50 docs/second on standard hardware.</p> </li> <li> <p>Vector Database: Chroma</p> </li> <li>Rationale: Lowest complexity, well-integrated with LangChain, sufficient for thousands of documents</li> <li> <p>Implementation Note: Working well with SQLite backend, efficient for up to 100,000 chunks. Filtering by metadata working as expected.</p> </li> <li> <p>LLM: Llama3 (8B variant) via Ollama</p> </li> <li>Rationale: Good balance of quality and performance on average hardware</li> <li> <p>Implementation Note: Response quality excellent with context, response time averaging 2-5 seconds depending on query complexity.</p> </li> <li> <p>Framework: LangChain core components</p> </li> <li>Rationale: Reduces custom code, well-tested integration patterns</li> <li> <p>Implementation Note: Updated to latest LangChain patterns, avoiding deprecated components. Custom components created where needed.</p> </li> <li> <p>UI Approach: CLI first, simple API for integration</p> </li> <li>Rationale: Fastest path to functional system, defer UI complexity</li> <li>Implementation Note: Both CLI and API implemented with full feature parity. API endpoints documented with OpenAPI.</li> </ol>"},{"location":"chatbot/rag/mvp/#project-structure","title":"Project Structure","text":"<pre><code>obelisk/\n\u251c\u2500\u2500 rag/\n\u2502   \u251c\u2500\u2500 __init__.py        # Package initialization and version info\n\u2502   \u251c\u2500\u2500 config.py          # Configuration management with env vars\n\u2502   \u251c\u2500\u2500 document.py        # Document processing with YAML extraction\n\u2502   \u251c\u2500\u2500 embedding.py       # Embedding generation via Ollama\n\u2502   \u251c\u2500\u2500 storage.py         # ChromaDB vector storage interface\n\u2502   \u251c\u2500\u2500 service.py         # Core RAG service implementation\n\u2502   \u251c\u2500\u2500 api.py             # FastAPI application and endpoints\n\u2502   \u251c\u2500\u2500 cli.py             # Command-line interface with commands\n\u2502   \u2514\u2500\u2500 utils/             # Utility functions and helpers\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 logging.py     # Logging configuration\n\u2502       \u2514\u2500\u2500 validation.py  # Input validation helpers\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 rag/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 conftest.py    # Test fixtures and configuration\n\u2502       \u251c\u2500\u2500 test_config.py # Configuration system tests\n\u2502       \u251c\u2500\u2500 test_document.py # Document processing tests\n\u2502       \u251c\u2500\u2500 test_embedding.py # Embedding generation tests\n\u2502       \u251c\u2500\u2500 test_storage.py # Vector storage tests\n\u2502       \u2514\u2500\u2500 test_service.py # End-to-end service tests\n</code></pre> <p>This implemented structure provides a clean separation of concerns while maintaining good cohesion between related components. The addition of a utils package helps keep the main modules focused on their core responsibilities.</p>"},{"location":"chatbot/rag/mvp/#docker-composition-implemented-for-mvp","title":"Docker Composition (Implemented for MVP)","text":"<pre><code>version: \"3.8\"\n\nservices:\n    ollama:\n        image: ollama/ollama:latest\n        ports:\n            - \"11434:11434\"\n        volumes:\n            - ollama_data:/root/.ollama\n        restart: unless-stopped\n        deploy:\n            resources:\n                reservations:\n                    devices:\n                        - driver: nvidia\n                          count: all\n                          capabilities: [gpu]\n\n    openwebui:\n        image: ghcr.io/open-webui/open-webui:latest\n        depends_on:\n            - ollama\n        ports:\n            - \"8080:8080\"\n        environment:\n            - OLLAMA_BASE_URL=http://ollama:11434\n            - OBELISK_RAG_API_URL=http://rag-api:8000\n        restart: unless-stopped\n        volumes:\n            - openwebui_data:/app/backend/data\n\n    rag-api:\n        build:\n            context: .\n            dockerfile: Dockerfile\n        ports:\n            - \"8000:8000\"\n        environment:\n            - OBELISK_VAULT_DIR=/vault\n            - OBELISK_OLLAMA_URL=http://ollama:11434\n            - OBELISK_OLLAMA_MODEL=llama3\n            - OBELISK_EMBEDDING_MODEL=mxbai-embed-large\n            - OBELISK_RETRIEVE_TOP_K=5\n            - OBELISK_CHROMA_DIR=/app/data/chroma_db\n            - OBELISK_LOG_LEVEL=INFO\n        volumes:\n            - ./vault:/vault:ro\n            - rag_data:/app/data\n        depends_on:\n            - ollama\n        restart: unless-stopped\n        healthcheck:\n            test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n            interval: 30s\n            timeout: 10s\n            retries: 3\n            start_period: 10s\n\n    obelisk:\n        build:\n            context: .\n            dockerfile: Dockerfile\n        ports:\n            - \"8888:8000\"\n        volumes:\n            - ./vault:/app/vault\n            - ./mkdocs.yml:/app/mkdocs.yml\n        command: [\"mkdocs\", \"serve\", \"--dev-addr=0.0.0.0:8000\"]\n        restart: unless-stopped\n\nvolumes:\n    ollama_data:\n    rag_data:\n    openwebui_data:\n</code></pre> <p>The implemented Docker Compose configuration includes:</p> <ol> <li>GPU support for Ollama when available</li> <li>Health checks for the RAG API service</li> <li>Proper environment variable naming with OBELISK_ prefix</li> <li>Read-only mounts for security where appropriate</li> <li>Volume persistence for all data</li> <li>Integration between OpenWebUI and the RAG API</li> <li>MkDocs server for the documentation website</li> </ol>"},{"location":"chatbot/rag/mvp/#example-implementation","title":"Example Implementation","text":"<pre><code># rag_pipeline.py\nimport os\nimport glob\nfrom typing import List, Dict, Any\nimport argparse\nimport time\n\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_ollama import OllamaEmbeddings, Ollama\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom fastapi import FastAPI, Body\nfrom pydantic import BaseModel\nimport uvicorn\n\n# Configuration\nVAULT_DIR = os.environ.get(\"VAULT_DIR\", \"./vault\")\nOLLAMA_URL = os.environ.get(\"OLLAMA_URL\", \"http://localhost:11434\")\nOLLAMA_MODEL = os.environ.get(\"OLLAMA_MODEL\", \"llama3\")\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"mxbai-embed-large\")\nCHROMA_DIR = os.environ.get(\"CHROMA_DIR\", \"./chroma_db\")\nCHUNK_SIZE = int(os.environ.get(\"CHUNK_SIZE\", \"1000\"))\nCHUNK_OVERLAP = int(os.environ.get(\"CHUNK_OVERLAP\", \"200\"))\nRETRIEVE_TOP_K = int(os.environ.get(\"RETRIEVE_TOP_K\", \"3\"))\n\n# Initialize components\nembeddings_model = OllamaEmbeddings(\n    model=EMBEDDING_MODEL,\n    base_url=OLLAMA_URL\n)\n\nllm = Ollama(\n    model=OLLAMA_MODEL,\n    temperature=0.1,\n    base_url=OLLAMA_URL\n)\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP\n)\n\n# Global vector store\nvector_store = None\n\ndef initialize_vector_store():\n    \"\"\"Initialize or load the vector store\"\"\"\n    global vector_store\n    if os.path.exists(CHROMA_DIR):\n        print(f\"Loading existing vector store from {CHROMA_DIR}\")\n        vector_store = Chroma(\n            persist_directory=CHROMA_DIR,\n            embedding_function=embeddings_model\n        )\n    else:\n        print(f\"Creating new vector store at {CHROMA_DIR}\")\n        vector_store = Chroma(\n            embedding_function=embeddings_model,\n            persist_directory=CHROMA_DIR\n        )\n        # Process all existing documents\n        process_all_files()\n\ndef process_file(file_path: str):\n    \"\"\"Process a single markdown file\"\"\"\n    print(f\"Processing file: {file_path}\")\n    try:\n        loader = TextLoader(file_path)\n        documents = loader.load()\n\n        # Add source metadata\n        for doc in documents:\n            doc.metadata[\"source\"] = file_path\n\n        # Split into chunks\n        chunks = text_splitter.split_documents(documents)\n\n        # Add or update in vector store\n        vector_store.add_documents(chunks)\n        vector_store.persist()\n        print(f\"Added {len(chunks)} chunks from {file_path}\")\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n\ndef process_all_files():\n    \"\"\"Process all markdown files in the vault directory\"\"\"\n    print(f\"Processing all files in {VAULT_DIR}\")\n    for md_file in glob.glob(f\"{VAULT_DIR}/**/*.md\", recursive=True):\n        process_file(md_file)\n\n# Create RAG chain\ndef create_qa_chain():\n    \"\"\"Create the question-answering chain\"\"\"\n    prompt_template = \"\"\"You are an assistant helping with questions about the Obelisk documentation.\nUse ONLY the following context to answer the question. If the information isn't in the context, say you don't know.\n\nContext:\n{context}\n\nQuestion: {question}\nAnswer: \"\"\"\n\n    prompt = PromptTemplate(\n        template=prompt_template,\n        input_variables=[\"context\", \"question\"]\n    )\n\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": RETRIEVE_TOP_K})\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        chain_type_kwargs={\"prompt\": prompt}\n    )\n    return qa_chain\n\n# File watcher\nclass MarkdownHandler(FileSystemEventHandler):\n    def on_modified(self, event):\n        if not event.is_directory and event.src_path.endswith('.md'):\n            process_file(event.src_path)\n\n    def on_created(self, event):\n        if not event.is_directory and event.src_path.endswith('.md'):\n            process_file(event.src_path)\n\ndef start_file_watcher():\n    \"\"\"Start watching for file changes in the vault directory\"\"\"\n    event_handler = MarkdownHandler()\n    observer = Observer()\n    observer.schedule(event_handler, VAULT_DIR, recursive=True)\n    observer.start()\n    return observer\n\n# CLI Interface\ndef run_cli():\n    \"\"\"Run the CLI interface\"\"\"\n    initialize_vector_store()\n    qa_chain = create_qa_chain()\n\n    parser = argparse.ArgumentParser(description=\"Query your Obelisk documentation\")\n    parser.add_argument(\"question\", help=\"Your question about the documentation\")\n    args = parser.parse_args()\n\n    answer = qa_chain.run(args.question)\n    print(f\"\\nQ: {args.question}\\n\")\n    print(f\"A: {answer}\")\n\n# API Interface\napp = FastAPI(title=\"Obelisk RAG API\")\n\nclass Query(BaseModel):\n    question: str\n\n@app.post(\"/api/query\")\nasync def query_docs(query: Query):\n    \"\"\"API endpoint to query the documentation\"\"\"\n    global qa_chain\n    response = qa_chain.run(query.question)\n    return {\"response\": response}\n\n@app.post(\"/api/reindex\")\nasync def reindex():\n    \"\"\"API endpoint to trigger reindexing of all documents\"\"\"\n    process_all_files()\n    return {\"status\": \"success\", \"message\": \"Reindexing complete\"}\n\n# Main entry point\nif __name__ == \"__main__\":\n    # Check if running as CLI or API\n    if len(os.sys.argv) &gt; 1:\n        # CLI mode\n        run_cli()\n    else:\n        # API mode\n        print(\"Starting Obelisk RAG API server...\")\n        initialize_vector_store()\n        qa_chain = create_qa_chain()\n\n        # Start file watcher\n        observer = start_file_watcher()\n\n        try:\n            # Start API server\n            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n        finally:\n            observer.stop()\n            observer.join()\n</code></pre>"},{"location":"chatbot/rag/mvp/#dockerfilerag","title":"Dockerfile.rag","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Copy poetry files\nCOPY pyproject.toml poetry.lock* /app/\n\n# Poetry already installed in the devcontainer\nRUN poetry config virtualenvs.create false &amp;&amp; \\\n    poetry install --no-dev\n\n# Copy application\nCOPY . /app/\n\n# Environment variables\nENV VAULT_DIR=/vault\nENV CHROMA_DIR=/app/data/chroma_db\nENV OLLAMA_URL=http://ollama:11434\nENV OLLAMA_MODEL=llama3\nENV EMBEDDING_MODEL=mxbai-embed-large\n\n# Expose port\nEXPOSE 8000\n\n# Run the application\nENTRYPOINT [\"python\", \"-m\", \"obelisk.rag.api\"]\n</code></pre> <p>By focusing on these core components, we can quickly build a functional MVP that demonstrates the power of local RAG while setting the foundation for all the advanced features in the future roadmap.</p>"},{"location":"chatbot/rag/ollama-integration/","title":"Ollama Integration","text":"<p>This page details how the RAG pipeline will integrate with Ollama to provide context-aware responses.</p>"},{"location":"chatbot/rag/ollama-integration/#integration-architecture","title":"Integration Architecture","text":"<p>The RAG pipeline will interact with Ollama through its API:</p> <pre><code>sequenceDiagram\n    participant User\n    participant OpenWebUI as Open WebUI\n    participant RAGMiddleware as RAG Middleware\n    participant VectorDB as Vector Database\n    participant Ollama\n\n    User-&gt;&gt;OpenWebUI: Ask question\n    OpenWebUI-&gt;&gt;RAGMiddleware: Forward query\n    RAGMiddleware-&gt;&gt;VectorDB: Retrieve context\n    VectorDB--&gt;&gt;RAGMiddleware: Return relevant docs\n    RAGMiddleware-&gt;&gt;Ollama: Send query + context\n    Ollama--&gt;&gt;OpenWebUI: Return enhanced response\n    OpenWebUI--&gt;&gt;User: Display response</code></pre>"},{"location":"chatbot/rag/ollama-integration/#ollama-api-interaction","title":"Ollama API Interaction","text":""},{"location":"chatbot/rag/ollama-integration/#model-generation-endpoint","title":"Model Generation Endpoint","text":"<p>The RAG pipeline will use Ollama's generation API:</p> <pre><code>POST /api/generate HTTP/1.1\nHost: localhost:11434\nContent-Type: application/json\n\n{\n  \"model\": \"mistral\",\n  \"prompt\": \"System: You are a helpful assistant. Use the following context to answer the question.\\n\\nContext: {retrieved_context}\\n\\nQuestion: {user_question}\",\n  \"stream\": false,\n  \"options\": {\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"num_ctx\": 4096\n  }\n}\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#context-formatting","title":"Context Formatting","text":"<p>The context will be formatted before being sent to Ollama:</p> <pre><code># Future implementation example\ndef format_context_for_ollama(retrieved_chunks):\n    \"\"\"Format retrieved chunks into a context string for Ollama.\"\"\"\n    formatted_chunks = []\n\n    for i, chunk in enumerate(retrieved_chunks):\n        formatted_chunks.append(\n            f\"[Document {i+1}] {chunk['metadata']['source']}\\n\"\n            f\"---\\n\"\n            f\"{chunk['text']}\\n\"\n        )\n\n    return \"\\n\\n\".join(formatted_chunks)\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#custom-modelfiles","title":"Custom Modelfiles","text":"<p>Obelisk will provide custom Modelfiles optimized for RAG:</p> <pre><code>FROM mistral:latest\n\n# Optimize for RAG\nPARAMETER num_ctx 8192\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER top_k 40\n\n# System instruction for RAG\nSYSTEM You are a helpful documentation assistant for the Obelisk documentation system.\nSYSTEM When given context from the documentation, use this information to answer the user's question.\nSYSTEM Always attribute your sources and only provide information contained in the given context.\nSYSTEM If the information can't be found in the context, acknowledge this and suggest where the user might find more information.\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#embedding-models","title":"Embedding Models","text":"<p>The RAG pipeline will use embedding models via Ollama:</p> Model Description Size Performance nomic-embed-text General text embeddings 137M High quality mxbai-embed-large Multilingual embeddings 137M Multilingual support all-mxbai-embed-large Specialized code embeddings 137M Code-focused <p>Example embedding request:</p> <pre><code>POST /api/embeddings HTTP/1.1\nHost: localhost:11434\nContent-Type: application/json\n\n{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The text to be embedded\"\n}\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#memory-management","title":"Memory Management","text":"<p>When dealing with limited resources:</p> <ol> <li>Context pruning: Dynamically adjust context size based on available memory</li> <li>Quantization selection: Use higher quantization levels (Q4_0) for large models</li> <li>Context batching: Process chunks in batches for large documents</li> <li>Model swapping: Automatically switch between models based on query complexity</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#response-processing","title":"Response Processing","text":"<p>After receiving responses from Ollama:</p> <ol> <li>Citation extraction: Identify and format source citations</li> <li>Response validation: Verify that the response uses the provided context</li> <li>Confidence scoring: Assess confidence in the generated response</li> <li>Metadata enrichment: Add metadata about sources used</li> </ol> <p>Example response processing:</p> <pre><code># Future implementation example\ndef process_ollama_response(response, retrieved_chunks):\n    \"\"\"Process and enhance response from Ollama.\"\"\"\n    text = response[\"response\"]\n\n    # Extract and verify citations\n    citations = extract_citations(text)\n    valid_citations = verify_citations(citations, retrieved_chunks)\n\n    # Format citations and append source information\n    enhanced_response = format_response_with_citations(text, valid_citations)\n\n    # Add metadata\n    metadata = {\n        \"sources_used\": [chunk[\"metadata\"][\"source\"] for chunk in retrieved_chunks],\n        \"confidence_score\": calculate_confidence(text, retrieved_chunks),\n        \"model_used\": response[\"model\"],\n        \"response_time\": response[\"total_duration\"] / 1000000000  # Convert ns to s\n    }\n\n    return {\n        \"response\": enhanced_response,\n        \"metadata\": metadata\n    }\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#performance-optimization","title":"Performance Optimization","text":"<p>To optimize performance with Ollama:</p> <ol> <li>Batched embeddings: Process multiple chunks in a single API call</li> <li>Connection pooling: Maintain persistent connections</li> <li>Response streaming: Stream responses for faster initial display</li> <li>Caching layer: Cache common queries and embeddings</li> <li>Load balancing: Support multiple Ollama instances for high demand</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#fallback-mechanisms","title":"Fallback Mechanisms","text":"<p>The system will include fallbacks when Ollama cannot provide good answers:</p> <ol> <li>General knowledge fallback: Use model's general knowledge when no context is found</li> <li>Search suggestions: Recommend search terms for more relevant results</li> <li>Documentation navigation: Suggest navigation paths in the documentation</li> <li>Human handoff: Provide contact information for human assistance</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#security-considerations","title":"Security Considerations","text":"<p>Important security aspects of the Ollama integration:</p> <ol> <li>Input validation: Sanitize all inputs to prevent prompt injection</li> <li>Rate limiting: Prevent abuse with rate limits</li> <li>Output filtering: Filter sensitive information from responses</li> <li>Network isolation: Restrict network access to the Ollama API</li> <li>Authentication: Add optional authentication for API access</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#advanced-models","title":"Advanced Models","text":"<p>Obelisk's RAG system supports various advanced models through Ollama integration, each offering different capabilities and performance characteristics.</p>"},{"location":"chatbot/rag/ollama-integration/#recommended-models","title":"Recommended Models","text":"Model Size Use Case Performance Llama3 8B General purpose, balanced Good general performance Phi-3-mini 3.8B Lightweight, efficient Excellent for resource-constrained systems Mistral 7B Technical content Strong reasoning capabilities Mixtral 8x7B Complex questions High quality with MoE architecture DeepSeek Coder 6.7B Code-focused Excellent for developer documentation"},{"location":"chatbot/rag/ollama-integration/#model-configuration","title":"Model Configuration","text":"<p>Advanced models can be configured in various ways:</p> <pre><code># Pull recommended models\nollama pull llama3\nollama pull phi:latest\nollama pull mistral\nollama pull mixtral\nollama pull deepseek-coder\n\n# Configure Obelisk to use a specific model\nexport OLLAMA_MODEL=\"mixtral\"\nobelisk-rag config --set \"ollama_model=mixtral\"\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#custom-model-parameters","title":"Custom Model Parameters","text":"<p>For advanced users, Obelisk allows customizing model parameters:</p> <pre><code># Example future configuration\nrag:\n  ollama:\n    model: \"llama3\"\n    parameters:\n      temperature: 0.1\n      top_p: 0.9\n      top_k: 40\n      num_ctx: 8192\n      repeat_penalty: 1.1\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#multi-model-strategy","title":"Multi-Model Strategy","text":"<p>The RAG system can use different models for different types of queries:</p> <ol> <li>Technical questions: Use models like Mistral or DeepSeek Coder</li> <li>General questions: Use Llama3 or Phi-3</li> <li>Complex reasoning: Use Mixtral or larger variants</li> </ol> <p>To implement a multi-model strategy, you can use the model selection feature:</p> <pre><code># Different models for different queries\nobelisk-rag query \"How do I use Docker with Obelisk?\" --model deepseek-coder\nobelisk-rag query \"Explain the architecture of Obelisk\" --model mixtral\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For optimal performance with advanced models, GPU acceleration is recommended:</p> <ol> <li>NVIDIA GPUs: Fully supported through CUDA</li> <li>AMD GPUs: Experimental support through ROCm</li> <li>Metal (Apple Silicon): Native support on Mac</li> </ol> <p>Configure GPU usage with:</p> <pre><code># Enable GPU acceleration in docker-compose.yml\nservices:\n  ollama:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/","title":"Query Pipeline","text":"<p>The query pipeline is a critical component of the Obelisk RAG system, processing user questions and retrieving relevant content to enhance AI responses.</p>"},{"location":"chatbot/rag/query-pipeline/#query-flow-architecture","title":"Query Flow Architecture","text":"<p>The query pipeline follows these steps:</p> <pre><code>sequenceDiagram\n    participant User\n    participant QueryProcessor\n    participant VectorDB\n    participant PromptEngine\n    participant Ollama\n\n    User-&gt;&gt;QueryProcessor: Ask question\n    QueryProcessor-&gt;&gt;QueryProcessor: Preprocess query\n    QueryProcessor-&gt;&gt;VectorDB: Generate embedding\n    VectorDB-&gt;&gt;QueryProcessor: Return similar chunks\n    QueryProcessor-&gt;&gt;PromptEngine: Pass query + chunks\n    PromptEngine-&gt;&gt;Ollama: Send enhanced prompt\n    Ollama-&gt;&gt;User: Return augmented response</code></pre>"},{"location":"chatbot/rag/query-pipeline/#query-preprocessing","title":"Query Preprocessing","text":"<p>Before retrieval, queries undergo several preprocessing steps:</p> <ol> <li>Query expansion: Enhance queries with related terms</li> <li>Intent recognition: Identify the type of question</li> <li>Metadata extraction: Extract filters from the query</li> <li>Language detection: Handle multilingual queries</li> <li>Query rewriting: Optimize for retrieval performance</li> </ol> <p>Example implementation:</p> <pre><code># Future implementation example\ndef preprocess_query(query_text):\n    \"\"\"Preprocess user query for optimal retrieval.\"\"\"\n    # Clean and normalize text\n    cleaned_query = clean_text(query_text)\n\n    # Extract potential filters\n    filters = extract_metadata_filters(cleaned_query)\n\n    # Expand query with related terms\n    expanded_query = query_expansion(cleaned_query)\n\n    return {\n        \"original_query\": query_text,\n        \"processed_query\": expanded_query,\n        \"filters\": filters,\n        \"detected_intent\": detect_intent(cleaned_query)\n    }\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#retrieval-strategies","title":"Retrieval Strategies","text":"<p>The pipeline will implement multiple retrieval strategies:</p>"},{"location":"chatbot/rag/query-pipeline/#1-dense-retrieval","title":"1. Dense Retrieval","text":"<p>Using vector similarity to find relevant content:</p> <ul> <li>Embedding space: Convert query to the same embedding space as documents</li> <li>Similarity metrics: Cosine similarity, dot product, or Euclidean distance</li> <li>Top-k retrieval: Return the k most similar chunks</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#2-hybrid-search","title":"2. Hybrid Search","text":"<p>Combining multiple search techniques:</p> <ul> <li>BM25 keyword search: Traditional information retrieval</li> <li>Dense vector search: Semantic similarity</li> <li>Fusion methods: Reciprocal rank fusion or weighted combinations</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#3-multi-stage-retrieval","title":"3. Multi-stage Retrieval","text":"<p>A two-step process for better results:</p> <ul> <li>Initial retrieval: Get a larger set of potentially relevant chunks</li> <li>Re-ranking: Apply more complex models to re-rank results</li> <li>Diversity optimization: Ensure varied context</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#context-assembly","title":"Context Assembly","text":"<p>Retrieved chunks are assembled into a coherent context:</p> <ol> <li>Chunk sorting: Order by relevance and document structure</li> <li>Deduplication: Remove redundant information</li> <li>Context limitation: Fit within model context window</li> <li>Metadata inclusion: Add source information</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#prompt-engineering","title":"Prompt Engineering","text":"<p>Crafting effective prompts is essential for quality responses:</p>"},{"location":"chatbot/rag/query-pipeline/#basic-rag-prompt-template","title":"Basic RAG Prompt Template","text":"<pre><code>You are an assistant for the Obelisk documentation.\nAnswer the question based ONLY on the following context:\n\n{retrieved_context}\n\nQuestion: {user_question}\n\nAnswer:\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#advanced-rag-prompt-template","title":"Advanced RAG Prompt Template","text":"<pre><code>You are an assistant for the Obelisk documentation system.\nUse ONLY the following retrieved documentation to answer the user's question.\nIf the information is not in the retrieved docs, acknowledge that and suggest where they might find the information.\n\nRetrieved documentation:\n{retrieved_chunks}\n\nUser question: {user_question}\n\nRespond in a helpful, concise manner. Include code examples if relevant.\nAlways cite your sources using the document names provided in the retrieved chunks.\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#customizing-prompts","title":"Customizing Prompts","text":"<p>The Obelisk RAG system allows for prompt template customization to better fit specific use cases:</p>"},{"location":"chatbot/rag/query-pipeline/#template-variables","title":"Template Variables","text":"<p>You can use the following variables in your prompt templates:</p> <ul> <li><code>{user_question}</code>: The original question asked by the user</li> <li><code>{retrieved_context}</code>: The full context assembled from retrieved chunks</li> <li><code>{retrieved_chunks}</code>: An array of individual content chunks with metadata</li> <li><code>{chunk_count}</code>: The number of chunks retrieved</li> <li><code>{confidence_score}</code>: The confidence score of the retrieval</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#custom-prompt-configuration","title":"Custom Prompt Configuration","text":"<p>Prompt templates can be customized through environment variables or the configuration API:</p> <pre><code># Set a custom prompt template\nexport OBELISK_PROMPT_TEMPLATE=\"You are an Obelisk expert. Use the following information to answer the question:\\n\\n{retrieved_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n\n# Or using the config API\nobelisk-rag config --set \"prompt_template=You are an Obelisk expert. Use the following information to answer the question:\\n\\n{retrieved_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#advanced-prompt-engineering-techniques","title":"Advanced Prompt Engineering Techniques","text":"<p>For optimal RAG performance, consider these prompt engineering practices:</p> <ol> <li>Clear instructions: Include specific instructions on how to use the context</li> <li>Context formatting: Format the context for better readability by the model</li> <li>Response formatting: Specify desired response format (bullets, paragraphs, etc.)</li> <li>Source attribution: Instruct the model to cite sources from the retrieved chunks</li> <li>Fallback handling: Guide how to respond when information is not in the context</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#response-generation","title":"Response Generation","text":"<p>The final step involves:</p> <ol> <li>Model invocation: Send the assembled prompt to Ollama</li> <li>Parameter optimization: Adjust temperature, top_p, etc.</li> <li>Citation tracking: Maintain source references</li> <li>Response validation: Ensure factuality and relevance</li> <li>Fallback strategies: Handle cases with no relevant context</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#measuring-effectiveness","title":"Measuring Effectiveness","text":"<p>The RAG pipeline will include evaluation metrics:</p> <ul> <li>Retrieval precision/recall: Measure retrieval quality</li> <li>Answer relevance: Assess response relevance</li> <li>Factual accuracy: Verify factual correctness</li> <li>Citation accuracy: Check if sources are properly cited</li> <li>User satisfaction: Collect user feedback</li> </ul>"},{"location":"chatbot/rag/using-rag/","title":"Using the Obelisk RAG System","text":"<p>This guide provides step-by-step instructions for setting up and using the Retrieval Augmented Generation (RAG) system in Obelisk. The RAG system enhances your chatbot by providing it with contextual information from your documentation.</p>"},{"location":"chatbot/rag/using-rag/#quick-start","title":"Quick Start","text":"<p>The RAG system is accessible through the <code>obelisk-rag</code> command-line tool. Here's how to get started:</p> <pre><code># Index your documentation\nobelisk-rag index\n\n# Query the system\nobelisk-rag query \"What is Obelisk?\"\n\n# Start the API server\nobelisk-rag serve --watch\n</code></pre>"},{"location":"chatbot/rag/using-rag/#installation","title":"Installation","text":"<p>The RAG system is included with Obelisk. If you've already installed Obelisk using Poetry, you have everything you need:</p> <pre><code># Verify installation\npoetry run obelisk-rag --help\n</code></pre> <p>If you're using Docker, the RAG system is available in the Obelisk container:</p> <pre><code>docker-compose up obelisk\ndocker exec -it obelisk obelisk-rag --help\n</code></pre>"},{"location":"chatbot/rag/using-rag/#setup-and-configuration","title":"Setup and Configuration","text":""},{"location":"chatbot/rag/using-rag/#basic-configuration","title":"Basic Configuration","text":"<p>By default, the RAG system is configured to:</p> <ol> <li>Read documentation from the <code>./vault</code> directory</li> <li>Store vector embeddings in <code>./.obelisk/vectordb</code></li> <li>Connect to Ollama at <code>http://localhost:11434</code></li> <li>Use <code>llama3</code> as the LLM and <code>mxbai-embed-large</code> as the embedding model</li> </ol> <p>You can view the current configuration with:</p> <pre><code>obelisk-rag config --show\n</code></pre>"},{"location":"chatbot/rag/using-rag/#custom-configuration","title":"Custom Configuration","text":"<p>You can customize the configuration using:</p> <ol> <li>Environment variables:</li> </ol> <pre><code># Set the vault directory\nexport VAULT_DIR=\"/path/to/your/docs\"\n\n# Set the Ollama URL\nexport OLLAMA_URL=\"http://ollama:11434\"\n\n# Set the models\nexport OLLAMA_MODEL=\"mistral\"\nexport EMBEDDING_MODEL=\"mxbai-embed-large\"\n\n# Set chunking parameters\nexport CHUNK_SIZE=\"1500\"\nexport CHUNK_OVERLAP=\"200\"\n\n# Set retrieval parameters\nexport RETRIEVE_TOP_K=\"5\"\n\n# Set API settings\nexport API_HOST=\"0.0.0.0\"\nexport API_PORT=\"8000\"\n</code></pre> <ol> <li>Command-line configuration:</li> </ol> <pre><code># Set a configuration value\nobelisk-rag config --set \"vault_dir=/path/to/your/docs\"\nobelisk-rag config --set \"ollama_model=mistral\"\n</code></pre> <ol> <li>Command-specific options:</li> </ol> <pre><code># Specify vault directory for indexing\nobelisk-rag index --vault /path/to/your/docs\n\n# Specify API host and port\nobelisk-rag serve --host 0.0.0.0 --port 9000\n</code></pre>"},{"location":"chatbot/rag/using-rag/#indexing-your-documentation","title":"Indexing Your Documentation","text":"<p>Before you can query your documentation, you need to index it. This process:</p> <ol> <li>Reads all markdown files in your vault</li> <li>Extracts content and metadata</li> <li>Chunks the content into appropriate segments</li> <li>Generates embeddings for each chunk</li> <li>Stores the embeddings in a vector database</li> </ol> <p>To index your documentation:</p> <pre><code># Index using the default vault directory\nobelisk-rag index\n\n# Index a specific directory\nobelisk-rag index --vault /path/to/your/docs\n</code></pre> <p>The indexing process might take some time depending on the size of your documentation. Progress will be displayed in the console.</p>"},{"location":"chatbot/rag/using-rag/#querying-the-system","title":"Querying the System","text":"<p>Once your documentation is indexed, you can query it:</p> <pre><code># Ask a question\nobelisk-rag query \"How do I customize the theme?\"\n\n# Get JSON output\nobelisk-rag query \"What is the configuration format?\" --json\n</code></pre> <p>The system will:</p> <ol> <li>Convert your query to an embedding</li> <li>Find the most relevant document chunks</li> <li>Include those chunks as context for the LLM</li> <li>Generate a response based on the documentation</li> </ol> <p>The output includes: - The query - The generated response - The sources of information used</p>"},{"location":"chatbot/rag/using-rag/#starting-the-api-server","title":"Starting the API Server","text":"<p>For integration with applications, you can start the RAG API server:</p> <pre><code># Start the server\nobelisk-rag serve\n\n# Start the server with document watching\nobelisk-rag serve --watch\n\n# Specify host and port\nobelisk-rag serve --host 0.0.0.0 --port 9000\n</code></pre> <p>The <code>--watch</code> flag enables real-time document monitoring, so changes to your documentation will be automatically indexed.</p>"},{"location":"chatbot/rag/using-rag/#api-endpoints","title":"API Endpoints","text":"<p>The API server provides the following endpoints:</p> <ol> <li>GET /stats</li> <li>Returns statistics about the RAG system</li> <li> <p>Example: <code>curl http://localhost:8000/stats</code></p> </li> <li> <p>POST /query</p> </li> <li>Processes a query using the RAG system</li> <li>Example:      <pre><code>curl -X POST http://localhost:8000/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"What is Obelisk?\"}'\n</code></pre></li> </ol>"},{"location":"chatbot/rag/using-rag/#integration-with-open-webui","title":"Integration with Open WebUI","text":"<p>You can integrate the RAG API with Open WebUI to enhance the chat interface with your documentation.</p>"},{"location":"chatbot/rag/using-rag/#setup","title":"Setup","text":"<ol> <li> <p>Ensure the RAG API server is running:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>In Open WebUI, add a new API-based model:</p> </li> <li>Name: \"Obelisk RAG\"</li> <li>Base URL: \"http://localhost:8000\"</li> <li>API Path: \"/query\"</li> <li>Request Format:      <pre><code>{\n  \"query\": \"{prompt}\"\n}\n</code></pre></li> <li> <p>Response Path: \"response\"</p> </li> <li> <p>Select the \"Obelisk RAG\" model in the chat interface</p> </li> </ol> <p>Now your chat interface will provide responses based on your documentation!</p>"},{"location":"chatbot/rag/using-rag/#advanced-features","title":"Advanced Features","text":""},{"location":"chatbot/rag/using-rag/#vector-database-management","title":"Vector Database Management","text":"<p>You can view statistics about the vector database:</p> <pre><code># View database stats\nobelisk-rag stats\n\n# View stats in JSON format\nobelisk-rag stats --json\n</code></pre>"},{"location":"chatbot/rag/using-rag/#document-watching","title":"Document Watching","text":"<p>The RAG system can watch for changes to your documentation and update the index in real-time:</p> <pre><code># Start the API server with document watching\nobelisk-rag serve --watch\n</code></pre> <p>This is useful during development when you're actively updating your documentation.</p>"},{"location":"chatbot/rag/using-rag/#debugging","title":"Debugging","text":"<p>If you encounter issues, you can enable debug mode:</p> <pre><code># Enable debug mode\nexport RAG_DEBUG=1\nobelisk-rag query \"Why isn't this working?\"\n</code></pre>"},{"location":"chatbot/rag/using-rag/#architecture","title":"Architecture","text":"<p>The Obelisk RAG system consists of several components:</p> <ol> <li>Document Processor: Handles parsing and chunking of markdown files</li> <li>Embedding Service: Generates vector embeddings using Ollama</li> <li>Vector Storage: Stores and retrieves embeddings using ChromaDB</li> <li>RAG Service: Integrates all components for document retrieval and generation</li> <li>CLI Interface: Provides command-line access to the system</li> </ol> <p>For more details on the architecture, see Architecture Draft and Implementation.</p>"},{"location":"chatbot/rag/using-rag/#next-steps","title":"Next Steps","text":"<p>Now that you have the RAG system set up, you might want to:</p> <ol> <li>Customize the prompt template</li> <li>Integrate with other vector databases</li> <li>Evaluate and improve retrieval quality</li> <li>Explore advanced Ollama models</li> </ol>"},{"location":"chatbot/rag/vector-database/","title":"Vector Database Integration","text":"<p>The Obelisk RAG pipeline will require a vector database to store and query document embeddings. This page outlines the planned implementation details.</p>"},{"location":"chatbot/rag/vector-database/#vector-database-options","title":"Vector Database Options","text":"<p>Several vector database options are being evaluated for integration with Obelisk:</p> Database Description Deployment Chroma Lightweight, open-source Embedded or server FAISS Meta's efficient similarity search In-memory Qdrant Scalable vector search engine Docker container Weaviate Knowledge graph + vector search Docker container Milvus Distributed vector database Kubernetes or Docker <p>The initial implementation will likely use ChromaDB for its simplicity and easy integration.</p>"},{"location":"chatbot/rag/vector-database/#embedding-models","title":"Embedding Models","text":"<p>Document chunks will be converted to vector embeddings using:</p> <ul> <li>Local embedding models via Ollama (e.g., <code>nomic-embed-text</code>)</li> <li>Optional integration with OpenAI embeddings for higher quality</li> </ul>"},{"location":"chatbot/rag/vector-database/#database-schema","title":"Database Schema","text":"<p>The vector database will store:</p> <ol> <li>Document embeddings: Vector representations of content chunks</li> <li>Metadata: Structured information about each chunk</li> <li>Document content: The original text for retrieval</li> </ol> <p>Example schema:</p> <pre><code>{\n  \"embedding\": [0.123, 0.456, ...],  // Vector embedding\n  \"metadata\": {\n    \"source\": \"development/docker.md\",\n    \"title\": \"Docker Configuration\",\n    \"heading_path\": [\"Docker Configuration\", \"Docker Compose Configuration\"],\n    \"last_updated\": \"2023-04-15\",\n    \"tags\": [\"docker\", \"configuration\"],\n    \"chunk_id\": \"dev-docker-compose-001\"\n  },\n  \"text\": \"The `docker-compose.yaml` file orchestrates the complete Obelisk stack, including optional AI components...\"\n}\n</code></pre>"},{"location":"chatbot/rag/vector-database/#storage-persistence","title":"Storage &amp; Persistence","text":"<p>The vector database will be stored in a configurable location:</p> <ul> <li>Default: <code>./.obelisk/vectordb/</code></li> <li>Configurable via environment variables or config file</li> <li>Docker volume mount for containerized deployments</li> </ul>"},{"location":"chatbot/rag/vector-database/#query-optimization","title":"Query Optimization","text":"<p>Several techniques will optimize retrieval performance:</p> <ol> <li>Filtering: Pre-filter by metadata before vector search</li> <li>Hybrid search: Combine keyword and semantic search</li> <li>Re-ranking: Two-stage retrieval with initial recall and re-ranking</li> <li>Caching: Frequently accessed embeddings and results</li> </ol>"},{"location":"chatbot/rag/vector-database/#integration-points","title":"Integration Points","text":"<p>The vector database will integrate with:</p> <ul> <li>Obelisk build process: Generate embeddings during site building</li> <li>Incremental updates: Update embeddings when content changes</li> <li>Ollama API: Connect with the model server for retrieval</li> <li>Open WebUI: Provide context to the chat interface</li> </ul>"},{"location":"chatbot/rag/vector-database/#configuration-options","title":"Configuration Options","text":"<p>Users will be able to configure the vector database through:</p> <pre><code># Example future configuration (mkdocs.yml)\nplugins:\n  - obelisk-rag:\n      embedding_model: nomic-embed-text\n      vector_db: chroma\n      vector_db_path: ./.obelisk/vectordb\n      chunk_size: 512\n      chunk_overlap: 128\n      filter_private_content: true\n      incremental_updates: true\n</code></pre>"},{"location":"chatbot/rag/vector-database/#performance-considerations","title":"Performance Considerations","text":"<p>Database performance will be optimized for different deployment scenarios:</p> <ul> <li>Small sites: In-memory vector database</li> <li>Medium sites: Local persistent database</li> <li>Large sites: Dedicated database service</li> <li>Multi-user deployments: Shared database with caching</li> </ul>"},{"location":"chatbot/rag/vector-database/#alternative-databases","title":"Alternative Databases","text":"<p>While ChromaDB is the default vector database for Obelisk's RAG implementation, several alternatives can be considered for different use cases:</p>"},{"location":"chatbot/rag/vector-database/#qdrant","title":"Qdrant","text":"<p>Benefits for Obelisk: - High-performance search with HNSW algorithm - Powerful filtering capabilities - Cloud-hosted or self-hosted options - Strong scaling capabilities</p> <p>Integration Example: <pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.http import models\n\n# Initialize Qdrant client\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Create collection for Obelisk embeddings\nclient.create_collection(\n    collection_name=\"obelisk_docs\",\n    vectors_config=models.VectorParams(\n        size=768,  # Embedding dimensions\n        distance=models.Distance.COSINE\n    )\n)\n\n# Store embeddings\nclient.upload_points(\n    collection_name=\"obelisk_docs\",\n    points=[\n        models.PointStruct(\n            id=chunk_id,\n            vector=embedding,\n            payload={\"text\": text, \"metadata\": metadata}\n        )\n        for chunk_id, embedding, text, metadata in zip(ids, embeddings, texts, metadatas)\n    ]\n)\n</code></pre></p>"},{"location":"chatbot/rag/vector-database/#milvus","title":"Milvus","text":"<p>Benefits for Obelisk: - Cloud-native architecture - Handles billions of vectors - Excellent for large documentation sites - Advanced query capabilities</p> <p>When to choose Milvus: - Your documentation exceeds 100,000 pages - You need multi-tenant isolation - You require complex metadata filtering - Enterprise deployment with high availability requirements</p>"},{"location":"chatbot/rag/vector-database/#faiss-with-sqlite","title":"FAISS (with SQLite)","text":"<p>Benefits for Obelisk: - Extremely lightweight - Optimized for in-memory performance - No additional services required - Perfect for small to medium documentation</p> <p>Integration approach: - Store vectors in FAISS - Use SQLite for metadata and text storage - Join results using document IDs</p>"},{"location":"chatbot/rag/vector-database/#configuring-alternative-databases","title":"Configuring Alternative Databases","text":"<p>To use an alternative vector database with Obelisk:</p> <pre><code># In a future configuration file\nrag:\n  vector_db:\n    type: \"qdrant\"  # Options: chroma, qdrant, milvus, faiss\n    connection:\n      host: \"localhost\"\n      port: 6333\n    collection: \"obelisk_docs\"\n    embedding_dimensions: 768\n</code></pre>"},{"location":"cloud/Cloud%20Native%20Tooling/","title":"Cloud Native Tooling","text":"<p>A collection of cloud-native tools and resources.</p> <p>Categories: cloud \u2601\ufe0f \u2022 kubernetes \ud83d\udea2 \u2022 devops \ud83d\udd04 \u2022 infrastructure \ud83c\udfd7\ufe0f</p>"},{"location":"cloud/Cloud%20Native%20Tooling/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes</p> <p>Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts.</p>"},{"location":"cloud/Cloud%20Native%20Tooling/#key-components","title":"Key Components","text":"<ul> <li>Pods: The smallest deployable units in Kubernetes</li> <li>Services: Abstract way to expose applications running on pods</li> <li>Deployments: Declarative updates for pods and replica sets</li> <li>ConfigMaps: External configuration for applications</li> </ul>"},{"location":"cloud/Cloud%20Native%20Tooling/#infrastructure-as-code","title":"Infrastructure as Code","text":"<pre><code>graph TD\n    A[Infrastructure as Code] --&gt; B[Terraform]\n    A --&gt; C[CloudFormation]\n    A --&gt; D[Pulumi]\n    B --&gt; E[Providers]\n    E --&gt; F[AWS]\n    E --&gt; G[Azure]\n    E --&gt; H[GCP]</code></pre>"},{"location":"cloud/Cloud%20Native%20Tooling/#container-technologies","title":"Container Technologies","text":"Technology Description Use Case Docker Container platform Development, packaging Podman Daemonless container engine Security-focused environments Buildah Building OCI images CI/CD pipelines Containerd Container runtime Kubernetes environments"},{"location":"cloud/Cloud%20Native%20Tooling/#devops-practices","title":"DevOps Practices","text":"<ul> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Infrastructure as Code</li> <li>Observability</li> <li>Security as Code</li> </ul> <p>Best Practice</p> <p>Implement GitOps workflows to manage infrastructure and application deployments through Git repositories.</p>"},{"location":"customization/","title":"MkDocs Customization Guide","text":"<p>This section documents all customizations applied to the MkDocs Material theme in the Obelisk project.</p>"},{"location":"customization/#overview","title":"Overview","text":"<p>Obelisk uses MkDocs with the Material theme to generate a static site from Markdown files. Several customizations have been applied to enhance the appearance and functionality.</p>"},{"location":"customization/#customization-categories","title":"Customization Categories","text":"<ul> <li>CSS Styling - Custom CSS styles applied to the Material theme</li> <li>HTML Templates - Customized HTML template overrides</li> <li>JavaScript Enhancements - Custom JavaScript functionality</li> <li>Python Integration - Python scripts and utilities for MkDocs</li> <li>Versioning - Documentation versioning with Mike</li> </ul>"},{"location":"customization/#configuration","title":"Configuration","text":"<p>The primary configuration for MkDocs is in the <code>mkdocs.yml</code> file in the project root. This file controls theme settings, plugins, navigation, and more.</p> <p>See the detailed MkDocs configuration guide for a complete reference.</p>"},{"location":"customization/#directory-structure","title":"Directory Structure","text":"<p>Key directories related to MkDocs customization:</p> <pre><code>/workspaces/obelisk/\n\u251c\u2500\u2500 mkdocs.yml           # Main configuration file\n\u251c\u2500\u2500 vault/               # Content source (similar to \"docs\" in standard MkDocs)\n\u2502   \u251c\u2500\u2500 index.md         # Homepage\n\u2502   \u251c\u2500\u2500 stylesheets/     # Custom CSS\n\u2502   \u2502   \u2514\u2500\u2500 extra.css    # Primary custom styles\n\u2502   \u251c\u2500\u2500 javascripts/     # Custom JavaScript\n\u2502   \u2502   \u2514\u2500\u2500 extra.js     # Custom scripts\n\u2502   \u2514\u2500\u2500 overrides/       # HTML template overrides\n\u2502       \u2514\u2500\u2500 main.html    # Main template override\n\u2514\u2500\u2500 site/                # Output directory (generated)\n    \u2514\u2500\u2500 versions.json    # Versioning information\n</code></pre>"},{"location":"customization/mkdocs-configuration/","title":"MkDocs Configuration","text":"<p>The <code>mkdocs.yml</code> file is the central configuration file for the Obelisk documentation site. This file controls theme settings, plugins, navigation structure, and more.</p>"},{"location":"customization/mkdocs-configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>site_name: Obelisk\nsite_description: Obsidian vault to MkDocs Material Theme site generator with AI integration\nsite_author: Obelisk Team\nsite_url: https://usrbinkat.github.io/obelisk\n\nrepo_name: usrbinkat/obelisk\nrepo_url: https://github.com/usrbinkat/obelisk\nedit_uri: edit/main/vault/\n</code></pre> <p>These settings define: - site_name: The name displayed in the header and browser title - site_description: Used for SEO and metadata - site_author: The author metadata - site_url: The base URL where the site is hosted - repo_name: The name of the GitHub repository  - repo_url: Link to the GitHub repository - edit_uri: Path for the \"edit this page\" functionality</p>"},{"location":"customization/mkdocs-configuration/#theme-configuration","title":"Theme Configuration","text":"<pre><code>theme:\n  name: material\n  custom_dir: vault/overrides\n  features:\n    # Navigation\n    - navigation.instant\n    - navigation.tracking\n    - navigation.tabs\n    - navigation.tabs.sticky\n    - navigation.sections\n    - navigation.expand\n    - navigation.indexes\n    - navigation.top\n    - navigation.footer\n    - navigation.path\n\n    # Table of contents\n    - toc.follow\n    - toc.integrate\n\n    # Search\n    - search.suggest\n    - search.highlight\n    - search.share\n\n    # Content\n    - content.tabs.link\n    - content.code.annotation\n    - content.code.copy\n    - content.action.edit\n    - content.action.view\n\n    # Header anchors and tooltips\n    - header.autohide\n\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n      primary: deep purple\n      accent: deep orange\n    # Dark mode  \n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n      primary: deep purple\n      accent: deep orange\n  font:\n    text: Roboto\n    code: Roboto Mono\n  favicon: assets/favicon.png\n  icon:\n    logo: material/book-open-page-variant\n    repo: fontawesome/brands/github\n  language: en\n</code></pre> <p>Key theme settings: - name: Specifies the Material theme - custom_dir: Directory for template overrides - features: Enables specific theme features - palette: Defines color schemes for light and dark mode - font: Specifies the fonts for text and code - favicon: Path to the favicon - icon: Icons for logo and repository - language: Default language</p>"},{"location":"customization/mkdocs-configuration/#markdown-extensions","title":"Markdown Extensions","text":"<pre><code>markdown_extensions:\n  - admonition\n  - attr_list\n  - def_list\n  - footnotes\n  - md_in_html\n  - toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: usrbinkat\n      repo: obelisk\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n</code></pre> <p>This section enables various Markdown extensions that enhance the standard Markdown syntax: - admonition: Adds note, warning, tip blocks - attr_list: Allows adding HTML attributes to elements - toc: Table of contents with permalinks - pymdownx.emoji: Adds emoji support - pymdownx.highlight: Code syntax highlighting - pymdownx.superfences: Fenced code blocks with support for Mermaid diagrams - pymdownx.tabbed: Tabbed content - pymdownx.tasklist: Task lists with custom checkboxes</p>"},{"location":"customization/mkdocs-configuration/#plugins","title":"Plugins","text":"<pre><code>plugins:\n  - search\n  - git-revision-date-localized:\n      enable_creation_date: true\n      fallback_to_build_date: true\n      type: date\n  - minify:\n      minify_html: true\n      minify_js: true\n      minify_css: true\n  - awesome-pages\n  - glightbox:\n      touchNavigation: true\n      loop: false\n      effect: zoom\n      width: 100%\n      height: auto\n      zoomable: true\n      draggable: true\n</code></pre> <p>Plugins add additional functionality: - search: Adds search functionality - git-revision-date-localized: Shows last update date based on git - minify: Reduces file sizes - awesome-pages: Simplifies navigation configuration - glightbox: Enhanced image viewing</p>"},{"location":"customization/mkdocs-configuration/#extra-configuration","title":"Extra Configuration","text":"<pre><code>extra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/usrbinkat\n      name: GitHub\n  version:\n    provider: mike\n    default: 0.1.0\n  consent:\n    title: Cookie consent\n    description: &gt;- \n      We use cookies to recognize your repeated visits and preferences, as well\n      as to analyze traffic and understand where our visitors are coming from.\n  generator: false\n\nextra_css:\n  - stylesheets/extra.css\nextra_javascript:\n  - javascripts/extra.js\n</code></pre> <p>Additional configuration options: - social: Social media links for the footer - version: Documentation versioning configuration - consent: Cookie consent configuration - generator: Hides the \"Made with Material for MkDocs\" notice - extra_css/extra_javascript: Paths to custom CSS and JavaScript files</p>"},{"location":"customization/mkdocs-configuration/#navigation","title":"Navigation","text":"<pre><code>docs_dir: vault\n\nnav:\n  - Home: index.md\n  - Cloud: \n    - Overview: cloud/Cloud Native Tooling.md\n</code></pre> <ul> <li>docs_dir: Specifies the directory containing documentation files</li> <li>nav: Defines the navigation structure</li> <li>Can contain nested sections</li> <li>Each entry has a title and a path to a markdown file</li> </ul>"},{"location":"customization/css/","title":"CSS Customization","text":"<p>The Obelisk project includes extensive CSS customizations to enhance the appearance and user experience of the default Material theme.</p>"},{"location":"customization/css/#implementation","title":"Implementation","text":"<p>Custom CSS is applied through the <code>extra_css</code> setting in <code>mkdocs.yml</code>:</p> <pre><code>extra_css:\n  - stylesheets/extra.css\n</code></pre> <p>The primary CSS file is located at <code>vault/stylesheets/extra.css</code>.</p>"},{"location":"customization/css/#key-customizations","title":"Key Customizations","text":""},{"location":"customization/css/#theme-colors","title":"Theme Colors","text":"<p>Custom color variables are defined in the <code>:root</code> selector:</p> <pre><code>:root {\n  /* Primary theme colors - modern purple-blue gradient */\n  --md-primary-fg-color: #5e35b1;\n  --md-primary-fg-color--light: #9575cd;\n  --md-primary-fg-color--dark: #4527a0;\n\n  /* Accent color - complementary orange for highlights */\n  --md-accent-fg-color: #ff8a65; \n  --md-accent-fg-color--transparent: rgba(255, 138, 101, 0.1);\n}\n</code></pre>"},{"location":"customization/css/#dark-mode-support","title":"Dark Mode Support","text":"<p>Dark mode specific styles ensure proper contrast and readability:</p> <pre><code>/* Dark mode custom variables */\n[data-md-color-scheme=\"slate\"] {\n  --obelisk-box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3), 0 1px 3px rgba(0, 0, 0, 0.2);\n}\n\n[data-md-color-scheme=\"slate\"] .md-content {\n  background-color: var(--md-default-bg-color);\n}\n</code></pre>"},{"location":"customization/css/#announcement-banner","title":"Announcement Banner","text":"<p>A customized announcement banner provides an eye-catching, yet unobtrusive notice at the top of each page:</p> <pre><code>/* Announcement bar styling */\n.md-banner {\n  background-color: #ff8a65; /* Orange background */\n  color: #212121; /* Dark text */\n  padding: 0; /* Remove padding, we'll control this through height */\n  font-size: 0.8rem; /* Slightly larger font size */\n  line-height: 1; /* Minimal line height */\n  height: 36px; /* Balanced height */\n  display: flex;\n  align-items: center; /* Vertical centering */\n  justify-content: center; /* Horizontal centering */\n}\n</code></pre>"},{"location":"customization/css/#layout-enhancements","title":"Layout Enhancements","text":"<p>Content areas use subtle shadows and rounded corners for a modern look:</p> <pre><code>.md-content {\n  background-color: white;\n  border-radius: var(--obelisk-border-radius);\n  box-shadow: var(--obelisk-box-shadow);\n  padding: 1.5rem;\n  margin-bottom: 2rem;\n}\n</code></pre>"},{"location":"customization/css/#typography-improvements","title":"Typography Improvements","text":"<p>Font styles and spacing are optimized for readability:</p> <pre><code>/* Heading styles */\n.md-content h1, \n.md-content h2, \n.md-content h3 {\n  font-weight: 500;\n  margin-top: 2rem;\n}\n\n.md-content h1 {\n  font-size: 2rem;\n  margin-bottom: 1rem;\n  color: var(--md-primary-fg-color--dark);\n}\n</code></pre>"},{"location":"customization/css/#admonition-callout-styling","title":"Admonition &amp; Callout Styling","text":"<p>Enhanced styling for admonitions and custom callouts:</p> <pre><code>/* Special styling for Obsidian compatibility */\n.admonition {\n  border-radius: var(--obelisk-border-radius);\n  margin: 1.5em 0;\n  box-shadow: var(--obelisk-box-shadow);\n  border: none;\n  transition: var(--obelisk-transition);\n}\n\n/* Custom callouts */\n.callout {\n  border-left: 4px solid var(--md-primary-fg-color);\n  padding: 1em 1.5em;\n  margin: 1.2em 0;\n  background-color: var(--md-accent-fg-color--transparent);\n  border-radius: 0 var(--obelisk-border-radius) var(--obelisk-border-radius) 0;\n  box-shadow: var(--obelisk-box-shadow);\n}\n</code></pre>"},{"location":"customization/css/#responsive-design","title":"Responsive Design","text":"<p>Media queries ensure the design works well on all screen sizes:</p> <pre><code>/* Mobile Responsiveness Improvements */\n@media screen and (max-width: 76.1875em) {\n  .md-nav--primary .md-nav__title {\n    background-color: var(--md-primary-fg-color);\n    color: white;\n  }\n\n  .md-sidebar--primary {\n    background-color: white;\n  }\n\n  [data-md-color-scheme=\"slate\"] .md-sidebar--primary {\n    background-color: var(--md-default-bg-color);\n  }\n}\n</code></pre>"},{"location":"customization/css/#full-css-reference","title":"Full CSS Reference","text":"<p>For the complete set of CSS customizations, see the extra.css file.</p>"},{"location":"customization/html/","title":"HTML Template Customization","text":"<p>The Material theme in MkDocs can be customized by overriding specific HTML templates. Obelisk uses this feature to customize key parts of the interface without modifying the theme itself.</p>"},{"location":"customization/html/#implementation","title":"Implementation","text":"<p>Template overrides are enabled by setting the <code>custom_dir</code> property in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  name: material\n  custom_dir: vault/overrides\n</code></pre> <p>All custom templates are placed in the <code>vault/overrides</code> directory.</p>"},{"location":"customization/html/#main-template-override","title":"Main Template Override","text":"<p>The primary template override is <code>main.html</code>, which extends the base template and replaces specific blocks:</p> <pre><code>{% extends \"base.html\" %}\n\n{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n\n{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#template-blocks","title":"Template Blocks","text":"<p>The Material theme provides several blocks that can be overridden:</p> Block Name Description <code>announce</code> Content for the announcement bar at the top of the page <code>outdated</code> Message displayed when viewing an outdated version of the documentation <code>content</code> Main content area of the page <code>extrahead</code> Additional content for the HTML head section <code>footer</code> Footer content"},{"location":"customization/html/#key-customizations","title":"Key Customizations","text":""},{"location":"customization/html/#announcement-banner","title":"Announcement Banner","text":"<p>The announcement banner is customized to display the current Obelisk version with a link to the GitHub repository:</p> <pre><code>{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#outdated-version-message","title":"Outdated Version Message","text":"<p>The message for outdated versions is customized to provide clear navigation to the latest version:</p> <pre><code>{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#adding-new-template-overrides","title":"Adding New Template Overrides","text":"<p>To add a new template override:</p> <ol> <li>Identify the template file you want to override from the Material theme</li> <li>Create a file with the same name in the <code>vault/overrides</code> directory</li> <li>Either extend the base template and override specific blocks, or provide a complete replacement</li> </ol> <p>For example, to override the footer:</p> <pre><code>{% extends \"base.html\" %}\n\n{% block footer %}\n  &lt;footer class=\"md-footer\"&gt;\n    &lt;div class=\"md-footer-meta md-typeset\"&gt;\n      &lt;div class=\"md-footer-meta__inner md-grid\"&gt;\n        &lt;div class=\"md-footer-copyright\"&gt;\n          &lt;div class=\"md-footer-copyright__highlight\"&gt;\n            Powered by Obelisk {{ config.extra.version.default }}\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/footer&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#full-template-reference","title":"Full Template Reference","text":"<p>For more information on template customization, refer to the Material theme documentation.</p>"},{"location":"customization/javascript/","title":"JavaScript Customization","text":"<p>Obelisk includes custom JavaScript to enhance the user experience beyond what's provided by the standard Material theme.</p>"},{"location":"customization/javascript/#implementation","title":"Implementation","text":"<p>JavaScript is added through the <code>extra_javascript</code> setting in <code>mkdocs.yml</code>:</p> <pre><code>extra_javascript:\n  - javascripts/extra.js\n</code></pre> <p>The custom JavaScript file is located at <code>vault/javascripts/extra.js</code>.</p>"},{"location":"customization/javascript/#key-features","title":"Key Features","text":""},{"location":"customization/javascript/#smooth-scrolling","title":"Smooth Scrolling","text":"<p>Implements smooth scrolling for anchor links within the same page:</p> <pre><code>// Add smooth scroll behavior\ndocument.querySelectorAll('a[href^=\"#\"]').forEach(anchor =&gt; {\n  anchor.addEventListener('click', function (e) {\n    e.preventDefault();\n\n    const target = document.querySelector(this.getAttribute('href'));\n    if (target) {\n      target.scrollIntoView({\n        behavior: 'smooth'\n      });\n    }\n  });\n});\n</code></pre>"},{"location":"customization/javascript/#version-footer","title":"Version Footer","text":"<p>Adds a version indicator to the footer:</p> <pre><code>// Add version text to footer\nconst footer = document.querySelector('.md-footer-meta');\nif (footer) {\n  const versionDiv = document.createElement('div');\n  versionDiv.className = 'md-footer-version';\n  versionDiv.innerHTML = '&lt;span&gt;Obelisk v0.1.0&lt;/span&gt;';\n  footer.querySelector('.md-footer-meta__inner').appendChild(versionDiv);\n}\n</code></pre>"},{"location":"customization/javascript/#cookie-consent-handling","title":"Cookie Consent Handling","text":"<p>Automatically handles cookie consent in development environments:</p> <pre><code>// Accept cookie consent by default in development\nconst acceptButtons = document.querySelectorAll('.md-dialog__accept');\nif (acceptButtons.length &gt; 0) {\n  setTimeout(() =&gt; {\n    acceptButtons[0].click();\n  }, 500);\n}\n</code></pre>"},{"location":"customization/javascript/#adding-custom-javascript","title":"Adding Custom JavaScript","text":"<p>To add new JavaScript functionality:</p> <ol> <li>Edit the <code>extra.js</code> file or create a new JavaScript file</li> <li>Add the file to the <code>extra_javascript</code> list in <code>mkdocs.yml</code> if needed</li> <li>Make sure your code is wrapped in a DOM ready event listener:</li> </ol> <pre><code>document.addEventListener('DOMContentLoaded', function() {\n  // Your code here\n});\n</code></pre>"},{"location":"customization/javascript/#integrating-with-material-theme","title":"Integrating with Material Theme","text":"<p>The Material theme provides several JavaScript hooks and events that can be used for customization. Common integration points include:</p> <ul> <li>Search functionality - Customizing search behavior</li> <li>Navigation - Enhancing navigation interactions</li> <li>Color scheme switching - Adding custom handling for light/dark mode</li> <li>Content tabs - Extending tabbed content behavior</li> </ul>"},{"location":"customization/javascript/#full-javascript-reference","title":"Full JavaScript Reference","text":"<p>For the complete set of JavaScript customizations, see the extra.js file.</p>"},{"location":"customization/python/","title":"Python Integration","text":"<p>Obelisk uses Python for extending MkDocs functionality and providing utilities for managing the documentation. This section details the Python components related to MkDocs customization.</p>"},{"location":"customization/python/#project-structure","title":"Project Structure","text":"<p>The Python code for Obelisk is organized in the <code>/workspaces/obelisk/obelisk</code> directory:</p> <pre><code>/workspaces/obelisk/obelisk/\n\u251c\u2500\u2500 __init__.py          # Package initialization with version info\n\u251c\u2500\u2500 cli.py               # Command-line interface \n\u251c\u2500\u2500 config.py            # Configuration utilities\n\u2514\u2500\u2500 convert.py           # Conversion utilities for Obsidian to MkDocs\n</code></pre>"},{"location":"customization/python/#package-initialization","title":"Package Initialization","text":"<p>The <code>__init__.py</code> file defines basic package information:</p> <pre><code>\"\"\"\nObelisk - Obsidian vault to MkDocs Material Theme site generator.\n\"\"\"\n\n__version__ = \"0.1.0\"\n__author__ = \"Obelisk Team\"\n</code></pre>"},{"location":"customization/python/#command-line-interface","title":"Command-Line Interface","text":"<p>The <code>cli.py</code> module provides a command-line interface for using Obelisk:</p> <pre><code>\"\"\"\nObelisk CLI tool to convert Obsidian vaults to MkDocs sites.\n\"\"\"\n\nimport argparse\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nfrom . import __version__\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Convert Obsidian vault to MkDocs Material Theme site.\"\n    )\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"Obelisk {__version__}\"\n    )\n    parser.add_argument(\n        \"--vault\", \n        type=str, \n        help=\"Path to Obsidian vault directory\",\n        default=\"vault\"\n    )\n    parser.add_argument(\n        \"--output\", \n        type=str, \n        help=\"Output directory for the generated site\",\n        default=\"site\"\n    )\n    parser.add_argument(\n        \"--serve\", \n        action=\"store_true\", \n        help=\"Start a development server after building\"\n    )\n\n    # Command processing logic...\n</code></pre>"},{"location":"customization/python/#configuration-utilities","title":"Configuration Utilities","text":"<p>The <code>config.py</code> module contains utilities for handling MkDocs configuration:</p> <pre><code>\"\"\"\nConfiguration utilities for Obelisk.\n\"\"\"\n\nimport yaml\n\ndef get_default_config():\n    \"\"\"Return the default Obelisk configuration.\"\"\"\n    return {\n        \"site_name\": \"Obelisk\",\n        \"site_description\": \"Obsidian vault to MkDocs Material Theme site generator\",\n        \"theme\": {\n            \"name\": \"material\",\n            \"features\": [\n                \"navigation.instant\",\n                \"navigation.tracking\",\n                \"navigation.tabs\",\n                # Other features...\n            ],\n            \"palette\": [\n                {\n                    \"scheme\": \"default\",\n                    \"primary\": \"deep purple\",\n                    \"accent\": \"deep orange\"\n                },\n                {\n                    \"scheme\": \"slate\",\n                    \"primary\": \"deep purple\",\n                    \"accent\": \"deep orange\"\n                }\n            ]\n        },\n        # Other configuration options...\n    }\n</code></pre>"},{"location":"customization/python/#conversion-utilities","title":"Conversion Utilities","text":"<p>The <code>convert.py</code> module handles conversion from Obsidian to MkDocs format:</p> <pre><code>\"\"\"\nConversion utilities for transforming Obsidian files to MkDocs-compatible format.\n\"\"\"\n\nimport re\nimport shutil\nfrom pathlib import Path\n\ndef process_obsidian_vault(vault_path, output_path=\"vault\"):\n    \"\"\"\n    Process an Obsidian vault directory and prepare it for MkDocs.\n    \"\"\"\n    # Process all markdown files...\n\ndef convert_file(input_path, output_path):\n    \"\"\"\n    Convert an Obsidian markdown file to MkDocs compatible format.\n    \"\"\"\n    # Convert wiki links to markdown links\n    # Convert Obsidian callouts to admonitions\n    # Convert Obsidian comments to HTML comments\n</code></pre>"},{"location":"customization/python/#integration-with-mkdocs","title":"Integration with MkDocs","text":"<p>The Python code integrates with MkDocs through:</p> <ol> <li>Command execution - Running MkDocs commands via <code>subprocess</code></li> <li>Configuration generation - Creating and modifying <code>mkdocs.yml</code></li> <li>Content processing - Transforming Obsidian syntax to MkDocs/Material compatible format</li> </ol>"},{"location":"customization/python/#using-the-python-cli","title":"Using the Python CLI","text":"<p>The CLI is registered as an entry point in <code>pyproject.toml</code>:</p> <pre><code>[project.scripts]\nobelisk = \"obelisk.cli:main\"\n</code></pre> <p>This allows running Obelisk as a command:</p> <pre><code>obelisk --vault /path/to/obsidian/vault --serve\n</code></pre>"},{"location":"customization/python/#extending-with-mkdocs-plugins","title":"Extending with MkDocs Plugins","text":"<p>For more complex customizations, Python can be used to create custom MkDocs plugins. These would be placed in a <code>plugins</code> subdirectory and registered in <code>mkdocs.yml</code>.</p>"},{"location":"customization/versioning/","title":"Documentation Versioning","text":"<p>Obelisk implements documentation versioning using Mike, which allows maintaining multiple versions of the documentation simultaneously.</p>"},{"location":"customization/versioning/#configuration","title":"Configuration","text":"<p>Versioning is configured in the <code>mkdocs.yml</code> file:</p> <pre><code>extra:\n  version:\n    provider: mike\n    default: 0.1.0\n</code></pre>"},{"location":"customization/versioning/#version-files","title":"Version Files","text":"<p>The versioning information is stored in two places:</p> <ol> <li>versions.json - Lists all available versions</li> <li>Site deployment directories - Each version has its own directory</li> </ol> <p>The <code>versions.json</code> file format:</p> <pre><code>[\n  {\n    \"version\": \"0.1.0\",\n    \"title\": \"0.1.0\",\n    \"aliases\": [\n      \"latest\"\n    ]\n  }\n]\n</code></pre>"},{"location":"customization/versioning/#deployment-commands","title":"Deployment Commands","text":"<p>Version deployment is managed through Taskfile commands:</p> <pre><code># From Taskfile.yaml\nversion-deploy:\n  desc: \"Deploy a new version (requires version number and description)\"\n  cmds:\n    - poetry run mike deploy --push --update-aliases {{.CLI_ARGS}}\n\nversion-set-default:\n  desc: \"Set the default version (requires version number)\"\n  cmds:\n    - poetry run mike set-default --push {{.CLI_ARGS}}\n</code></pre> <p>Usage:</p> <pre><code># Deploy new version\ntask version-deploy -- 0.1.0 \"Initial boilerplate template release\"\n\n# Set as default\ntask version-set-default -- 0.1.0\n</code></pre>"},{"location":"customization/versioning/#version-selection-ui","title":"Version Selection UI","text":"<p>The Material theme displays a version selection dropdown when versioning is enabled. The styling for this dropdown is customized in <code>extra.css</code>:</p> <pre><code>/* Version selector styling */\n.md-version {\n  background-color: rgba(0, 0, 0, 0.1);\n  border-radius: 4px;\n  padding: 0 0.5rem;\n}\n</code></pre>"},{"location":"customization/versioning/#outdated-version-notice","title":"Outdated Version Notice","text":"<p>When viewing an outdated version, a notice is displayed at the top of the page. This notice is customized in <code>main.html</code>:</p> <pre><code>{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/versioning/#version-integration-with-announcement-banner","title":"Version Integration with Announcement Banner","text":"<p>The current version is displayed in the announcement banner:</p> <pre><code>{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/versioning/#managing-multiple-versions","title":"Managing Multiple Versions","text":"<p>To maintain multiple versions:</p> <ol> <li>Make changes to the documentation</li> <li>Deploy a new version with an appropriate version number and description</li> <li>Optionally set the new version as the default</li> <li>Users can switch between versions using the dropdown in the UI</li> </ol>"},{"location":"customization/versioning/#versioning-best-practices","title":"Versioning Best Practices","text":"<ol> <li>Semantic Versioning - Use SemVer format (MAJOR.MINOR.PATCH)</li> <li>Clear Descriptions - Provide meaningful descriptions for each version</li> <li>Default Version - Typically set the latest stable version as default</li> <li>Aliases - Use aliases like \"latest\" or \"stable\" for important versions</li> </ol>"},{"location":"development/","title":"Development Configuration","text":"<p>This section documents the development environment configuration files and tools used in the Obelisk project.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<p>The root level of the Obelisk project contains several configuration files and directories that control development, building, and deployment:</p> <pre><code>/workspaces/obelisk/\n\u251c\u2500\u2500 .devcontainer/                # Development container configuration\n\u2502   \u251c\u2500\u2500 devcontainer.json         # VS Code development container settings\n\u2502   \u2514\u2500\u2500 Dockerfile                # Development container image definition\n\u251c\u2500\u2500 .github/                      # GitHub-specific configurations\n\u2502   \u2514\u2500\u2500 dependabot.yml            # Dependabot configuration\n\u251c\u2500\u2500 obelisk/                      # Python package for Obelisk\n\u2502   \u251c\u2500\u2500 __init__.py               # Package initialization\n\u2502   \u251c\u2500\u2500 cli.py                    # Command-line interface\n\u2502   \u251c\u2500\u2500 config.py                 # Configuration utilities\n\u2502   \u2514\u2500\u2500 convert.py                # Conversion utilities\n\u251c\u2500\u2500 site/                         # Generated static site (output)\n\u251c\u2500\u2500 vault/                        # Documentation content (input)\n\u251c\u2500\u2500 .editorconfig                 # Editor configuration for consistency\n\u251c\u2500\u2500 .envrc                        # direnv configuration for environment setup\n\u251c\u2500\u2500 .gitattributes                # Git attributes configuration\n\u251c\u2500\u2500 .gitignore                    # Files ignored by Git\n\u251c\u2500\u2500 CLAUDE.md                     # Instructions for Claude AI assistant\n\u251c\u2500\u2500 CONTRIBUTING.md               # Contribution guidelines\n\u251c\u2500\u2500 docker-compose.yaml           # Docker Compose configuration\n\u251c\u2500\u2500 Dockerfile                    # Main Docker image definition\n\u251c\u2500\u2500 LICENSE                       # Project license\n\u251c\u2500\u2500 mkdocs.yml                    # MkDocs configuration\n\u251c\u2500\u2500 poetry.lock                   # Poetry locked dependencies\n\u251c\u2500\u2500 pyproject.toml                # Python project configuration\n\u251c\u2500\u2500 README.md                     # Project README\n\u251c\u2500\u2500 Taskfile.yaml                 # Task runner configuration\n\u2514\u2500\u2500 versions.json                 # Documentation version information\n</code></pre>"},{"location":"development/#development-environment-files","title":"Development Environment Files","text":"<p>The following sections document the various configuration files and their purposes:</p> <ul> <li>Docker Configuration - Docker and container configuration</li> <li>Task Runner - Task runner configuration and usage</li> <li>Python Configuration - Python project settings and dependencies</li> <li>Git Configuration - Git workflow and settings</li> <li>Editor Configuration - Code editor settings</li> <li>Documentation Files - Project documentation</li> </ul>"},{"location":"development/docker/","title":"Docker Configuration","text":"<p>Obelisk uses Docker for containerization of both the development environment and the production deployment.</p>"},{"location":"development/docker/#dockerfile","title":"Dockerfile","text":"<p>The main <code>Dockerfile</code> defines the container image for running Obelisk:</p> <pre><code># View the Dockerfile contents\ncat /workspaces/obelisk/Dockerfile\n</code></pre> <p>This file defines: - Base image selection - System dependencies installation - Python environment setup - Obelisk installation - Default command execution</p>"},{"location":"development/docker/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>The <code>docker-compose.yaml</code> file orchestrates the complete Obelisk stack, including optional AI components:</p> <pre><code># View the docker-compose.yaml contents\ncat /workspaces/obelisk/docker-compose.yaml\n</code></pre> <p>Key services include: - obelisk: The main Obelisk documentation server - ollama: (Optional) AI model server for local embedding and inference - openwebui: (Optional) Web interface for interacting with AI models</p>"},{"location":"development/docker/#development-container","title":"Development Container","text":"<p>The <code>.devcontainer</code> directory contains configuration for VS Code's Development Containers feature:</p>"},{"location":"development/docker/#devcontainerjson","title":"devcontainer.json","text":"<pre><code># View the devcontainer.json contents\ncat /workspaces/obelisk/.devcontainer/devcontainer.json\n</code></pre> <p>This file configures: - Development container settings - VS Code extensions to install - Port forwarding - Environment variables - Startup commands</p>"},{"location":"development/docker/#dockerfile-dev","title":"Dockerfile (Dev)","text":"<pre><code># View the development container Dockerfile\ncat /workspaces/obelisk/.devcontainer/Dockerfile\n</code></pre> <p>The development container Dockerfile includes: - Development-specific tools and dependencies - Debugging utilities - Additional build tools</p>"},{"location":"development/docker/#running-with-docker","title":"Running with Docker","text":"<p>To run Obelisk using Docker:</p> <ol> <li> <p>Build the image:    <pre><code>docker build -t obelisk .\n</code></pre></p> </li> <li> <p>Run with Docker:    <pre><code>docker run -p 8000:8000 obelisk\n</code></pre></p> </li> <li> <p>Run with Docker Compose:    <pre><code>docker-compose up obelisk\n</code></pre></p> </li> <li> <p>Run the full stack with AI:    <pre><code>docker-compose up\n</code></pre></p> </li> </ol>"},{"location":"development/docker/#task-runner-integration","title":"Task Runner Integration","text":"<p>Docker commands are also available through the Task runner:</p> <pre><code># Build Docker container\ntask docker-build\n\n# Run with local volumes mounted\ntask docker-run\n\n# Run Obelisk service only\ntask compose-obelisk\n\n# Run full stack with Ollama and OpenWebUI\ntask compose\n</code></pre>"},{"location":"development/documentation/","title":"Documentation Files","text":"<p>Obelisk includes several documentation files to guide developers and users.</p>"},{"location":"development/documentation/#readmemd","title":"README.md","text":"<p>The primary project README provides an overview of Obelisk:</p> <pre><code># View README.md contents\ncat /workspaces/obelisk/README.md\n</code></pre> <p>The README typically includes: - Project description and purpose - Quick start instructions - Key features - Requirements - Basic usage examples - Links to more detailed documentation</p>"},{"location":"development/documentation/#contributingmd","title":"CONTRIBUTING.md","text":"<p>Guidelines for contributing to the project:</p> <pre><code># View CONTRIBUTING.md contents\ncat /workspaces/obelisk/CONTRIBUTING.md\n</code></pre> <p>This file covers: - How to set up a development environment - Coding standards and conventions - Testing requirements - Pull request process - Issue reporting guidelines</p>"},{"location":"development/documentation/#claudemd","title":"CLAUDE.md","text":"<p>Special instructions for Claude AI assistants:</p> <pre><code># View CLAUDE.md contents\ncat /workspaces/obelisk/CLAUDE.md\n</code></pre> <p>This file provides: - Project context for Claude AI - Common commands and workflows - Code style guidelines - Specific instructions for AI-assisted development</p>"},{"location":"development/documentation/#license","title":"LICENSE","text":"<p>The project's license file:</p> <pre><code># View LICENSE contents\ncat /workspaces/obelisk/LICENSE\n</code></pre> <p>Obelisk is licensed under the MIT License, which: - Permits commercial use, modification, distribution, and private use - Requires license and copyright notice inclusion - Provides no warranty or liability</p>"},{"location":"development/documentation/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation content is organized in the <code>vault</code> directory, with additional content generated during the build process:</p> <pre><code>vault/\n\u251c\u2500\u2500 assets/                  # Static assets like images\n\u251c\u2500\u2500 cloud/                   # Cloud-related documentation\n\u251c\u2500\u2500 customization/           # Theme customization documentation\n\u251c\u2500\u2500 development/             # Development setup documentation\n\u251c\u2500\u2500 javascripts/             # Custom JavaScript files\n\u251c\u2500\u2500 overrides/               # Theme template overrides\n\u251c\u2500\u2500 stylesheets/             # Custom CSS styles\n\u2514\u2500\u2500 index.md                 # Home page\n</code></pre>"},{"location":"development/documentation/#generated-documentation","title":"Generated Documentation","text":"<p>The built documentation is generated to the <code>site</code> directory:</p> <pre><code>site/\n\u251c\u2500\u2500 404.html                 # Not found page\n\u251c\u2500\u2500 assets/                  # Processed assets\n\u251c\u2500\u2500 cloud/                   # Built cloud documentation\n\u251c\u2500\u2500 customization/           # Built customization documentation\n\u251c\u2500\u2500 development/             # Built development documentation\n\u251c\u2500\u2500 index.html               # Built home page\n\u251c\u2500\u2500 search/                  # Search index files\n\u251c\u2500\u2500 sitemap.xml              # Site map for search engines\n\u2514\u2500\u2500 versions.json            # Documentation version information\n</code></pre>"},{"location":"development/editor-config/","title":"Editor Configuration","text":"<p>Obelisk includes configuration files for consistent code formatting and editor behavior across development environments.</p>"},{"location":"development/editor-config/#editorconfig","title":".editorconfig","text":"<p>The <code>.editorconfig</code> file defines coding style preferences that many editors and IDEs support:</p> <pre><code># View .editorconfig contents\ncat /workspaces/obelisk/.editorconfig\n</code></pre> <p>Key settings include: - indent_style: Spaces vs tabs - indent_size: Number of spaces per indentation level - end_of_line: Line ending style (LF, CRLF) - charset: Character encoding - trim_trailing_whitespace: Remove trailing spaces - insert_final_newline: Ensure files end with a newline</p> <p>Different rules can be specified for different file types:</p> <pre><code># Python files\n[*.py]\nindent_style = space\nindent_size = 4\n\n# YAML files\n[*.{yml,yaml}]\nindent_style = space\nindent_size = 2\n</code></pre>"},{"location":"development/editor-config/#vs-code-integration","title":"VS Code Integration","text":"<p>The <code>.devcontainer/devcontainer.json</code> file includes VS Code-specific settings:</p> <pre><code>\"settings\": {\n    \"python.defaultInterpreterPath\": \"/usr/local/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true,\n    \"editor.rulers\": [88],\n    \"terminal.integrated.defaultProfile.linux\": \"bash\"\n}\n</code></pre> <p>These settings configure: - Python interpreter path - Linting tools and settings - Code formatting (Black) - Format on save behavior - Visual rulers for line length - Terminal configuration</p>"},{"location":"development/editor-config/#recommended-extensions","title":"Recommended Extensions","text":"<p>The development container configuration includes recommended VS Code extensions:</p> <pre><code>\"extensions\": [\n    \"ms-python.python\",\n    \"ms-python.vscode-pylance\",\n    \"batisteo.vscode-django\",\n    \"yzhang.markdown-all-in-one\"\n]\n</code></pre> <p>Popular extensions for Obelisk development include: - Python: Python language support - Pylance: Python language server - Django: Django framework support - Markdown All in One: Markdown editing features</p>"},{"location":"development/editor-config/#code-style-enforcement","title":"Code Style Enforcement","text":"<p>Obelisk uses several tools to enforce code style:</p> <ol> <li>Black: Code formatter with minimal configuration</li> <li>Ruff: Fast Python linter</li> <li>mypy: Optional static type checking</li> </ol> <p>Configuration for these tools is in <code>pyproject.toml</code>:</p> <pre><code>[tool.black]\nline-length = 88\ntarget-version = [\"py312\"]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\"E\", \"F\", \"I\", \"W\"]\nignore = []\n</code></pre>"},{"location":"development/git-config/","title":"Git Configuration","text":"<p>Obelisk uses Git for version control with several additional configuration files for managing repository behavior.</p>"},{"location":"development/git-config/#gitignore","title":".gitignore","text":"<p>The <code>.gitignore</code> file specifies which files and directories should be excluded from Git:</p> <pre><code># View .gitignore contents\ncat /workspaces/obelisk/.gitignore\n</code></pre> <p>Key exclusions include: - Built documentation (<code>/site/</code>) - Python cache files (<code>__pycache__/</code>, <code>*.py[cod]</code>) - Virtual environments (<code>.venv/</code>, <code>venv/</code>) - Environment files (<code>.env</code>) - System and editor files (<code>.DS_Store</code>, <code>.idea/</code>)</p>"},{"location":"development/git-config/#gitattributes","title":".gitattributes","text":"<p>The <code>.gitattributes</code> file defines attributes for paths in the repository:</p> <pre><code># View .gitattributes contents\ncat /workspaces/obelisk/.gitattributes\n</code></pre> <p>This file controls: - Line ending normalization - Diff behavior for specific file types - Merge strategies - Export settings</p>"},{"location":"development/git-config/#github-configuration","title":"GitHub Configuration","text":""},{"location":"development/git-config/#githubdependabotyml","title":".github/dependabot.yml","text":"<p>Dependabot configuration for automated dependency updates:</p> <pre><code># View dependabot.yml contents\ncat /workspaces/obelisk/.github/dependabot.yml\n</code></pre> <p>This configuration specifies: - Package ecosystems to monitor (e.g., pip, docker) - Update frequency - Target branches - Reviewers and assignees - Version update strategy</p>"},{"location":"development/git-config/#git-workflow","title":"Git Workflow","text":""},{"location":"development/git-config/#branching-strategy","title":"Branching Strategy","text":"<p>The project follows a feature branch workflow:</p> <ol> <li>The <code>main</code> branch contains stable releases</li> <li>Feature branches are created for new features or fixes</li> <li>Pull requests are used to merge changes back to main</li> <li>The <code>v4</code> branch is used for publishing docs with versioning</li> </ol>"},{"location":"development/git-config/#commit-conventions","title":"Commit Conventions","text":"<p>Commit messages follow the conventional commits format:</p> <pre><code>&lt;type&gt;: &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types include: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Formatting changes - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or updating tests - <code>chore</code>: Maintenance tasks</p>"},{"location":"development/git-config/#version-tags","title":"Version Tags","text":"<p>Git tags are used to mark version releases:</p> <pre><code># Create a version tag\ngit tag -a v0.1.0 -m \"Initial release\"\n\n# Push tags to remote\ngit push --tags\n</code></pre>"},{"location":"development/python-config/","title":"Python Configuration","text":"<p>Obelisk uses modern Python tooling for dependency management, packaging, and development.</p>"},{"location":"development/python-config/#pyprojecttoml","title":"pyproject.toml","text":"<p>The <code>pyproject.toml</code> file defines the project's metadata, dependencies, and build system:</p> <pre><code># View the pyproject.toml contents\ncat /workspaces/obelisk/pyproject.toml\n</code></pre> <p>Key sections include:</p>"},{"location":"development/python-config/#project-metadata","title":"Project Metadata","text":"<pre><code>[project]\nname = \"obelisk\"\nversion = \"0.1.0\"\ndescription = \"Obsidian vault to MkDocs Material Theme site generator\"\nauthors = [\n    {name = \"Obelisk Team\"}\n]\nreadme = \"README.md\"\nrequires-python = \"^3.12\"\nlicense = \"MIT\"\n</code></pre>"},{"location":"development/python-config/#dependencies","title":"Dependencies","text":"<pre><code>dependencies = [\n    \"mkdocs&gt;=1.6.0,&lt;2.0.0\",\n    \"mkdocs-material&gt;=9.6.11,&lt;10.0.0\",\n    \"mkdocs-material-extensions&gt;=1.3.1,&lt;2.0.0\",\n    \"mkdocs-git-revision-date-localized-plugin&gt;=1.4.5,&lt;2.0.0\",\n    # Other dependencies...\n]\n</code></pre>"},{"location":"development/python-config/#script-entrypoints","title":"Script Entrypoints","text":"<pre><code>[project.scripts]\nobelisk = \"obelisk.cli:main\"\n</code></pre>"},{"location":"development/python-config/#development-tools-configuration","title":"Development Tools Configuration","text":"<pre><code>[tool.black]\nline-length = 88\ntarget-version = [\"py312\"]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\"E\", \"F\", \"I\", \"W\"]\nignore = []\n</code></pre>"},{"location":"development/python-config/#poetrylock","title":"poetry.lock","text":"<p>The <code>poetry.lock</code> file contains the exact versions of all dependencies:</p> <pre><code># View lock file summary\ncat /workspaces/obelisk/poetry.lock | head -n 20\n</code></pre> <p>This file ensures reproducible builds by pinning exact versions of: - Direct dependencies - Transitive dependencies - Platform-specific dependencies</p>"},{"location":"development/python-config/#poetry-usage","title":"Poetry Usage","text":"<p>Poetry is used for dependency management and packaging:</p> <pre><code># Install dependencies\npoetry install\n\n# Add a dependency\npoetry add package-name\n\n# Update dependencies\npoetry update\n\n# Run a command in the virtual environment\npoetry run command\n\n# Activate the virtual environment\npoetry shell\n</code></pre>"},{"location":"development/python-config/#direnv-integration","title":"direnv Integration","text":"<p>The <code>.envrc</code> file integrates with direnv for automatic environment activation:</p> <pre><code># View .envrc contents\ncat /workspaces/obelisk/.envrc\n</code></pre> <p>When entering the project directory, direnv: 1. Activates the Python virtual environment 2. Sets necessary environment variables 3. Adds development tools to the PATH</p>"},{"location":"development/task-runner/","title":"Task Runner","text":"<p>Obelisk uses Task (via Taskfile.yaml) as a task runner for development and deployment workflows.</p>"},{"location":"development/task-runner/#taskfileyaml","title":"Taskfile.yaml","text":"<p>The <code>Taskfile.yaml</code> file defines all project tasks and their dependencies:</p> <pre><code># View the Taskfile.yaml contents\ncat /workspaces/obelisk/Taskfile.yaml\n</code></pre>"},{"location":"development/task-runner/#available-tasks","title":"Available Tasks","text":""},{"location":"development/task-runner/#development-tasks","title":"Development Tasks","text":"<pre><code># Install dependencies (Poetry)\ntask install\n\n# Build static site\ntask build\n\n# Run strict build testing\ntask test\n\n# Fast development server with livereload\ntask run\n\n# Build and serve with browser opening\ntask serve\n\n# Remove build artifacts\ntask clean\n</code></pre>"},{"location":"development/task-runner/#versioning-tasks","title":"Versioning Tasks","text":"<pre><code># Deploy version (requires version number and description)\ntask version-deploy -- X.Y.Z \"Description\"\n\n# Set default version (requires version number)\ntask version-set-default -- X.Y.Z\n</code></pre>"},{"location":"development/task-runner/#docker-tasks","title":"Docker Tasks","text":"<pre><code># Build Docker container\ntask docker-build\n\n# Run with local volumes mounted\ntask docker-run\n\n# Run Obelisk service only\ntask compose-obelisk\n\n# Run full stack with Ollama and OpenWebUI\ntask compose\n</code></pre>"},{"location":"development/task-runner/#task-implementation-details","title":"Task Implementation Details","text":"<p>Each task in the Taskfile defines:</p> <ul> <li>desc: Description of what the task does</li> <li>deps: Tasks that must run before this one</li> <li>cmds: Commands to execute</li> <li>vars: Variables used by the task</li> <li>sources/generates: Files to watch for incremental builds</li> </ul>"},{"location":"development/task-runner/#task-variables","title":"Task Variables","text":"<p>The Taskfile also defines variables for configuration:</p> <pre><code>vars:\n  # Project settings\n  PACKAGE: obelisk\n  PYTHON_VERSION: \"3.12\"\n\n  # Docker settings\n  DOCKER_IMAGE: {{.PACKAGE}}\n  DOCKER_TAG: latest\n  DOCKER_RUN_PORT: 8000\n</code></pre>"},{"location":"development/task-runner/#example-usage","title":"Example Usage","text":"<pre><code># Install dependencies\ntask install\n\n# Start development server\ntask run\n\n# Deploy a new version\ntask version-deploy -- 0.1.0 \"Initial release\"\n</code></pre>"}]}
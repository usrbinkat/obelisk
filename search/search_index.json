{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obelisk","text":"<p>Welcome to Obelisk - a powerful Obsidian vault to MkDocs Material Theme converter with integrated AI capabilities.</p> <p>Categories: documentation \ud83d\udcda \u2022 knowledge base \ud83e\udde0 \u2022 AI integration \ud83e\udd16 \u2022 RAG system \ud83d\udd0d</p>"},{"location":"#overview","title":"Overview","text":"<p>Obelisk transforms Obsidian vaults into beautifully rendered static sites using MkDocs with the Material theme. It offers a complete documentation solution with built-in AI-powered search and chat capabilities through Retrieval Augmented Generation (RAG).</p> <p>Complete Documentation Platform</p> <p>Obelisk is more than just a converter - it's a complete documentation platform with AI assistance built in.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Seamless Obsidian Conversion: Preserves Obsidian's rich features in your MkDocs site</li> <li>Integrated RAG Pipeline: Enables AI to answer questions directly from your documentation</li> <li>Material Theme Integration: Beautiful, responsive, and feature-rich documentation</li> <li>AI Chatbot: Connect with Ollama and Open WebUI for context-aware answers</li> <li>Docker Orchestration: One-command deployment of the entire stack</li> <li>Customization Options: Extensive styling and theming capabilities</li> <li>Python-based Workflow: Managed with Poetry for reproducible builds</li> </ul>"},{"location":"#rag-system","title":"RAG System","text":"<p>Obelisk's RAG (Retrieval Augmented Generation) system connects your documentation directly to AI models:</p> <pre><code>graph TD\n    User[User] --&gt; Query[Query Interface]\n    Query --&gt; RAG[RAG Pipeline]\n    RAG --&gt; VectorDB[(Vector Database)]\n    RAG --&gt; Embeddings[Embedding Generation]\n    RAG --&gt; Context[Context Assembly]\n    Context --&gt; Ollama[Ollama LLM]\n    Ollama --&gt; Response[Enhanced Response]\n    Response --&gt; User</code></pre> <p>The RAG pipeline: 1. Processes your documentation into searchable chunks 2. Generates vector embeddings for semantic search 3. Retrieves relevant content based on user queries 4. Feeds this context to an LLM through Ollama 5. Returns accurate, contextual responses based on your content</p>"},{"location":"#quick-start","title":"Quick Start","text":"Using Docker (Recommended)Using Poetry <pre><code># Clone the repository\ngit clone https://github.com/usrbinkat/obelisk.git\ncd obelisk\n\n# Start all services (docs, Ollama, Open WebUI)\ndocker-compose up\n\n# Access documentation at http://localhost:8000\n# Access AI chatbot at http://localhost:8080\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/usrbinkat/obelisk.git\ncd obelisk\n\n# Install dependencies\npoetry install\n\n# Start the documentation server\npoetry run mkdocs serve\n\n# In a separate terminal, set up the RAG system\npoetry run obelisk-rag index\npoetry run obelisk-rag serve --watch\n</code></pre>"},{"location":"#ai-integration","title":"AI Integration","text":"<p>Obelisk provides a complete AI integration stack:</p> <ol> <li>Ollama: Serves optimized LLMs locally (llama3, mistral, phi, etc.)</li> <li>Open WebUI: Provides a user-friendly chat interface</li> <li>RAG System: Connects your documentation content to AI responses</li> </ol> <p>To get started with the AI features:</p> <pre><code># Start the full stack with Docker\ndocker-compose up\n\n# Pull recommended models\ndocker exec -it ollama ollama pull llama3\ndocker exec -it ollama ollama pull mxbai-embed-large\n\n# Index your documentation\ndocker exec -it obelisk obelisk-rag index\n</code></pre>"},{"location":"#customization","title":"Customization","text":"<p>Obelisk supports extensive customization options:</p> <ul> <li>CSS Styling - Custom themes and styles</li> <li>JavaScript Enhancements - Interactive features</li> <li>HTML Templates - Layout customization</li> <li>Python Extensions - Extend functionality</li> <li>Versioning - Documentation versioning</li> </ul>"},{"location":"#development","title":"Development","text":"<p>Obelisk uses modern development tools and practices:</p> <ul> <li>Task Runner: Simplified commands via Taskfile.yaml</li> <li>Docker Support: Containerized development and deployment</li> <li>EditorConfig: Consistent coding styles</li> <li>Poetry: Dependency management and packaging</li> <li>MkDocs: Documentation generation with Material theme</li> </ul> <p>For more information, see the Development Guide.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore our comprehensive documentation: - Chatbot Integration - AI chat setup and configuration - RAG System - Retrieval Augmented Generation details - Customization - Styling and theming options - Development - Developer guides and references</p>"},{"location":"chatbot/","title":"Chatbot Integration","text":"<p>Obelisk includes integration with Ollama and Open WebUI to provide AI-powered chat capabilities directly within your documentation site. This section describes how to set up and use these features.</p>"},{"location":"chatbot/#overview","title":"Overview","text":"<p>The chatbot integration consists of three key components:</p> <ol> <li>Ollama: A lightweight, local AI model server that runs models like Llama2, Mistral, and others.</li> <li>Open WebUI: A web interface for interacting with the AI models served by Ollama.</li> <li>RAG System: A Retrieval Augmented Generation system that enhances responses with content from your documentation.</li> </ol> <p>Together, these services provide a complete AI chat experience that is directly connected to your documentation content, providing accurate, contextually relevant answers.</p>"},{"location":"chatbot/#how-it-works","title":"How It Works","text":"<p>The chatbot integration uses Docker Compose to orchestrate the services:</p> <pre><code>graph TD\n    User[User] --&gt; WebUI[Open WebUI]\n    WebUI --&gt; Ollama[Ollama Model Server]\n    WebUI --&gt; RAG[RAG System]\n    RAG --&gt; Ollama\n    RAG --&gt; VectorDB[(Vector Database)]\n    Ollama --&gt; Models[(AI Models)]\n    WebUI --&gt; Config[(Configuration)]\n    User --&gt; Obelisk[Obelisk Docs]\n    Obelisk --&gt; DocContent[(Documentation Content)]\n    DocContent --&gt; RAG</code></pre> <p>For a comprehensive view of all components and their interactions, see the complete architecture diagram.</p> <ol> <li>Users interact with the Open WebUI interface at <code>http://localhost:8080</code></li> <li>Queries can be processed either directly by Ollama or through the RAG system</li> <li>When using RAG, the system retrieves relevant content from your documentation</li> <li>Ollama loads and runs AI models to generate responses enhanced with your content</li> <li>The Obelisk documentation server runs independently at <code>http://localhost:8000</code></li> </ol>"},{"location":"chatbot/#services-configuration","title":"Services Configuration","text":""},{"location":"chatbot/#ollama-service","title":"Ollama Service","text":"<p>The Ollama service runs the model server with GPU acceleration:</p> <pre><code>ollama:\n  container_name: ollama\n  image: ollama/ollama:latest\n  runtime: nvidia\n  environment:\n    - NVIDIA_VISIBLE_DEVICES=all\n    - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n    - CUDA_VISIBLE_DEVICES=0\n    - LOG_LEVEL=debug\n  deploy:\n    resources:\n      reservations:\n        devices:\n          - driver: nvidia\n            capabilities: [gpu]\n            count: all\n  volumes:\n    - ollama:/root/.ollama\n    - models:/models\n  ports:\n    - \"11434:11434\"\n  networks:\n    - ollama-net\n  restart: unless-stopped\n</code></pre>"},{"location":"chatbot/#open-webui-service","title":"Open WebUI Service","text":"<p>The Open WebUI service provides the chat interface:</p> <pre><code>open-webui:\n  container_name: open-webui\n  image: ghcr.io/open-webui/open-webui:main\n  environment:\n    - MODEL_DOWNLOAD_DIR=/models\n    - OLLAMA_API_BASE_URL=http://ollama:11434\n    - OLLAMA_API_URL=http://ollama:11434\n    - LOG_LEVEL=debug\n  volumes:\n    - data:/data\n    - models:/models\n    - open-webui:/config\n  ports:\n    - \"8080:8080\"\n  depends_on:\n    - ollama\n  networks:\n    - ollama-net\n  restart: unless-stopped\n</code></pre>"},{"location":"chatbot/#getting-started","title":"Getting Started","text":"<p>To start using the chatbot integration:</p> <ol> <li>Ensure you have Docker and Docker Compose installed</li> <li>For GPU acceleration, install the NVIDIA Container Toolkit</li> <li>Start the full stack:</li> </ol> <pre><code>task compose\n</code></pre> <ol> <li>Access the chat interface at <code>http://localhost:8080</code></li> <li>Access your documentation at <code>http://localhost:8000</code></li> </ol>"},{"location":"chatbot/#available-models","title":"Available Models","text":"<p>By default, no models are pre-loaded. You can pull models through the Open WebUI interface or directly via Ollama commands:</p> <pre><code># Connect to the Ollama container\ndocker exec -it ollama bash\n\n# Pull a model (example: mistral)\nollama pull mistral\n</code></pre> <p>Popular models to consider:</p> <ul> <li><code>llama2</code> - Meta's Llama 2 model</li> <li><code>mistral</code> - Mistral AI's 7B model</li> <li><code>phi</code> - Microsoft's Phi model</li> <li><code>gemma</code> - Google's Gemma model</li> </ul>"},{"location":"chatbot/#customizing-the-chat-experience","title":"Customizing the Chat Experience","text":"<p>You can customize the chat experience by:</p> <ol> <li>Configuring Open WebUI settings through the interface</li> <li>Creating custom model configurations</li> <li>Using the RAG system to enhance responses with your documentation</li> <li>Customizing the RAG system parameters for better retrieval</li> </ol> <p>See the Open WebUI documentation, Ollama documentation, and our RAG documentation for more details.</p>"},{"location":"chatbot/#rag-system-integration","title":"RAG System Integration","text":"<p>The Retrieval Augmented Generation (RAG) system enhances your chatbot with knowledge from your documentation:</p> <ol> <li> <p>Index your documentation:    <pre><code>obelisk-rag index\n</code></pre></p> </li> <li> <p>Start the RAG API server:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>In Open WebUI, add a new API-based model:</p> </li> <li>Name: \"Obelisk RAG\"</li> <li>Base URL: \"http://localhost:8000\"</li> <li>API Path: \"/query\"</li> <li>Request Format: <code>{\"query\": \"{prompt}\"}</code></li> <li>Response Path: \"response\"</li> </ol> <p>For detailed instructions on setting up and using the RAG system, see the RAG Getting Started Guide and Using RAG.</p>"},{"location":"chatbot/architecture/","title":"System Architecture","text":"<p>This page contains a comprehensive architecture diagram for the Obelisk system, showing how all components interact from the document processing pipeline through to the client applications.</p>"},{"location":"chatbot/architecture/#full-architecture-diagram","title":"Full Architecture Diagram","text":"<pre><code>flowchart TB\n    %% STYLE: Optimized layout for vertical scrolling with better horizontal space utilization\n\n    %% Document Processing Pipeline\n    subgraph DocumentProcessing [\"Document Processing Pipeline\"]\n        direction TB\n\n        subgraph Reconciliation [\"Document Reconciliation\"]\n            direction LR\n            VaultDir[\"Vault Directory\"]:::file --&gt; DocReconciler[\"Document Object Reconciler\"]:::process\n            HashTable[\"Document Hash Table\"]:::database --&gt; DocReconciler\n\n            %% Simplified reconciliation flow (horizontal layout)\n            DocReconciler --&gt; NewHash[\"New Document Hash\"]:::process --&gt; HashDecision{\"Hash Exists?\"}:::decision\n            HashDecision -- \"Yes\" --&gt; DiscardDoc[\"Discard Document\"]:::process\n            HashDecision -- \"No\" --&gt; ProcessDoc[\"Process Document\"]:::process\n\n            DocReconciler -- \"Delete\" --&gt; RemoveHash[\"Remove Hash &amp; Vectors\"]:::process\n            DocReconciler -- \"Change\" --&gt; UpdateHash[\"Update Vectors &amp; Reprocess\"]:::process\n            VaultDir -. \"Deleted Files\" .-&gt; DeletedDocs[\"Deleted Documents\"]:::deleted\n        end\n\n        Reconciliation --&gt; Encoder[\"Embedding Generation\"]\n\n        %% Embedding process (more compact)\n        subgraph Encoder\n            direction LR\n            DocChunker[\"Document Chunker\"]:::process --&gt; MetadataGen[\"Metadata Generator\"]:::process --&gt; DocMetadata[\"Document Metadata\"]:::data\n            DocChunker --&gt; VectorGen[\"Vector Generator\"]:::process --&gt; ChunkVectors[\"Chunk Vectors\"]:::data\n            VectorGen -. \"Uses mxbai-embed-large\" .-&gt; ChunkVectors\n        end\n    end\n\n    %% Storage and AI layers side by side\n    subgraph MiddleLayers [\"Data &amp; AI Layers\"]\n        direction LR\n\n        subgraph StorageLayer [\"Vector Persistence\"]\n            direction TB\n            SQLMetadata[\"SQL Metadata DB\"]:::database\n            MilvusDB[\"Milvus Vector DB\"]:::database\n        end\n\n        subgraph AILayer [\"AI Service Layer\"]\n            direction TB\n            subgraph LiteLLMProxy [\"LiteLLM Proxy\"]\n                CloudLLMs[\"Cloud LLMs (OpenAI, Claude)\"]:::service\n                LocalLLMs[\"Local LLMs (Ollama/Llama3/Phi-4)\"]:::service\n            end\n        end\n    end\n\n    %% Integration and Clients (more compact)\n    subgraph BottomLayers [\"Integration &amp; Client Layers\"]\n        direction LR\n\n        subgraph MCPLayer [\"MCP Integration\"]\n            direction TB\n            ObeliskRAGAPI[\"Obelisk RAG API\"]:::api\n            MCPServer[\"MCP Server\"]:::service\n        end\n\n        subgraph ClientApps [\"Client Applications\"]\n            direction TB\n            WebUI[\"OpenWebUI\"]:::client\n            MCPClients[\"Claude Desktop, ChatGPT Desktop, VSCode\"]:::client\n        end\n    end\n\n    %% CONNECTIONS: Simplified for clarity\n    DocMetadata --&gt; SQLMetadata\n    ChunkVectors --&gt; MilvusDB\n\n    %% WebUI direct connections\n    SQLMetadata --&gt; WebUI\n    MilvusDB --&gt; WebUI\n    LiteLLMProxy --&gt; WebUI\n\n    %% MCP pathway\n    SQLMetadata --&gt; ObeliskRAGAPI\n    MilvusDB --&gt; ObeliskRAGAPI\n    LiteLLMProxy --&gt; ObeliskRAGAPI\n    ObeliskRAGAPI --&gt; MCPServer\n    MCPServer --&gt; MCPClients\n\n    %% Main flow\n    DocumentProcessing --&gt; MiddleLayers\n    MiddleLayers --&gt; BottomLayers\n\n    %% STYLING: Enhanced visual appearance\n    classDef process fill:#f9f,stroke:#333,stroke-width:2px\n    classDef data fill:#bbf,stroke:#333,stroke-width:2px\n    classDef file fill:#afa,stroke:#333,stroke-width:2px\n    classDef database fill:#fda,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n    classDef deleted fill:transparent,stroke:#FF6D00,stroke-width:3px\n    classDef service fill:#d9f,stroke:#333,stroke-width:2px\n    classDef api fill:#faa,stroke:#333,stroke-width:2px\n    classDef client fill:#adf,stroke:#333,stroke-width:2px\n    classDef decision fill:#ffb,stroke:#333,stroke-width:2px,shape:diamond\n\n    %% Visual grouping emphasis\n    style DocumentProcessing fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style MiddleLayers fill:#f0f8ff,stroke:#333,stroke-width:2px\n    style BottomLayers fill:#fff0f5,stroke:#333,stroke-width:2px\n    style Reconciliation fill:#f0f0f0,stroke:#333,stroke-width:1px\n    style Encoder fill:#f0f0f0,stroke:#333,stroke-width:1px\n    style StorageLayer fill:#e6f2ff,stroke:#333,stroke-width:1px\n    style AILayer fill:#f0fff0,stroke:#333,stroke-width:1px \n    style MCPLayer fill:#fff0f5,stroke:#333,stroke-width:1px\n    style ClientApps fill:#fffaf0,stroke:#333,stroke-width:1px</code></pre>"},{"location":"chatbot/architecture/#component-descriptions","title":"Component Descriptions","text":""},{"location":"chatbot/architecture/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>The foundation of the Obelisk RAG system is the document processing pipeline, which handles:</p>"},{"location":"chatbot/architecture/#document-reconciliation","title":"Document Reconciliation","text":"<ul> <li>Vault Directory: Source of markdown documents from the Obsidian vault</li> <li>Document Object Reconciler: Determines which documents need processing based on changes</li> <li>Hash Table: Stores document hashes to detect changes</li> <li>Change Detection: Identifies new, modified, and deleted documents</li> </ul>"},{"location":"chatbot/architecture/#embedding-generation","title":"Embedding Generation","text":"<ul> <li>Document Chunker: Breaks documents into semantic chunks for better retrieval</li> <li>Metadata Generator: Extracts and creates metadata for each document and chunk</li> <li>Vector Generator: Creates embeddings using mxbai-embed-large model</li> <li>Chunk Vectors: The embedded vector representations of document chunks</li> </ul>"},{"location":"chatbot/architecture/#data-ai-layers","title":"Data &amp; AI Layers","text":"<p>The middle layers provide data persistence and AI model access:</p>"},{"location":"chatbot/architecture/#vector-persistence","title":"Vector Persistence","text":"<ul> <li>SQL Metadata DB: Stores document metadata and relationships</li> <li>Milvus Vector DB: High-performance vector database for semantic search</li> </ul>"},{"location":"chatbot/architecture/#ai-service-layer","title":"AI Service Layer","text":"<ul> <li>LiteLLM Proxy: Unified interface to multiple LLM providers</li> <li>Cloud LLMs: Access to OpenAI and Anthropic Claude models</li> <li>Local LLMs: Integration with Ollama for running Llama 3, Phi-4, etc.</li> </ul>"},{"location":"chatbot/architecture/#integration-client-layers","title":"Integration &amp; Client Layers","text":"<p>The user-facing components of the system:</p>"},{"location":"chatbot/architecture/#mcp-integration","title":"MCP Integration","text":"<ul> <li>Obelisk RAG API: REST API for accessing RAG capabilities</li> <li>MCP Server: Model Control Protocol server for standardized AI interaction</li> </ul>"},{"location":"chatbot/architecture/#client-applications","title":"Client Applications","text":"<ul> <li>OpenWebUI: Web-based chat interface with direct RAG integration</li> <li>MCP Clients: Desktop and IDE clients that connect via the MCP protocol</li> </ul>"},{"location":"chatbot/architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Document Ingestion: The system monitors the Vault Directory for changes, creating hash values for each document</li> <li>Document Processing: Changed documents are chunked, embedded, and stored in the vector database</li> <li>Storage: Document metadata is stored in SQL, while vector embeddings are stored in Milvus</li> <li>Query Processing: When a user query arrives, relevant documents are retrieved from vector storage</li> <li>LLM Enhancement: Retrieved documents are used to enhance prompts sent to LLM models</li> <li>Client Delivery: Responses are delivered through either OpenWebUI or MCP-compatible clients</li> </ol>"},{"location":"chatbot/architecture/#integration-points","title":"Integration Points","text":"<p>The architecture supports multiple integration pathways:</p> <ol> <li>Direct OpenWebUI Path: For web-based chat interface users</li> <li>MCP Protocol Path: For desktop applications and IDE integrations</li> <li>API Access: For custom integrations with the Obelisk RAG capability</li> </ol> <p>For detailed implementation guidance, see the RAG Implementation Guide and Using RAG sections.</p>"},{"location":"chatbot/containerization/","title":"Obelisk Containerization Architecture","text":"<p>This document provides a comprehensive overview of Obelisk's containerization architecture, focusing on the Docker Compose configuration, container dependencies, and microservice interactions.</p>"},{"location":"chatbot/containerization/#container-stack-overview","title":"Container Stack Overview","text":"<p>Obelisk leverages a microservices architecture implemented using Docker and Docker Compose to provide a complete ecosystem for documentation publishing with AI assistance and RAG capabilities.</p> <pre><code>graph TB\n    %% Container Groups\n    subgraph Core[\"Core Documentation Platform\"]\n        Obelisk[\"obelisk&lt;br/&gt;(MkDocs Server)\"]\n        ObeliskRAG[\"obelisk-rag&lt;br/&gt;(RAG API Service)\"]\n        Tika[\"tika&lt;br/&gt;(Document Processing)\"]\n    end\n\n    subgraph AI[\"AI Model Services\"]\n        Ollama[\"ollama&lt;br/&gt;(Local LLM Host)\"]\n        LiteLLM[\"litellm&lt;br/&gt;(LLM Proxy)\"]\n        LiteLLMDB[\"litellm_db&lt;br/&gt;(PostgreSQL)\"]\n    end\n\n    subgraph Vector[\"Vector Database Ecosystem\"]\n        Milvus[\"milvus&lt;br/&gt;(Vector DB)\"]\n        Etcd[\"etcd&lt;br/&gt;(Metadata Store)\"]\n        MinIO[\"minio&lt;br/&gt;(Object Storage)\"]\n    end\n\n    subgraph Frontend[\"Frontend UI\"]\n        OpenWebUI[\"open-webui&lt;br/&gt;(Chat Interface)\"]\n    end\n\n    %% Dependencies\n    LiteLLMDB --&gt; LiteLLM\n    Etcd --&gt; Milvus\n    MinIO --&gt; Milvus\n\n    Ollama --&gt; LiteLLM\n    Milvus --&gt; ObeliskRAG\n    Ollama --&gt; ObeliskRAG\n    Tika --&gt; OpenWebUI\n\n    ObeliskRAG --&gt; LiteLLM\n    Milvus --&gt; OpenWebUI\n    LiteLLM --&gt; OpenWebUI\n    Ollama --&gt; OpenWebUI\n\n    classDef coreStyle fill:#C5E8B7,stroke:#000,stroke-width:1px\n    classDef aiStyle fill:#BFD7ED,stroke:#000,stroke-width:1px\n    classDef vectorStyle fill:#FFD8BE,stroke:#000,stroke-width:1px\n    classDef frontendStyle fill:#FFB5DA,stroke:#000,stroke-width:1px\n\n    class Obelisk,ObeliskRAG,Tika coreStyle\n    class Ollama,LiteLLM,LiteLLMDB aiStyle\n    class Milvus,Etcd,MinIO vectorStyle\n    class OpenWebUI frontendStyle</code></pre>"},{"location":"chatbot/containerization/#container-services","title":"Container Services","text":""},{"location":"chatbot/containerization/#core-components","title":"Core Components","text":"Service Image Description Ports Key Dependencies obelisk Custom build Documentation server running MkDocs with Material theme 8000 None obelisk-rag Custom build Retrieval-Augmented Generation API service 8001 Ollama, Milvus tika apache/tika:latest-full Document processing and content extraction 9998 None"},{"location":"chatbot/containerization/#ai-services","title":"AI Services","text":"Service Image Description Ports Key Dependencies ollama ollama/ollama:latest Local LLM hosting with GPU support 11434 None litellm ghcr.io/berriai/litellm:main-latest LLM provider proxy for unified API access 4000 litellm_db, ollama litellm_db postgres:16.1 Database for LiteLLM configuration and logging 5432 None"},{"location":"chatbot/containerization/#vector-database","title":"Vector Database","text":"Service Image Description Ports Key Dependencies milvus milvusdb/milvus:v2.5.10 High-performance vector database 19530, 9091 etcd, minio etcd quay.io/coreos/etcd:v3.5.18 Distributed key-value store for Milvus metadata 2379 None minio minio/minio:RELEASE.2023-03-20T20-16-18Z S3-compatible object storage for Milvus 9000, 9001 None"},{"location":"chatbot/containerization/#frontend","title":"Frontend","text":"Service Image Description Ports Key Dependencies open-webui ghcr.io/open-webui/open-webui:main Web interface for chatbot and RAG interaction 8080 litellm, tika, ollama, milvus"},{"location":"chatbot/containerization/#dependency-flow","title":"Dependency Flow","text":"<p>The container services form a layered architecture with specific dependencies:</p> <pre><code>flowchart TD\n    %% Base Infrastructure Layer\n    subgraph Base[\"Base Infrastructure Layer\"]\n        MinIO[\"minio (Object Storage)\"]\n        Etcd[\"etcd (Metadata Store)\"]\n        LiteLLMDB[\"litellm_db (PostgreSQL)\"]\n        Tika[\"tika (Document Processing)\"]\n    end\n\n    %% Core Services Layer\n    subgraph Core[\"Core Services Layer\"]\n        Milvus[\"milvus (Vector DB)\"]\n        Ollama[\"ollama (Local LLM Host)\"]\n        Obelisk[\"obelisk (MkDocs Server)\"]\n    end\n\n    %% Integration Layer\n    subgraph Integration[\"Integration Layer\"]\n        LiteLLM[\"litellm (LLM Proxy)\"]\n        ObeliskRAG[\"obelisk-rag (RAG API)\"]\n    end\n\n    %% User Interface Layer\n    subgraph UI[\"User Interface Layer\"]\n        OpenWebUI[\"open-webui (Chat Interface)\"]\n    end\n\n    %% Dependencies between layers\n    MinIO --&gt; Milvus\n    Etcd --&gt; Milvus\n    LiteLLMDB --&gt; LiteLLM\n\n    Milvus --&gt; ObeliskRAG\n    Ollama --&gt; ObeliskRAG\n    Ollama --&gt; LiteLLM\n\n    ObeliskRAG --&gt; OpenWebUI\n    LiteLLM --&gt; OpenWebUI\n    Tika --&gt; OpenWebUI\n    Milvus --&gt; OpenWebUI\n\n    %% Styling\n    classDef baseStyle fill:#f9f9f9,stroke:#333,stroke-width:1px\n    classDef coreStyle fill:#e6f3ff,stroke:#333,stroke-width:1px\n    classDef integrationStyle fill:#fff2e6,stroke:#333,stroke-width:1px\n    classDef uiStyle fill:#f9e6ff,stroke:#333,stroke-width:1px\n\n    class MinIO,Etcd,LiteLLMDB,Tika baseStyle\n    class Milvus,Ollama,Obelisk coreStyle\n    class LiteLLM,ObeliskRAG integrationStyle\n    class OpenWebUI uiStyle</code></pre>"},{"location":"chatbot/containerization/#data-flow","title":"Data Flow","text":"<p>The container architecture implements several key data flows:</p>"},{"location":"chatbot/containerization/#1-document-processing-flow","title":"1. Document Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Obelisk as Obelisk MkDocs\n    participant ObeliskRAG as Obelisk RAG\n    participant Milvus as Milvus Vector DB\n\n    User-&gt;&gt;+Obelisk: Edit/Add Markdown Documents\n    Obelisk-&gt;&gt;+ObeliskRAG: Document Update Event\n    ObeliskRAG-&gt;&gt;ObeliskRAG: Generate Embeddings\n    ObeliskRAG-&gt;&gt;+Milvus: Store Vectors &amp; Metadata\n    Milvus--&gt;&gt;-ObeliskRAG: Storage Confirmation\n    ObeliskRAG--&gt;&gt;-Obelisk: Indexing Complete</code></pre>"},{"location":"chatbot/containerization/#2-rag-query-flow","title":"2. RAG Query Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant OpenWebUI as Open WebUI\n    participant Milvus as Milvus Vector DB\n    participant ObeliskRAG as Obelisk RAG\n    participant LiteLLM as LiteLLM Proxy\n    participant Ollama as Ollama LLM\n\n    User-&gt;&gt;+OpenWebUI: Ask Question\n    OpenWebUI-&gt;&gt;+ObeliskRAG: Query for Context\n    ObeliskRAG-&gt;&gt;+Milvus: Vector Similarity Search\n    Milvus--&gt;&gt;-ObeliskRAG: Relevant Document Chunks\n    ObeliskRAG--&gt;&gt;-OpenWebUI: Context Information\n    OpenWebUI-&gt;&gt;+LiteLLM: LLM Request with Context\n    LiteLLM-&gt;&gt;+Ollama: Route to Local LLM\n    Ollama--&gt;&gt;-LiteLLM: Generate Response\n    LiteLLM--&gt;&gt;-OpenWebUI: Enhanced Response\n    OpenWebUI--&gt;&gt;-User: Display Answer</code></pre>"},{"location":"chatbot/containerization/#volume-management","title":"Volume Management","text":"<p>The Docker Compose configuration defines persistent volumes for data storage:</p> Volume Name Description Used By data General data storage open-webui models LLM model storage ollama, open-webui ollama Ollama configuration ollama open-webui OpenWebUI configuration open-webui rag-data RAG pipeline data obelisk-rag rag-vault Document vault storage obelisk-rag postgres_data PostgreSQL database files litellm_db milvus_data Milvus database files milvus etcd_data etcd storage etcd minio_data MinIO object storage minio"},{"location":"chatbot/containerization/#network-configuration","title":"Network Configuration","text":"<p>All services are connected through a custom bridge network called <code>ollama-net</code>, which enables: - Service discovery by container name - Isolated communication between containers - Security through network isolation</p> <pre><code>networks:\n  ollama-net:\n    driver: bridge\n</code></pre>"},{"location":"chatbot/containerization/#docker-compose-environment-variables","title":"Docker Compose Environment Variables","text":"<p>The Docker Compose configuration uses environment variables for service configuration:</p>"},{"location":"chatbot/containerization/#openwebui-configuration","title":"OpenWebUI Configuration","text":"<pre><code>- RETRIEVAL_ENABLED=true\n- RETRIEVAL_VECTOR_STORE=milvus\n- MILVUS_URI=http://milvus:19530\n- MILVUS_HOST=milvus\n- MILVUS_PORT=19530\n</code></pre>"},{"location":"chatbot/containerization/#obelisk-rag-configuration","title":"Obelisk-RAG Configuration","text":"<pre><code>- VECTOR_DB=milvus\n- MILVUS_URI=http://milvus:19530\n- MILVUS_HOST=milvus\n- MILVUS_PORT=19530\n</code></pre>"},{"location":"chatbot/containerization/#litellm-configuration","title":"LiteLLM Configuration","text":"<pre><code>- LITELLM_MASTER_KEY=sk-1234\n- LITELLM_ADMIN_PASSWORD=admin\n- DATABASE_URL=postgresql://postgres:postgres@litellm_db:5432/postgres\n- OLLAMA_API_BASE=http://ollama:11434\n</code></pre>"},{"location":"chatbot/containerization/#healthchecks","title":"Healthchecks","text":"<p>The Docker Compose configuration implements healthchecks for critical services:</p> <pre><code>healthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/stats\"]\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 20s\n</code></pre>"},{"location":"chatbot/containerization/#resource-management","title":"Resource Management","text":"<p>The Ollama service is configured with GPU access when available:</p> <pre><code>runtime: nvidia\nenvironment:\n  - NVIDIA_VISIBLE_DEVICES=all\n  - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n  - CUDA_VISIBLE_DEVICES=0\ndeploy:\n  resources:\n    reservations:\n      devices:\n        - driver: nvidia\n          capabilities: [gpu]\n          count: all\n</code></pre>"},{"location":"chatbot/containerization/#container-build-process","title":"Container Build Process","text":"<p>Two custom Dockerfiles are used for building Obelisk services:</p> <ol> <li>Dockerfile: Builds the main Obelisk documentation service</li> <li>Dockerfile.rag: Builds the Obelisk RAG service with additional dependencies</li> </ol>"},{"location":"chatbot/containerization/#deployment-patterns","title":"Deployment Patterns","text":"<p>The Docker Compose configuration supports multiple deployment patterns:</p> <ol> <li> <p>Core Documentation Only:    <pre><code>docker-compose up obelisk\n</code></pre></p> </li> <li> <p>Documentation with RAG:    <pre><code>docker-compose up obelisk obelisk-rag milvus etcd minio\n</code></pre></p> </li> <li> <p>Full AI Stack:    <pre><code>docker-compose up\n</code></pre></p> </li> </ol>"},{"location":"chatbot/containerization/#performance-considerations","title":"Performance Considerations","text":"<p>The containerized architecture is optimized for performance with:</p> <ol> <li>Volume Mounts: Persistent storage for data that needs to survive container restarts</li> <li>Logging Controls: Maximum log file size and rotation to prevent disk space issues    <pre><code>logging:\n  driver: json-file\n  options:\n    max-size: \"5m\"\n    max-file: \"2\"\n</code></pre></li> <li>Restart Policies: Automatic restart for critical services    <pre><code>restart: unless-stopped\n</code></pre></li> </ol>"},{"location":"chatbot/containerization/#container-maintenance","title":"Container Maintenance","text":"<p>To maintain the containerized infrastructure:</p> <ol> <li> <p>Prune Unused Resources:    <pre><code>docker system prune -a\n</code></pre></p> </li> <li> <p>Update Images:    <pre><code>docker-compose pull\ndocker-compose build --no-cache\n</code></pre></p> </li> <li> <p>View Logs:    <pre><code>docker-compose logs -f [service-name]\n</code></pre></p> </li> </ol>"},{"location":"chatbot/containerization/#related-documentation","title":"Related Documentation","text":"<p>For additional information on specific components:</p> <ul> <li>Docker Configuration: Core Docker setup information</li> <li>Architecture Diagram: Full system architecture</li> <li>Vector Database Integration: Details on vector database options</li> <li>Milvus Integration: Specific details on Milvus configuration</li> <li>Using RAG: How to use RAG features</li> </ul>"},{"location":"chatbot/integration/","title":"Integrating AI Chat with Documentation","text":"<p>Learn how to effectively integrate the AI chatbot capabilities with your Obelisk documentation site.</p>"},{"location":"chatbot/integration/#basic-integration","title":"Basic Integration","text":"<p>The Obelisk stack includes Ollama and Open WebUI running alongside your documentation server. Users can access:</p> <ul> <li>Documentation site: <code>http://localhost:8000</code></li> <li>Chat interface: <code>http://localhost:8080</code></li> </ul> <p>This separation allows flexible deployment options while keeping the services connected through a shared Docker network.</p>"},{"location":"chatbot/integration/#advanced-integration-options","title":"Advanced Integration Options","text":""},{"location":"chatbot/integration/#embedding-chat-in-documentation","title":"Embedding Chat in Documentation","text":"<p>To embed the chat interface directly within your documentation pages:</p> <ol> <li>Create a custom HTML template by modifying <code>vault/overrides/main.html</code>:</li> </ol> <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n  {{ super() }}\n\n  &lt;!-- Chat button in corner --&gt;\n  &lt;div class=\"chat-launcher\"&gt;\n    &lt;button class=\"chat-button\" onclick=\"toggleChat()\"&gt;\n      &lt;span class=\"material-icons\"&gt;chat&lt;/span&gt;\n    &lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;!-- Chat iframe container --&gt;\n  &lt;div id=\"chat-container\" class=\"hidden\"&gt;\n    &lt;div class=\"chat-header\"&gt;\n      &lt;span&gt;Obelisk AI Assistant&lt;/span&gt;\n      &lt;button onclick=\"toggleChat()\"&gt;\u00d7&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;iframe id=\"chat-frame\" src=\"http://localhost:8080\"&gt;&lt;/iframe&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n{% block extrahead %}\n  {{ super() }}\n  &lt;style&gt;\n    .chat-launcher {\n      position: fixed;\n      bottom: 20px;\n      right: 20px;\n      z-index: 999;\n    }\n    .chat-button {\n      background: var(--md-primary-fg-color);\n      color: white;\n      border: none;\n      border-radius: 50%;\n      width: 60px;\n      height: 60px;\n      cursor: pointer;\n      box-shadow: 0 2px 10px rgba(0,0,0,0.2);\n    }\n    #chat-container {\n      position: fixed;\n      bottom: 90px;\n      right: 20px;\n      width: 400px;\n      height: 600px;\n      background: white;\n      border-radius: 10px;\n      box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n      z-index: 1000;\n      display: flex;\n      flex-direction: column;\n    }\n    .hidden {\n      display: none !important;\n    }\n    .chat-header {\n      padding: 10px;\n      background: var(--md-primary-fg-color);\n      color: white;\n      border-radius: 10px 10px 0 0;\n      display: flex;\n      justify-content: space-between;\n    }\n    #chat-frame {\n      flex: 1;\n      border: none;\n      border-radius: 0 0 10px 10px;\n    }\n  &lt;/style&gt;\n  &lt;script&gt;\n    function toggleChat() {\n      const container = document.getElementById('chat-container');\n      container.classList.toggle('hidden');\n    }\n  &lt;/script&gt;\n{% endblock %}\n</code></pre> <ol> <li> <p>Add custom CSS in <code>vault/stylesheets/extra.css</code> if needed</p> </li> <li> <p>Update the JavaScript in <code>vault/javascripts/extra.js</code> to handle chat functionality</p> </li> </ol>"},{"location":"chatbot/integration/#training-on-documentation-content","title":"Training on Documentation Content","text":"<p>For more contextual responses about your documentation:</p> <ol> <li>Extract your documentation content:</li> </ol> <pre><code># Create a training data directory\nmkdir -p ~/obelisk-training-data\n\n# Use a script to extract content from markdown files\nfind /workspaces/obelisk/vault -name \"*.md\" -exec sh -c 'cat \"$1\" &gt;&gt; ~/obelisk-training-data/docs.txt' sh {} \\;\n</code></pre> <ol> <li>Create a new Modelfile with your documentation context:</li> </ol> <pre><code>FROM mistral\n\n# Include documentation context\nSYSTEM You are an AI assistant for the Obelisk documentation system. \nSYSTEM You specialize in helping users understand how to use Obelisk to convert their Obsidian vaults to MkDocs Material Theme sites.\nSYSTEM You should give concise, helpful answers based on the official documentation.\n\n# Reference documentation content\nPARAMETER temperature 0.7\nPARAMETER num_ctx 4096\n</code></pre> <ol> <li>Build a custom model:</li> </ol> <pre><code>docker exec -it ollama ollama create obelisk-docs -f /path/to/Modelfile\n</code></pre>"},{"location":"chatbot/integration/#security-considerations","title":"Security Considerations","text":"<p>When integrating AI chat with your documentation:</p> <ol> <li>Access Control:</li> <li>Consider securing the chat interface with authentication</li> <li> <p>Limit network access to the Ollama API</p> </li> <li> <p>Content Filtering:</p> </li> <li>Configure model parameters to avoid harmful outputs</li> <li> <p>Set appropriate system prompts to guide model behavior</p> </li> <li> <p>Privacy:</p> </li> <li>Be aware that conversations may be stored in the <code>data</code> volume</li> <li> <p>Configure data retention policies in Open WebUI</p> </li> <li> <p>Deployment:</p> </li> <li>For public deployments, consider using a reverse proxy</li> <li>Implement rate limiting to prevent abuse</li> </ol>"},{"location":"chatbot/integration/#best-practices","title":"Best Practices","text":"<p>For effective documentation-chat integration:</p> <ol> <li>Clear Distinction: Make it obvious when users are interacting with AI vs. reading documentation</li> <li>Contextual Linking: Have the AI provide links to relevant documentation pages</li> <li>Feedback Loop: Collect user feedback on AI responses to improve over time</li> <li>Fallbacks: Provide easy ways for users to access human help when AI can't solve a problem</li> <li>Monitoring: Track usage patterns to identify documentation gaps</li> </ol>"},{"location":"chatbot/litellm-integration/","title":"LiteLLM Proxy Integration","text":"<p>Obelisk includes integration with LiteLLM proxy to provide access to multiple AI model providers through a single, standardized interface. This integration enhances the capabilities of the chatbot system while maintaining compatibility with the existing Obelisk RAG pipeline.</p>"},{"location":"chatbot/litellm-integration/#overview","title":"Overview","text":"<p>The LiteLLM proxy serves as a middleware layer between OpenWebUI and various LLM providers, offering:</p> <ol> <li>Unified API Access: Connect to 100+ AI models through a single API endpoint</li> <li>API Key Management: Securely manage access credentials for different providers</li> <li>Usage Tracking: Monitor API usage and implement budget controls</li> <li>Routing Logic: Direct requests to the most appropriate model based on configuration</li> <li>Compatibility: Maintain existing Obelisk RAG functionality while adding new capabilities</li> </ol>"},{"location":"chatbot/litellm-integration/#architecture-integration","title":"Architecture Integration","text":"<pre><code>graph TD\n    User[User] --&gt; WebUI[Open WebUI]\n    WebUI --&gt; LiteLLM[LiteLLM Proxy]\n    LiteLLM --&gt; ObeliskRAG[Obelisk RAG]\n    LiteLLM --&gt; OllamaLocal[Ollama Local Models]\n    LiteLLM --&gt; CloudLLMs[Cloud LLMs]\n    LiteLLM --&gt; LiteLLMDB[(LiteLLM Database)]\n    ObeliskRAG --&gt; VectorDB[(Vector Database)]\n    ObeliskRAG --&gt; Ollama[Ollama API]\n    WebUI --&gt; Tika[Apache Tika]\n\n    subgraph \"Cloud Providers\"\n        CloudLLMs --&gt; OpenAI[OpenAI]\n        CloudLLMs --&gt; Gemini[Google Gemini]\n        CloudLLMs --&gt; Claude[Anthropic Claude]\n    end</code></pre>"},{"location":"chatbot/litellm-integration/#components","title":"Components","text":""},{"location":"chatbot/litellm-integration/#litellm-proxy","title":"LiteLLM Proxy","text":"<p>The LiteLLM proxy is the central component of this integration:</p> <ul> <li>Image: <code>ghcr.io/berriai/litellm:main-latest</code></li> <li>Configuration: <code>litellm-config.yaml</code></li> <li>Purpose: Routes API requests to the appropriate model provider</li> </ul>"},{"location":"chatbot/litellm-integration/#postgresql-database","title":"PostgreSQL Database","text":"<p>Stores configuration and usage data for the LiteLLM proxy:</p> <ul> <li>Image: <code>postgres:16.1</code></li> <li>Purpose: Persistent storage for API keys, usage logs, and model configurations</li> </ul>"},{"location":"chatbot/litellm-integration/#apache-tika","title":"Apache Tika","text":"<p>Document processing service for OpenWebUI's built-in RAG capabilities:</p> <ul> <li>Image: <code>apache/tika:latest-full</code></li> <li>Purpose: Extract and process content from various document formats</li> </ul>"},{"location":"chatbot/litellm-integration/#configuration","title":"Configuration","text":""},{"location":"chatbot/litellm-integration/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file based on the provided <code>.env.example</code> template with:</p> <pre><code># Required settings\nLITELLM_MASTER_KEY=your-master-key\nLITELLM_VIRTUAL_KEY=your-virtual-key\n\n# API keys for specific providers (as needed)\nOPENAI_API_KEY=your-openai-key\nGEMINI_API_KEY=your-gemini-key\n</code></pre>"},{"location":"chatbot/litellm-integration/#model-configuration","title":"Model Configuration","text":"<p>The <code>litellm-config.yaml</code> file defines available models and routing logic:</p> <pre><code>model_list:\n  - model_name: ollama/llama3\n    litellm_params:\n      model: ollama/llama3\n      api_base: http://ollama:11434\n\n  - model_name: obelisk-rag/llama3\n    litellm_params:\n      model: openai/gpt-3.5-turbo  # Placeholder\n      api_base: http://obelisk-rag:8000\n\n  # Additional cloud models as needed\n</code></pre>"},{"location":"chatbot/litellm-integration/#using-multiple-model-providers","title":"Using Multiple Model Providers","text":"<p>With this integration, you can:</p> <ol> <li>Use Local Models: Access Ollama models running locally</li> <li>Access Obelisk RAG: Continue using the Obelisk RAG pipeline for document-enriched responses</li> <li>Connect to Cloud Services: Add OpenAI, Gemini, Claude, and other providers</li> </ol>"},{"location":"chatbot/litellm-integration/#adding-a-new-provider","title":"Adding a New Provider","text":"<p>To add a new model provider:</p> <ol> <li>Add the appropriate API key to your <code>.env</code> file</li> <li>Add the model configuration to <code>litellm-config.yaml</code></li> <li>Restart the containers with <code>docker-compose up -d</code></li> </ol> <p>Example for adding Anthropic Claude:</p> <pre><code># In litellm-config.yaml\nmodel_list:\n  # Existing models...\n\n  - model_name: anthropic/claude-3-opus\n    litellm_params:\n      model: anthropic/claude-3-opus\n      api_key: os.environ/ANTHROPIC_API_KEY\n      drop_params: true\n</code></pre>"},{"location":"chatbot/litellm-integration/#switching-between-modes","title":"Switching Between Modes","text":"<p>The system is configured to allow flexible use of different capabilities:</p> <ol> <li>Obelisk RAG: Access document-aware responses through the <code>obelisk-rag/llama3</code> model</li> <li>Direct Ollama: Use local models directly via the <code>ollama/llama3</code> model</li> <li>Cloud Models: Access providers like OpenAI via their respective models</li> </ol>"},{"location":"chatbot/litellm-integration/#admin-interface","title":"Admin Interface","text":"<p>LiteLLM provides an admin interface available at <code>http://localhost:4000/dashboard</code> for:</p> <ul> <li>Monitoring usage</li> <li>Managing API keys</li> <li>Viewing logs</li> <li>Adding/configuring models</li> </ul> <p>Access requires the master key defined in your environment variables.</p>"},{"location":"chatbot/litellm-integration/#compatibility-notes","title":"Compatibility Notes","text":"<p>This integration maintains backward compatibility with the existing Obelisk RAG system:</p> <ul> <li>All existing RAG capabilities continue to function</li> <li>Existing documents and vector databases are preserved</li> <li>The middleware layer adds capabilities without removing functionality</li> </ul>"},{"location":"chatbot/litellm-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chatbot/litellm-integration/#common-issues","title":"Common Issues","text":"<ol> <li>Connection errors: </li> <li>Verify network connectivity between containers</li> <li> <p>Check that all services are running with <code>docker-compose ps</code></p> </li> <li> <p>Authentication failures:</p> </li> <li>Verify API keys in the <code>.env</code> file</li> <li> <p>Check that LiteLLM is correctly loading environment variables</p> </li> <li> <p>Model not found errors:</p> </li> <li>Ensure the model is correctly configured in <code>litellm-config.yaml</code></li> <li>Check that the model provider's API is accessible</li> </ol>"},{"location":"chatbot/litellm-integration/#viewing-logs","title":"Viewing Logs","text":"<p>To view logs for debugging:</p> <pre><code># LiteLLM proxy logs\ndocker-compose logs -f litellm\n\n# Database logs\ndocker-compose logs -f litellm_db\n\n# OpenWebUI logs\ndocker-compose logs -f open-webui\n</code></pre>"},{"location":"chatbot/models/","title":"AI Models Configuration","text":"<p>This guide explains how to configure and use AI models with the Ollama and Open WebUI integration in Obelisk.</p>"},{"location":"chatbot/models/#model-management","title":"Model Management","text":""},{"location":"chatbot/models/#pulling-models","title":"Pulling Models","text":"<p>Models can be pulled through the Open WebUI interface or directly using Ollama:</p> <pre><code># Using Ollama CLI\ndocker exec -it ollama ollama pull mistral\n\n# List available models\ndocker exec -it ollama ollama list\n</code></pre>"},{"location":"chatbot/models/#model-storage","title":"Model Storage","text":"<p>Models are stored in persistent Docker volumes:</p> <ul> <li><code>models</code>: Shared volume for model files</li> <li><code>ollama</code>: Ollama-specific configuration and model registry</li> </ul> <p>This ensures your models persist between container restarts.</p>"},{"location":"chatbot/models/#recommended-models","title":"Recommended Models","text":"<p>Here are some recommended models to use with the Obelisk chatbot integration:</p> Model Size Description Command Llama 2 7B Meta's general purpose model <code>ollama pull llama2</code> Mistral 7B High-performance open model <code>ollama pull mistral</code> Phi-2 2.7B Microsoft's compact model <code>ollama pull phi</code> Gemma 7B Google's lightweight model <code>ollama pull gemma:7b</code> CodeLlama 7B Code-specialized model <code>ollama pull codellama</code> <p>For documentation-specific tasks, consider models that excel at knowledge retrieval and explanation.</p>"},{"location":"chatbot/models/#custom-model-configuration","title":"Custom Model Configuration","text":"<p>You can create custom model configurations using Modelfiles:</p> <ol> <li>Create a Modelfile:</li> </ol> <pre><code>FROM mistral\nSYSTEM You are a helpful documentation assistant for the Obelisk project.\n</code></pre> <ol> <li>Build the custom model:</li> </ol> <pre><code>docker exec -it ollama ollama create obelisk-assistant -f Modelfile\n</code></pre> <ol> <li>Use the custom model in Open WebUI.</li> </ol>"},{"location":"chatbot/models/#hardware-requirements","title":"Hardware Requirements","text":"<p>Model performance depends on available hardware:</p> <ul> <li>7B models: Minimum 8GB VRAM (GPU) or 16GB RAM (CPU)</li> <li>13B models: Minimum 16GB VRAM (GPU) or 32GB RAM (CPU)</li> <li>70B models: Minimum 80GB VRAM (GPU) or distributed setup</li> </ul> <p>For optimal performance, use GPU acceleration with the NVIDIA Container Toolkit.</p>"},{"location":"chatbot/models/#quantization-options","title":"Quantization Options","text":"<p>Ollama supports various quantization levels to balance performance and resource usage:</p> Quantization Quality Memory Usage Example F16 Highest Highest <code>ollama pull mistral:latest</code> Q8_0 High Medium <code>ollama pull mistral:8b-q8_0</code> Q4_K_M Medium Low <code>ollama pull mistral:8b-q4_k_m</code> Q4_0 Lowest Lowest <code>ollama pull mistral:8b-q4_0</code> <p>Choose quantization based on your hardware capabilities and quality requirements.</p>"},{"location":"chatbot/models/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Out of memory errors:</li> <li>Try a smaller model or higher quantization level</li> <li> <p>Reduce context length in Open WebUI settings</p> </li> <li> <p>Slow responses:</p> </li> <li>Ensure GPU acceleration is properly configured</li> <li> <p>Check for other processes using GPU resources</p> </li> <li> <p>Model not found:</p> </li> <li>Verify the model was pulled correctly</li> <li>Check network connectivity to model repositories</li> </ol> <p>For more troubleshooting, consult the Ollama documentation.</p>"},{"location":"chatbot/openwebui/","title":"Open WebUI Configuration","text":"<p>Open WebUI provides a powerful interface for interacting with AI models through Ollama. This guide explains how to configure and customize it for use with Obelisk.</p>"},{"location":"chatbot/openwebui/#basic-configuration","title":"Basic Configuration","text":"<p>Open WebUI is configured through environment variables in the <code>docker-compose.yaml</code> file:</p> <pre><code>open-webui:\n  environment:\n    - MODEL_DOWNLOAD_DIR=/models\n    - OLLAMA_API_BASE_URL=http://ollama:11434\n    - OLLAMA_API_URL=http://ollama:11434\n    - LOG_LEVEL=debug\n</code></pre> <p>These settings establish connection to the Ollama service and configure basic behavior.</p>"},{"location":"chatbot/openwebui/#user-interface-features","title":"User Interface Features","text":"<p>Open WebUI provides several key features:</p>"},{"location":"chatbot/openwebui/#chat-interface","title":"Chat Interface","text":"<p>The main chat interface allows:</p> <ul> <li>Conversational interactions with AI models</li> <li>Code highlighting and formatting</li> <li>File attachment and reference</li> <li>Conversation history and management</li> </ul>"},{"location":"chatbot/openwebui/#model-selection","title":"Model Selection","text":"<p>Users can select from available models with options for:</p> <ul> <li>Parameter adjustment (temperature, top_p, etc.)</li> <li>Context length configuration</li> <li>Model-specific presets</li> </ul>"},{"location":"chatbot/openwebui/#prompt-templates","title":"Prompt Templates","text":"<p>Create and manage prompt templates to:</p> <ul> <li>Define consistent AI behavior</li> <li>Create specialized assistants for different tasks</li> <li>Share templates with your team</li> </ul>"},{"location":"chatbot/openwebui/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"chatbot/openwebui/#custom-branding","title":"Custom Branding","text":"<p>To customize the Open WebUI appearance for your Obelisk deployment:</p> <ol> <li>Mount a custom assets volume:</li> </ol> <pre><code>open-webui:\n  volumes:\n    - ./custom-webui-assets:/app/public/custom\n</code></pre> <ol> <li>Create the following files:</li> <li><code>custom-webui-assets/logo.png</code> - Main logo</li> <li><code>custom-webui-assets/logo-dark.png</code> - Logo for dark mode</li> <li><code>custom-webui-assets/favicon.png</code> - Browser tab icon</li> <li><code>custom-webui-assets/background.png</code> - Login page background</li> </ol>"},{"location":"chatbot/openwebui/#authentication","title":"Authentication","text":"<p>Enable authentication for multi-user setups:</p> <pre><code>open-webui:\n  environment:\n    - ENABLE_USER_AUTH=true\n    - DEFAULT_USER_EMAIL=admin@example.com\n    - DEFAULT_USER_PASSWORD=strongpassword\n</code></pre>"},{"location":"chatbot/openwebui/#api-integration","title":"API Integration","text":"<p>Open WebUI can be integrated with other services via its API:</p> <pre><code>open-webui:\n  environment:\n    - ENABLE_API=true\n    - API_KEY=your-secure-api-key\n</code></pre> <p>This allows programmatic access to model interactions.</p>"},{"location":"chatbot/openwebui/#persistent-data","title":"Persistent Data","text":"<p>Open WebUI stores its data in Docker volumes:</p> <ul> <li><code>data</code>: Conversations, user settings, and app data</li> <li><code>open-webui</code>: Configuration files</li> <li><code>models</code>: Shared with Ollama for model storage</li> </ul> <p>These volumes persist across container restarts and updates.</p>"},{"location":"chatbot/openwebui/#customizing-for-documentation-support","title":"Customizing for Documentation Support","text":"<p>To optimize Open WebUI for documentation support:</p> <ol> <li>Create a specialized preset:</li> <li>Navigate to Settings &gt; Presets</li> <li>Create a new preset named \"Documentation Helper\"</li> <li>Configure with appropriate temperature (0.3-0.5) and parameters</li> <li> <p>Set system prompt to documentation-specific instructions</p> </li> <li> <p>Create documentation-focused prompt templates:</p> </li> <li>\"Explain this concept\"</li> <li>\"How do I configure X\"</li> <li> <p>\"Troubleshoot this error\"</p> </li> <li> <p>Enable RAG (Retrieval Augmented Generation):</p> </li> <li>Upload documentation files through the interface</li> <li>Enable \"Knowledge Base\" feature</li> <li>Configure vector storage settings</li> </ol>"},{"location":"chatbot/openwebui/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Connection errors:</li> <li>Verify network settings in docker-compose</li> <li> <p>Check that Ollama service is running</p> </li> <li> <p>Authentication problems:</p> </li> <li>Reset password using the API</li> <li> <p>Check environment variables for auth settings</p> </li> <li> <p>Performance issues:</p> </li> <li>Adjust interface settings for slower devices</li> <li>Configure page size and context window appropriately</li> </ol>"},{"location":"chatbot/openwebui/#resources","title":"Resources","text":"<ul> <li>Open WebUI GitHub Repository</li> <li>Open WebUI Documentation</li> <li>Ollama Documentation</li> </ul>"},{"location":"chatbot/rag/","title":"Obelisk RAG Documentation","text":"<p>This document serves as the central reference for Obelisk's Retrieval Augmented Generation (RAG) implementation. It provides an overview of components, configuration options, and integration points, while directing readers to specific documentation files for detailed information.</p>"},{"location":"chatbot/rag/#current-implementation","title":"Current Implementation","text":"<p>The Obelisk RAG system provides document indexing, vector storage, and query capabilities through a modular architecture with the following key components:</p>"},{"location":"chatbot/rag/#core-components","title":"Core Components","text":"Component Documentation Purpose Document Processor implementation.md Handles markdown parsing, chunking, and metadata extraction Embedding Service implementation.md Generates vector embeddings using Ollama Vector Storage vector-database.md Stores and retrieves document vectors using ChromaDB Query Pipeline query-pipeline.md Processes queries to retrieve relevant context RAG Service implementation.md Central service orchestrating all RAG components"},{"location":"chatbot/rag/#apis-integration","title":"APIs &amp; Integration","text":"Feature Documentation Status OpenAI-compatible API using-rag.md \u2705 Implemented Ollama Integration ollama-integration.md \u2705 Implemented OpenWebUI Integration openwebui-integration.md \u2705 Implemented"},{"location":"chatbot/rag/#user-guides","title":"User Guides","text":"<ul> <li>getting-started.md - Step-by-step guide with RAG overview and first-time setup</li> <li>using-rag.md - Comprehensive usage documentation with examples</li> <li>implementation-status.md - Current implementation status and roadmap</li> </ul>"},{"location":"chatbot/rag/#vector-database-implementation","title":"Vector Database Implementation","text":"<p>The current implementation uses ChromaDB for vector storage. Future development roadmap includes potential migration to Milvus Lite for improved performance and scalability.</p> Database Status Documentation ChromaDB Current Implementation vector-database.md Milvus Lite Future Roadmap vector-database.md"},{"location":"chatbot/rag/#documentation-progress","title":"Documentation Progress","text":"<p>Based on the analysis in <code>task.md</code>, we've made significant improvements to the documentation:</p>"},{"location":"chatbot/rag/#completed-tasks","title":"Completed Tasks","text":"<p>\u2705 High Priority - Consolidated API to use only the OpenAI-compatible endpoint  - Created comprehensive <code>openwebui-integration.md</code> documentation - Updated vector database documentation to reflect ChromaDB implementation - Fixed WebUI integration inconsistencies</p> <p>\u2705 Medium Priority - Updated API documentation with OpenAI-compatible format - Fixed environment variable inconsistencies (<code>OLLAMA_URL</code> vs <code>OLLAMA_API_URL</code>) - Added detailed document chunking documentation to <code>implementation.md</code> - Documented error handling strategies and debug mode</p>"},{"location":"chatbot/rag/#all-tasks-completed","title":"All Tasks Completed \u2705","text":"<ol> <li>Low Priority Items</li> <li>\u2705 Document the discrepancy between documented embedding batching and implementation</li> <li>\u2705 Update roadmap documentation to clearly mark implemented vs. planned features in architecture-draft.md</li> <li>\u2705 Add debug mode documentation</li> <li>\u2705 Improve troubleshooting guidance with more examples in using-rag.md</li> </ol>"},{"location":"chatbot/rag/#documentation-maintenance-guidelines","title":"Documentation Maintenance Guidelines","text":"<p>When updating the RAG documentation, follow these principles:</p> <ol> <li>Accuracy: Documentation must precisely match the implemented code</li> <li>Completeness: Cover all aspects including setup, configuration, usage, and APIs</li> <li>Examples: Include working code samples for key functionality</li> <li>Future Direction: Clearly label future roadmap items vs. current implementation</li> </ol>"},{"location":"chatbot/rag/#document-update-matrix","title":"Document Update Matrix","text":"When changing... Update these docs API endpoints <code>using-rag.md</code> Configuration options <code>using-rag.md</code>, <code>getting-started.md</code> Vector database <code>vector-database.md</code>, <code>implementation.md</code> Query pipeline <code>query-pipeline.md</code>, <code>implementation.md</code> OpenWebUI integration <code>openwebui-integration.md</code> (to be created) Ollama integration <code>ollama-integration.md</code>"},{"location":"chatbot/rag/#related-documentation","title":"Related Documentation","text":"<p>Architecture documentation and detailed implementation information can be found in separate files:</p> <ul> <li>architecture-draft.md - Technical architecture with implementation status markers</li> <li>implementation-status.md - Implementation roadmap and current status</li> </ul>"},{"location":"chatbot/rag/#implementation-notes","title":"Implementation Notes","text":"<p>The current RAG implementation includes:</p> <ol> <li>Document Processing</li> <li>Markdown parsing with YAML frontmatter extraction</li> <li>RecursiveCharacterTextSplitter for document chunking</li> <li> <p>Real-time file watching with the watchdog library</p> </li> <li> <p>Embedding Generation</p> </li> <li>Ollama integration using mxbai-embed-large model</li> <li>1024-dimension vector embeddings</li> <li> <p>Support for both document and query embedding</p> </li> <li> <p>Vector Storage</p> </li> <li>ChromaDB for vector database storage</li> <li>Metadata filtering for search refinement</li> <li> <p>Local persistence on disk</p> </li> <li> <p>Query Processing</p> </li> <li>Top-k similarity search with configurable parameters</li> <li>Contextual prompt construction</li> <li> <p>LLM response generation via Ollama</p> </li> <li> <p>API &amp; Integration</p> </li> <li>OpenAI-compatible API endpoint for tool integration</li> <li>Source information included in API responses</li> <li>Docker containerization for deployment</li> </ol>"},{"location":"chatbot/rag/#configuration-reference","title":"Configuration Reference","text":"<p>The RAG system can be configured through environment variables:</p> Variable Description Default VAULT_DIR Directory containing markdown files /app/vault CHROMA_DIR Directory for vector database /app/data/chroma_db OLLAMA_URL URL of the Ollama service http://ollama:11434 OLLAMA_MODEL Ollama model for generation llama3 EMBEDDING_MODEL Model for embeddings mxbai-embed-large RETRIEVE_TOP_K Number of document chunks to retrieve 3 API_HOST Host to bind API server 0.0.0.0 API_PORT Port for API server 8000 LOG_LEVEL Logging level INFO RAG_DEBUG Enable debug mode false"},{"location":"chatbot/rag/architecture-draft/","title":"RAG System Design Document: Markdown-Based Knowledge Retrieval","text":"<p>Note on Implementation Status: This document represents the architectural vision for the RAG system. The current implementation uses ChromaDB for vector storage rather than Milvus, which is planned for future versions. Features marked with \"\ud83d\udd04\" are planned for future implementation.</p>"},{"location":"chatbot/rag/architecture-draft/#executive-summary","title":"Executive Summary","text":"<p>This document outlines a comprehensive design for a retrieval-augmented generation (RAG) system that processes markdown documents from a monitored directory, creates vector embeddings, stores them in a vector database, and leverages LangChain to create a pipeline that connects with Ollama-hosted LLMs for intelligent querying and response generation.</p> <p>The design emphasizes: - \u2705 Real-time document processing with change detection - \u2705 State-of-the-art embedding models for optimal semantic understanding - \ud83d\udd04 Horizontally scalable vector storage with Milvus (future enhancement, currently using ChromaDB) - \u2705 Modular pipeline architecture for extensibility - \ud83d\udd04 Comprehensive evaluation metrics for continuous improvement</p>"},{"location":"chatbot/rag/architecture-draft/#1-system-architecture-overview","title":"1. System Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              \u2502    \u2502              \u2502    \u2502              \u2502    \u2502              \u2502\n\u2502   Document   \u2502    \u2502  Embedding   \u2502    \u2502    Vector    \u2502    \u2502   Response   \u2502\n\u2502  Processing  \u2502\u2500\u2500\u2500\u25b6\u2502  Generation  \u2502\u2500\u2500\u2500\u25b6\u2502    Store     \u2502\u2500\u2500\u2500\u25b6\u2502  Generation  \u2502\n\u2502              \u2502    \u2502              \u2502    \u2502  (ChromaDB)  \u2502    \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                                       \u25b2                    \u2502\n       \u2502                                       \u2502                    \u2502\n       \u2502                                       \u2502                    \u2502\n       \u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502                    \u2502\n       \u2502                 \u2502              \u2502      \u2502                    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Metadata   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n                         \u2502   (Stored    \u2502                           \u2502\n                         \u2502  in ChromaDB)\u2502                           \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n                                                                    \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n                         \u2502              \u2502                           \u2502\n                         \u2502     User     \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502  Interface   \u2502\n                         \u2502              \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Implementation Note: The current implementation uses ChromaDB for both vector storage and metadata storage. The separate metadata store component shown in this diagram represents a future enhancement.</p>"},{"location":"chatbot/rag/architecture-draft/#2-component-design","title":"2. Component Design","text":""},{"location":"chatbot/rag/architecture-draft/#21-document-processing-pipeline","title":"2.1 Document Processing Pipeline","text":""},{"location":"chatbot/rag/architecture-draft/#directory-watcher-service","title":"Directory Watcher Service","text":"<p><pre><code>from watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nimport os\nclass MarkdownWatcher(FileSystemEventHandler):\n    def init(self, processor):\n        self.processor = processor\n    def on_created(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.process_file(event.src_path)\n    def on_modified(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.process_file(event.src_path)\n    def on_deleted(self, event):\n        if event.is_directory or not event.src_path.endswith('.md'):\n            return\n        self.processor.delete_file(event.src_path)\nclass DirectoryWatcherService:\n    def init(self, directory_path, processor):\n        self.observer = Observer()\n        self.directory_path = directory_path\n        self.event_handler = MarkdownWatcher(processor)\n    def start(self):\n        self.observer.schedule(self.event_handler, self.directory_path, recursive=True)\n        self.observer.start()\n    def stop(self):\n        self.observer.stop()\n        self.observer.join()\n</code></pre> The directory watcher service uses the Watchdog library to monitor the ./vault directory for changes to markdown files. When changes are detected, the appropriate processing function is called.</p>"},{"location":"chatbot/rag/architecture-draft/#document-processor","title":"Document Processor","text":"<p><pre><code>from langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\nimport frontmatter\nimport hashlib\nclass DocumentProcessor:\n    def init(self, embedding_service, vectorstore_service, metadata_store):\n        self.embedding_service = embedding_service\n        self.vectorstore_service = vectorstore_service\n        self.metadata_store = metadata_store\n        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=512,\n            chunk_overlap=50,\n            separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n        )\n    def process_file(self, file_path):\n        # Generate document hash for tracking changes\n        file_hash = self._hash_file(file_path)\n        # Check if document has changed\n        existing_hash = self.metadata_store.get_document_hash(file_path)\n        if existing_hash == file_hash:\n            print(f\"No changes detected for {file_path}\")\n            return\n        # Extract content and metadata\n        with open(file_path, 'r') as f:\n            content = f.read()\n        try:\n            post = frontmatter.loads(content)\n            metadata = post.metadata\n            content_text = post.content\n        except:\n            metadata = {}\n            content_text = content\n        # Add file path and hash to metadata\n        metadata['source'] = file_path\n        metadata['file_hash'] = file_hash\n        # Split the document\n        docs = self.text_splitter.create_documents([content_text], [metadata])\n        # Delete old vectors if they exist\n        if existing_hash:\n            self.vectorstore_service.delete_document(file_path)\n        # Generate embeddings and store\n        self.embedding_service.embed_documents(docs)\n        self.vectorstore_service.add_documents(docs)\n        # Update metadata store\n        self.metadata_store.update_document_metadata(file_path, metadata, file_hash)\n    def delete_file(self, file_path):\n        self.vectorstore_service.delete_document(file_path)\n        self.metadata_store.delete_document(file_path)\n    def process_all_files(self, directory_path):\n        for root, , files in os.walk(directorypath):\n            for file in files:\n                if file.endswith('.md'):\n                    file_path = os.path.join(root, file)\n                    self.process_file(file_path)\n    def hashfile(self, file_path):\n        with open(file_path, 'rb') as f:\n            return hashlib.md5(f.read()).hexdigest()\n</code></pre> The Document Processor handles loading, parsing, and processing markdown files. It uses frontmatter for parsing YAML frontmatter in markdown files, and implements a hashing mechanism to track changes and avoid redundant processing.</p>"},{"location":"chatbot/rag/architecture-draft/#22-embedding-generation","title":"2.2 Embedding Generation","text":"<p><pre><code>from langchain_ollama import OllamaEmbeddings\nimport numpy as np\nclass EmbeddingService:\n    def init(self, model_name=\"mxbai-embed-large\", batch_size=32):\n        self.embeddings_model = OllamaEmbeddings(\n            model=model_name,\n            base_url=\"http://localhost:11434\"\n        )\n        self.batch_size = batch_size\n    def embed_documents(self, documents):\n        \"\"\"Generate embeddings for a list of documents\"\"\"\n        texts = [doc.page_content for doc in documents]\n        # Process in batches to avoid memory issues\n        all_embeddings = []\n        for i in range(0, len(texts), self.batch_size):\n            batch_texts = texts[i:i+self.batch_size]\n            batch_embeddings = self.embeddings_model.embed_documents(batch_texts)\n            all_embeddings.extend(batch_embeddings)\n        # Add embeddings to document objects\n        for i, doc in enumerate(documents):\n            doc.embedding = all_embeddings[i]\n        return documents\n    def embed_query(self, query):\n        \"\"\"Generate embedding for a query string\"\"\"\n        return self.embeddings_model.embed_query(query)\n</code></pre> The Embedding Service leverages Ollama to generate high-quality embeddings using the mxbai-embed-large model. It includes batch processing to handle large document sets efficiently.</p>"},{"location":"chatbot/rag/architecture-draft/#23-vector-storage","title":"2.3 Vector Storage","text":""},{"location":"chatbot/rag/architecture-draft/#planned-future-implementation-with-milvus","title":"\ud83d\udd04 Planned Future Implementation with Milvus","text":"<p>Implementation Note: The current implementation uses ChromaDB instead of Milvus. The code below represents the planned future implementation with Milvus for enhanced scalability. <pre><code>from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType\nimport numpy as np\nimport uuid\nclass MilvusVectorStore:\n    def init(self, host=\"localhost\", port=\"19530\", embedding_dim=1024):\n        self.embedding_dim = embedding_dim\n        self.collection_name = \"markdown_documents\"\n        # Connect to Milvus\n        connections.connect(host=host, port=port)\n        # Create collection if it doesn't exist\n        if not utility.has_collection(self.collection_name):\n            self._create_collection()\n    def createcollection(self):\n        fields = [\n            FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=100),\n            FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=100),\n            FieldSchema(name=\"chunk_id\", dtype=DataType.INT64),\n            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n            FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1000),\n            FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim)\n        ]\n        schema = CollectionSchema(fields=fields)\n        collection = Collection(name=self.collection_name, schema=schema)\n        # Create index for fast retrieval\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"HNSW\",\n            \"params\": {\"M\": 8, \"efConstruction\": 200}\n        }\n        collection.create_index(field_name=\"embedding\", index_params=index_params)\n        print(f\"Created Milvus collection: {self.collection_name}\")\n    def add_documents(self, documents):\n        collection = Collection(self.collection_name)\n        entities = []\n        for i, doc in enumerate(documents):\n            # Generate document ID if needed\n            doc_id = doc.metadata.get(\"source\", str(uuid.uuid4()))\n            # Prepare entity\n            entity = {\n                \"id\": f\"{doc_id}_{i}\",\n                \"document_id\": doc_id,\n                \"chunk_id\": i,\n                \"text\": doc.page_content,\n                \"source\": doc.metadata.get(\"source\", \"\"),\n                \"metadata\": doc.metadata,\n                \"embedding\": doc.embedding\n            }\n            entities.append(entity)\n        # Insert data in batches\n        collection.insert(entities)\n        collection.flush()\n        print(f\"Added {len(documents)} documents to Milvus\")\n    def search(self, query_embedding, top_k=5, filter=None):\n        collection = Collection(self.collection_name)\n        collection.load()\n        search_params = {\"metric_type\": \"L2\", \"params\": {\"ef\": 128}}\n        results = collection.search(\n            data=[query_embedding],\n            anns_field=\"embedding\",\n            param=search_params,\n            limit=top_k,\n            expr=filter,\n            output_fields=[\"text\", \"source\", \"metadata\"]\n        )\n        matches = []\n        for hits in results:\n            for hit in hits:\n                matches.append({\n                    \"id\": hit.id,\n                    \"score\": hit.score,\n                    \"text\": hit.entity.get(\"text\"),\n                    \"source\": hit.entity.get(\"source\"),\n                    \"metadata\": hit.entity.get(\"metadata\")\n                })\n        collection.release()\n        return matches\n    def delete_document(self, document_id):\n        collection = Collection(self.collection_name)\n        expr = f'document_id == \"{document_id}\"'\n        collection.delete(expr)\n        print(f\"Deleted document: {document_id}\")\n</code></pre> The Milvus Vector Store service provides efficient storage and retrieval of document embeddings using Milvus's HNSW indexing for fast similarity search. It handles document addition, deletion, and searching with metadata filtering capabilities.</p>"},{"location":"chatbot/rag/architecture-draft/#24-metadata-store","title":"2.4 Metadata Store","text":"<p><pre><code>import sqlite3\nimport json\nclass SQLiteMetadataStore:\n    def init(self, db_path=\"metadata.db\"):\n        self.db_path = db_path\n        self._create_tables()\n    def createtables(self):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        # Create documents table\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS documents (\n            id TEXT PRIMARY KEY,\n            title TEXT,\n            hash TEXT,\n            metadata TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n        ''')\n        conn.commit()\n        conn.close()\n    def update_document_metadata(self, document_id, metadata, document_hash):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata)\n        # Check if document exists\n        cursor.execute(\"SELECT id FROM documents WHERE id = ?\", (document_id,))\n        exists = cursor.fetchone()\n        if exists:\n            cursor.execute('''\n            UPDATE documents SET\n                title = ?,\n                hash = ?,\n                metadata = ?,\n                updated_at = CURRENT_TIMESTAMP\n            WHERE id = ?\n            ''', (metadata.get('title', document_id), document_hash, metadata_json, document_id))\n        else:\n            cursor.execute('''\n            INSERT INTO documents (id, title, hash, metadata)\n            VALUES (?, ?, ?, ?)\n            ''', (document_id, metadata.get('title', document_id), document_hash, metadata_json))\n        conn.commit()\n        conn.close()\n    def get_document_hash(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT hash FROM documents WHERE id = ?\", (document_id,))\n        result = cursor.fetchone()\n        conn.close()\n        if result:\n            return result[0]\n        return None\n    def get_document_metadata(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT metadata FROM documents WHERE id = ?\", (document_id,))\n        result = cursor.fetchone()\n        conn.close()\n        if result:\n            return json.loads(result[0])\n        return None\n    def delete_document(self, document_id):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"DELETE FROM documents WHERE id = ?\", (document_id,))\n        conn.commit()\n        conn.close()\n    def list_all_documents(self):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT id, title, updated_at FROM documents ORDER BY updated_at DESC\")\n        results = cursor.fetchall()\n        conn.close()\n        return [{\"id\": r[0], \"title\": r[1], \"updated_at\": r[2]} for r in results]\n</code></pre> The SQLite Metadata Store provides efficient storage and retrieval of document metadata, including tracking document hashes to detect changes and avoid redundant processing.</p>"},{"location":"chatbot/rag/architecture-draft/#25-langchain-rag-pipeline","title":"2.5 LangChain RAG Pipeline","text":"<p><pre><code>from langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_ollama import Ollama\nfrom typing import List, Dict, Any\nclass RAGPipeline:\n    def init(\n        self,\n        embedding_service,\n        vectorstore_service,\n        metadata_store,\n        model_name=\"llama3\",\n        temperature=0.1,\n        top_k=5\n    ):\n        self.embedding_service = embedding_service\n        self.vectorstore_service = vectorstore_service\n        self.metadata_store = metadata_store\n        self.model_name = model_name\n        self.temperature = temperature\n        self.top_k = top_k\n        # Initialize LLM\n        self.llm = Ollama(\n            model=model_name,\n            temperature=temperature\n        )\n        # Create QA prompt\n        self.qa_prompt = PromptTemplate(\n            template=\"\"\"You are an assistant that helps users find information in a collection of markdown documents.\n            Answer the question based on the following context:\n            {context}\n            Question: {question}\n            Provide a comprehensive answer. If the answer cannot be found in the context, say so clearly.\n            Include relevant source information when possible.\n            \"\"\",\n            input_variables=[\"context\", \"question\"]\n        )\n    def query(self, query_text, filter_metadata=None):\n        # Embed the query\n        query_embedding = self.embedding_service.embed_query(query_text)\n        # Construct filter expression if needed\n        filter_expr = None\n        if filter_metadata:\n            filter_parts = []\n            for key, value in filter_metadata.items():\n                filter_parts.append(f'metadata[\"{key}\"] == \"{value}\"')\n            filter_expr = \" &amp;&amp; \".join(filter_parts)\n        # Retrieve relevant documents\n        results = self.vectorstore_service.search(\n            query_embedding=query_embedding,\n            top_k=self.top_k,\n            filter=filter_expr\n        )\n        # Construct context from results\n        context_texts = []\n        sources = []\n        for result in results:\n            context_texts.append(result[\"text\"])\n            if result[\"source\"]:\n                sources.append(result[\"source\"])\n        context = \"\\n\\n\".join(context_texts)\n        # Generate response with LLM\n        prompt = self.qa_prompt.format(context=context, question=query_text)\n        response = self.llm.invoke(prompt)\n        return {\n            \"query\": query_text,\n            \"response\": response,\n            \"sources\": list(set(sources)),\n            \"results\": results\n        }\n</code></pre> The RAG Pipeline brings together the embedding service, vector store, and LLM to create a complete retrieval-augmented generation system. It handles query embedding, retrieval of relevant documents, and generation of responses using the selected Ollama model.</p>"},{"location":"chatbot/rag/architecture-draft/#26-api-and-interface","title":"2.6 API and Interface","text":"<p><pre><code>from fastapi import FastAPI, HTTPException, Body\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional, Any\nimport uvicorn\nimport os\napp = FastAPI(title=\"Markdown RAG API\")\n# Initialize services\nembedding_service = EmbeddingService(model_name=\"mxbai-embed-large\")\nmetadata_store = SQLiteMetadataStore(db_path=\"metadata.db\")\nvectorstore_service = MilvusVectorStore(\n    host=os.environ.get(\"MILVUS_HOST\", \"localhost\"),\n    port=os.environ.get(\"MILVUS_PORT\", \"19530\")\n)\ndocument_processor = DocumentProcessor(\n    embedding_service=embedding_service,\n    vectorstore_service=vectorstore_service,\n    metadata_store=metadata_store\n)\nrag_pipeline = RAGPipeline(\n    embedding_service=embedding_service,\n    vectorstore_service=vectorstore_service,\n    metadata_store=metadata_store,\n    model_name=os.environ.get(\"OLLAMA_MODEL\", \"llama3\")\n)\n# Initialize directory watcher\nwatcher_service = DirectoryWatcherService(\n    directory_path=os.environ.get(\"VAULT_DIR\", \"./vault\"),\n    processor=document_processor\n)\n# Start directory watcher on startup\n@app.on_event(\"startup\")\nasync def startup_event():\n    # Process all existing files first\n    print(f\"Processing existing files in {os.environ.get('VAULT_DIR', './vault')}\")\n    document_processor.process_all_files(os.environ.get(\"VAULT_DIR\", \"./vault\"))\n    # Start watching for changes\n    print(\"Starting directory watcher\")\n    watcher_service.start()\n# Shutdown directory watcher on shutdown\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    print(\"Stopping directory watcher\")\n    watcher_service.stop()\n# Define API models\nclass QueryRequest(BaseModel):\n    query: str\n    filter_metadata: Optional[Dict[str, Any]] = None\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    top_k: Optional[int] = None\nclass QueryResponse(BaseModel):\n    query: str\n    response: str\n    sources: List[str]\n# Define API endpoints\n@app.post(\"/api/query\", response_model=QueryResponse)\nasync def query(request: QueryRequest = Body(...)):\n    try:\n        # Use provided model params or defaults\n        model = request.model or rag_pipeline.model_name\n        temperature = request.temperature or rag_pipeline.temperature\n        top_k = request.top_k or rag_pipeline.top_k\n        # Create a customized pipeline if needed\n        if model != rag_pipeline.model_name or temperature != rag_pipeline.temperature or top_k != rag_pipeline.top_k:\n            custom_pipeline = RAGPipeline(\n                embedding_service=embedding_service,\n                vectorstore_service=vectorstore_service,\n                metadata_store=metadata_store,\n                model_name=model,\n                temperature=temperature,\n                top_k=top_k\n            )\n            result = custom_pipeline.query(request.query, request.filter_metadata)\n        else:\n            result = rag_pipeline.query(request.query, request.filter_metadata)\n        return {\n            \"query\": result[\"query\"],\n            \"response\": result[\"response\"],\n            \"sources\": result[\"sources\"]\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n@app.get(\"/api/documents\")\nasync def list_documents():\n    try:\n        documents = metadata_store.list_all_documents()\n        return documents\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n@app.post(\"/api/reindex\")\nasync def reindex():\n    try:\n        document_processor.process_all_files(os.environ.get(\"VAULT_DIR\", \"./vault\"))\n        return {\"status\": \"success\", \"message\": \"Reindexing complete\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n# Run the API server\nif name == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> The API provides RESTful endpoints for querying the RAG system, listing indexed documents, and triggering reindexing. It uses FastAPI for high performance and includes Pydantic models for request/response validation.</p>"},{"location":"chatbot/rag/architecture-draft/#3-docker-deployment-configuration","title":"3. Docker Deployment Configuration","text":"<pre><code>version: '3.8'\nservices:\n  # LLM service\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n    restart: unless-stopped\n  # Web UI for Ollama\n  openwebui:\n    image: ghcr.io/open-webui/open-webui:latest\n    depends_on:\n      - ollama\n    ports:\n      - \"8080:8080\"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    restart: unless-stopped\n  # Vector database\n  milvus:\n    image: milvusdb/milvus:v2.3.2\n    ports:\n      - \"19530:19530\"\n      - \"9091:9091\"\n    environment:\n      - ETCD_ENDPOINTS=etcd:2379\n      - MINIO_ADDRESS=minio:9000\n    volumes:\n      - milvus_data:/var/lib/milvus\n    restart: unless-stopped\n    depends_on:\n      - etcd\n      - minio\n  etcd:\n    image: quay.io/coreos/etcd:v3.5.5\n    environment:\n      - ETCD_AUTO_COMPACTION_MODE=revision\n      - ETCD_AUTO_COMPACTION_RETENTION=1000\n      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n      - ETCD_SNAPSHOT_COUNT=50000\n    volumes:\n      - etcd_data:/etcd\n    command: etcd --advertise-client-urls=http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n    restart: unless-stopped\n  minio:\n    image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n    environment:\n      - MINIO_ACCESS_KEY=minioadmin\n      - MINIO_SECRET_KEY=minioadmin\n    ports:\n      - \"9000:9000\"\n    volumes:\n      - minio_data:/data\n    command: minio server /data\n    restart: unless-stopped\n  # RAG API service\n  rag-api:\n    build:\n      context: .\n      dockerfile: Dockerfile.rag\n    ports:\n      - \"8000:8000\"\n    environment:\n      - VAULT_DIR=/vault\n      - MILVUS_HOST=milvus\n      - MILVUS_PORT=19530\n      - OLLAMA_URL=http://ollama:11434\n      - OLLAMA_MODEL=llama3\n    volumes:\n      - ./vault:/vault\n      - rag_data:/app/data\n    depends_on:\n      - ollama\n      - milvus\n    restart: unless-stopped\n  # Minimal UI for RAG testing\n  streamlit-ui:\n    build:\n      context: .\n      dockerfile: Dockerfile.ui\n    ports:\n      - \"8501:8501\"\n    environment:\n      - RAG_API_URL=http://rag-api:8000\n    depends_on:\n      - rag-api\n    restart: unless-stopped\nvolumes:\n  ollama_data:\n  milvus_data:\n  etcd_data:\n  minio_data:\n  rag_data:\n</code></pre>"},{"location":"chatbot/rag/architecture-draft/#4-implementation-details-and-best-practices","title":"4. Implementation Details and Best Practices","text":""},{"location":"chatbot/rag/architecture-draft/#41-markdown-processing-strategies","title":"4.1 Markdown Processing Strategies","text":"<p>For optimal handling of markdown files, we implement: 1. Hierarchical Chunking: Split documents based on heading structure to maintain context. 2. Metadata Extraction: Parse YAML frontmatter for enhanced filtering and context. 3. Link Resolution: Handle internal links (e.g., [[wiki-style]] links) to maintain cross-references. 4. Code Block Handling: Special processing for code blocks to preserve formatting and syntax.</p>"},{"location":"chatbot/rag/architecture-draft/#42-embedding-model-selection-and-optimization","title":"4.2 Embedding Model Selection and Optimization","text":"<p>The mxbai-embed-large model provides excellent performance for semantic understanding of technical content. Key considerations: 1. Dimension Reduction: Consider implementing PCA for large collections to reduce storage requirements. 2. Batch Processing: Process embeddings in batches to optimize throughput. 3. Caching: Implement caching for frequently accessed embeddings. 4. Quantization: For larger collections, consider quantizing embeddings to reduce storage and memory footprint.</p>"},{"location":"chatbot/rag/architecture-draft/#43-vector-database-configuration-and-optimization","title":"4.3 Vector Database Configuration and Optimization","text":""},{"location":"chatbot/rag/architecture-draft/#current-chromadb-configuration","title":"\u2705 Current ChromaDB Configuration","text":"<p>The current implementation uses ChromaDB with the following configuration:</p> <ol> <li>Persistence: Local directory storage at the configured location</li> <li>Embedding Integration: Direct integration with Ollama embedding models </li> <li>Default Settings: Using ChromaDB's default HNSW parameters</li> <li>Metadata Handling: Filtering of complex data types for compatibility</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#future-milvus-configuration-planned","title":"\ud83d\udd04 Future Milvus Configuration (Planned)","text":"<p>For optimal Milvus performance (future implementation):</p> <ol> <li>Index Selection: HNSW (Hierarchical Navigable Small World) provides the best balance of accuracy and performance</li> <li>Parameter Tuning:</li> <li>M: Controls the maximum number of connections per node (8-16 recommended)</li> <li>efConstruction: Controls index build quality (100-200 recommended)</li> <li>ef: Controls search accuracy (50-150 recommended)</li> <li>Resource Allocation: Configure adequate memory for Milvus, especially for the index</li> <li>Collection Design: Use partitioning for larger collections to improve query performance</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#44-advanced-rag-techniques","title":"4.4 Advanced RAG Techniques","text":"<p>Implementation Note: The following techniques are planned for future enhancements and are not part of the current implementation.</p>"},{"location":"chatbot/rag/architecture-draft/#future-rag-enhancements-planned","title":"\ud83d\udd04 Future RAG Enhancements (Planned)","text":"<ol> <li> <p>Query Reformulation: Process user queries to improve retrieval effectiveness:    <pre><code># PLANNED ENHANCEMENT\ndef preprocess_query(query):\n    # Expand acronyms, handle synonyms, etc.\n    # ...\n    return processed_query\n</code></pre></p> </li> <li> <p>Hybrid Search: Combine vector similarity with keyword search for improved recall:    <pre><code># PLANNED ENHANCEMENT\ndef hybrid_search(query, vectorstore, metadata_store):\n    # Vector search\n    vector_results = vectorstore.search(query_embedding)\n    # Keyword search\n    keyword_results = metadata_store.keyword_search(query)\n    # Combine results with appropriate weighting\n    combined_results = combine_search_results(vector_results, keyword_results)\n    return combined_results\n</code></pre></p> </li> <li> <p>Reranking: Implement a two-stage retrieval process to refine results:    <pre><code># PLANNED ENHANCEMENT\ndef rerank_results(query, initial_results, reranker_model):\n    query_doc_pairs = [(query, result[\"text\"]) for result in initial_results]\n    scores = reranker_model.compute_scores(query_doc_pairs)\n    # Sort by reranker scores\n    reranked_results = [\n        (initial_results[i], scores[i])\n        for i in range(len(initial_results))\n    ]\n    reranked_results.sort(key=lambda x: x[1], reverse=True)\n    return [item[0] for item in reranked_results]\n</code></pre></p> </li> <li> <p>LLM Agents for Query Planning:    <pre><code># PLANNED ENHANCEMENT\ndef agent_based_query(query, rag_pipeline):\n    # Analyze query to determine approach\n    query_plan = rag_pipeline.llm.invoke(f\"\"\"\n    Analyze this query and create a search plan:\n    Query: {query}\n    What kind of information is needed? Should I:\n    1. Perform a direct search\n    2. Break this into sub-questions\n    3. Filter by specific metadata\n    Plan:\n    \"\"\")\n    # Execute the plan\n    if \"sub-questions\" in query_plan:\n        # Handle multi-hop retrieval\n        # ...\n    else:\n        # Direct retrieval\n        return rag_pipeline.query(query)\n</code></pre></p> </li> </ol>"},{"location":"chatbot/rag/architecture-draft/#45-evaluation-and-monitoring","title":"4.5 Evaluation and Monitoring","text":"<p>Implementation Note: The following evaluation metrics are planned for future implementation to measure and improve RAG performance.</p>"},{"location":"chatbot/rag/architecture-draft/#planned-evaluation-framework","title":"\ud83d\udd04 Planned Evaluation Framework","text":"<ol> <li>Retrieval Evaluation:</li> <li>Mean Reciprocal Rank (MRR)</li> <li>Normalized Discounted Cumulative Gain (NDCG)</li> <li> <p>Precision@K and Recall@K</p> </li> <li> <p>Response Quality Evaluation:</p> </li> <li>Factual Accuracy</li> <li>Answer Relevance</li> <li>Citation Accuracy</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#current-evaluation-methods","title":"\u2705 Current Evaluation Methods","text":"<p>The current implementation provides basic statistics and manual evaluation:</p> <ul> <li>Document count tracking</li> <li>Source file listing</li> <li>Manual verification of responses</li> </ul>"},{"location":"chatbot/rag/architecture-draft/#planned-system-monitoring","title":"\ud83d\udd04 Planned System Monitoring","text":"<ol> <li>System Monitoring:</li> <li>Query latency</li> <li>Embedding generation throughput</li> <li>Vector store query performance</li> <li>LLM response time</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#5-extension-points","title":"5. Extension Points","text":"<p>Implementation Note: The following are potential extension points for future development beyond the current implementation.</p>"},{"location":"chatbot/rag/architecture-draft/#planned-future-extensions","title":"\ud83d\udd04 Planned Future Extensions","text":"<p>The modular design allows for several extensions:</p> <ol> <li>Multi-Modal Support: Extend to handle images and other media in markdown</li> <li>Semantic Caching: Implement a semantic cache for similar queries</li> <li>Custom Embedding Models: Allow customization of embedding models based on domain</li> <li>Advanced Vector Database Integration: Support for Milvus and other scalable vector databases</li> <li>Hybrid Search Implementation: Combine vector search with keyword-based search</li> <li>User Feedback Integration: Capture user feedback to improve retrieval and generation</li> <li>Self-Critique and Refinement: Implement self-evaluation and refinement of responses</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#6-testing-strategy","title":"6. Testing Strategy","text":"<p>Implementation Note: The current implementation includes basic unit tests. The comprehensive testing framework described below is planned for future development.</p>"},{"location":"chatbot/rag/architecture-draft/#planned-testing-framework","title":"\ud83d\udd04 Planned Testing Framework","text":"<p>Comprehensive testing includes:</p> <ol> <li>Unit Tests: For individual components</li> <li>Integration Tests: For component interactions</li> <li>End-to-End Tests: Using a test corpus of markdown documents</li> <li>Performance Testing: Under various loads and document sizes</li> <li>Regression Testing: To ensure continued quality as the system evolves</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#current-testing-implementation","title":"\u2705 Current Testing Implementation","text":"<p>The current implementation includes:</p> <ol> <li>Basic Unit Tests: Testing core functionality of each component</li> <li>Service-level Tests: Verifying RAG service integration</li> <li>Document Processing Tests: Ensuring correct handling of markdown files</li> </ol>"},{"location":"chatbot/rag/architecture-draft/#7-appendix","title":"7. Appendix","text":""},{"location":"chatbot/rag/architecture-draft/#71-installation-instructions","title":"7.1 Installation Instructions","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/markdown-rag.git\ncd markdown-rag\n# Build and start services\ndocker-compose up -d\n# Download required models\ncurl -X POST http://localhost:11434/api/pull -d '{\"name\": \"mxbai-embed-large\"}'\ncurl -X POST http://localhost:11434/api/pull -d '{\"name\": \"llama3\"}'\n# Verify installation\ncurl http://localhost:8000/api/documents\n</code></pre>"},{"location":"chatbot/rag/architecture-draft/#72-api-documentation","title":"7.2 API Documentation","text":"<p>The API is documented using OpenAPI and accessible at http://localhost:8000/docs when the service is running.</p>"},{"location":"chatbot/rag/architecture-draft/#73-performance-benchmarks","title":"7.3 Performance Benchmarks","text":"<p>Initial benchmarks with a corpus of 1,000 markdown files (~10MB total): - Document processing: ~5 documents/second - Query latency: ~500ms (including embedding generation and retrieval) - Memory usage: ~2GB (Milvus) + ~1GB (Python services)</p>"},{"location":"chatbot/rag/evaluation/","title":"RAG Pipeline Evaluation","text":"<p>This page outlines how to evaluate and optimize the performance of the Obelisk RAG system.</p>"},{"location":"chatbot/rag/evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The RAG pipeline will be evaluated using several key metrics:</p>"},{"location":"chatbot/rag/evaluation/#retrieval-metrics","title":"Retrieval Metrics","text":"<ul> <li>Precision@k: Proportion of relevant documents in the top k results</li> <li>Recall@k: Proportion of all relevant documents that appear in the top k</li> <li>Mean Reciprocal Rank (MRR): Average position of the first relevant document</li> <li>Normalized Discounted Cumulative Gain (NDCG): Measures ranking quality</li> </ul>"},{"location":"chatbot/rag/evaluation/#generation-metrics","title":"Generation Metrics","text":"<ul> <li>Answer Relevance: How relevant the answer is to the question</li> <li>Factual Correctness: Whether the answer contains factual errors</li> <li>Hallucination Rate: Proportion of generated content not supported by context</li> <li>Citation Accuracy: Whether sources are accurately cited</li> <li>Completeness: Whether the answer fully addresses the question</li> </ul>"},{"location":"chatbot/rag/evaluation/#system-metrics","title":"System Metrics","text":"<ul> <li>Latency: End-to-end response time</li> <li>Token Efficiency: Number of tokens used vs. information conveyed</li> <li>Resource Usage: Memory and CPU consumption</li> <li>Throughput: Queries processed per second</li> </ul>"},{"location":"chatbot/rag/evaluation/#evaluation-framework","title":"Evaluation Framework","text":"<p>The RAG pipeline will include a built-in evaluation framework:</p> <pre><code># Future implementation example\nclass RAGEvaluator:\n    def __init__(self, config):\n        self.config = config\n        self.test_cases = self._load_test_cases()\n\n    def _load_test_cases(self):\n        \"\"\"Load test cases from configuration.\"\"\"\n        # Implementation details\n\n    def evaluate_retrieval(self, query_processor):\n        \"\"\"Evaluate retrieval performance.\"\"\"\n        results = {}\n\n        for test_case in self.test_cases:\n            query = test_case[\"query\"]\n            relevant_docs = test_case[\"relevant_docs\"]\n\n            retrieved = query_processor.process_query(query)\n            retrieved_ids = [doc[\"id\"] for doc in retrieved[\"retrieved_chunks\"]]\n\n            results[query] = {\n                \"precision\": self._calculate_precision(retrieved_ids, relevant_docs),\n                \"recall\": self._calculate_recall(retrieved_ids, relevant_docs),\n                \"mrr\": self._calculate_mrr(retrieved_ids, relevant_docs),\n                \"ndcg\": self._calculate_ndcg(retrieved_ids, relevant_docs)\n            }\n\n        return results\n\n    def evaluate_generation(self, rag_pipeline):\n        \"\"\"Evaluate generation quality.\"\"\"\n        # Implementation details\n\n    def evaluate_system(self, rag_pipeline):\n        \"\"\"Evaluate system performance.\"\"\"\n        # Implementation details\n\n    def generate_report(self, results):\n        \"\"\"Generate evaluation report.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/evaluation/#synthetic-test-suite","title":"Synthetic Test Suite","text":"<p>The evaluation framework will include a synthetic test suite:</p> <ol> <li>Query Generation: Generate realistic user queries</li> <li>Expected Answer Creation: Create expected answers</li> <li>Document Tagging: Tag documents for relevance</li> <li>Test Case Assembly: Create complete test cases</li> </ol> <p>Example test case:</p> <pre><code>{\n  \"query\": \"How do I configure the Ollama service in Docker?\",\n  \"query_type\": \"how-to\",\n  \"relevant_docs\": [\n    \"development/docker.md#ollama-service\",\n    \"chatbot/index.md#services-configuration\"\n  ],\n  \"expected_answer_elements\": [\n    \"Ollama service configuration in docker-compose.yaml\",\n    \"Environment variables for GPU acceleration\",\n    \"Volume mounts for model storage\"\n  ],\n  \"difficulty\": \"medium\"\n}\n</code></pre>"},{"location":"chatbot/rag/evaluation/#human-evaluation","title":"Human Evaluation","text":"<p>In addition to automated metrics, human evaluation will be critical:</p> <ol> <li>Side-by-side comparisons: Compare RAG vs. non-RAG responses</li> <li>Blind evaluation: Rate responses without knowing the source</li> <li>Expert review: Domain experts evaluate factual correctness</li> <li>User feedback collection: Gather feedback from real users</li> </ol>"},{"location":"chatbot/rag/evaluation/#evaluation-dashboard","title":"Evaluation Dashboard","text":"<p>The RAG pipeline will include a visual dashboard for evaluation:</p> <pre><code>graph TD\n    A[Evaluation Runner] --&gt;|Executes Tests| B[Test Suite]\n    B --&gt;|Generates Metrics| C[Metrics Store]\n    C --&gt;|Visualizes Results| D[Dashboard]\n    D --&gt;|Precision| E[Retrieval Metrics]\n    D --&gt;|Correctness| F[Generation Metrics]\n    D --&gt;|Performance| G[System Metrics]\n    D --&gt;|Overall| H[Combined Score]</code></pre>"},{"location":"chatbot/rag/evaluation/#continuous-improvement","title":"Continuous Improvement","text":"<p>The evaluation system will enable continuous improvement:</p>"},{"location":"chatbot/rag/evaluation/#error-analysis","title":"Error Analysis","text":"<p>Categorizing and tracking error types:</p> <ul> <li>Retrieval failures: Relevant content not retrieved</li> <li>Context utilization errors: Context ignored or misinterpreted</li> <li>Hallucination instances: Information not grounded in context</li> <li>Citation errors: Missing or incorrect citations</li> </ul>"},{"location":"chatbot/rag/evaluation/#optimization-process","title":"Optimization Process","text":"<p>A systematic approach to RAG optimization:</p> <ol> <li>Baseline establishment: Measure initial performance</li> <li>Component isolation: Test each component independently</li> <li>Ablation studies: Remove components to measure impact</li> <li>Parameter tuning: Optimize configuration parameters</li> <li>A/B testing: Compare variations with real users</li> </ol>"},{"location":"chatbot/rag/evaluation/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>The evaluation system will be implemented in phases:</p> Phase Feature Description 1 Basic Metrics Implement core precision/recall metrics 2 Automated Test Suite Create synthetic test cases 3 Human Evaluation Tools Build tools for human feedback 4 Dashboard Create visualization dashboard 5 Continuous Monitoring Implement ongoing evaluation"},{"location":"chatbot/rag/evaluation/#best-practices","title":"Best Practices","text":"<p>Recommendations for effective RAG evaluation:</p> <ol> <li>Diverse test cases: Include varied query types and difficulty levels</li> <li>Regular re-evaluation: Test after each significant change</li> <li>User-focused metrics: Prioritize metrics aligned with user satisfaction</li> <li>Documentation-specific evaluation: Create tests specific to documentation use cases</li> <li>Comparative analysis: Benchmark against similar systems</li> </ol>"},{"location":"chatbot/rag/getting-started/","title":"Getting Started with Obelisk RAG","text":"<p>This guide will help you get started with the Retrieval Augmented Generation (RAG) system in Obelisk. We'll walk through the process of setting up your environment, initializing the system, and making your first query.</p>"},{"location":"chatbot/rag/getting-started/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant information retrieved from a knowledge base before generating responses. This approach combines the strengths of:</p> <ol> <li>Retrieval systems - Finding relevant content from your knowledge base</li> <li>Generation capabilities - Using that content to produce accurate, contextual responses</li> </ol> <p>For Obelisk, this means the AI chatbot will be able to answer questions based specifically on your documentation content.</p>"},{"location":"chatbot/rag/getting-started/#rag-architecture","title":"RAG Architecture","text":"<p>The Obelisk RAG pipeline consists of several key components:</p> <pre><code>graph TD\n    A[Documentation Files] --&gt;|Extraction| B[Content Processor]\n    B --&gt;|Chunking| C[Text Chunks]\n    C --&gt;|Embedding Generation| D[Vector Database]\n    E[User Query] --&gt;|Query Processing| F[Query Embeddings]\n    D --&gt;|Similarity Search| G[Relevant Chunks]\n    F --&gt;|Search| G\n    G --&gt;|Context Assembly| H[Prompt Assembly]\n    H --&gt;|LLM Input| I[Ollama Model]\n    I --&gt;|Response| J[Enhanced Answer]</code></pre> <p>The RAG pipeline follows these steps: 1. Content Processing: Extract content from Markdown files in your Obsidian vault 2. Chunking: Split content into appropriate segments for embedding 3. Embedding Generation: Convert text chunks into vector embeddings 4. Vector Storage: Store embeddings in a vector database for efficient retrieval 5. Query Processing: Process and embed user queries 6. Retrieval: Find the most relevant document chunks 7. Context Assembly: Combine retrieved content into a prompt 8. Response Generation: Generate accurate responses based on retrieved content</p>"},{"location":"chatbot/rag/getting-started/#implementation-status","title":"Implementation Status","text":"<p>The RAG pipeline has been implemented with the following components:</p> Phase Feature Status 1 Document Processing Pipeline Completed \u2713 2 Vector Database Integration Completed \u2713 3 Query Processing &amp; Retrieval Completed \u2713 4 Integration with Ollama Completed \u2713 5 Web UI Extensions Planned"},{"location":"chatbot/rag/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you start, ensure you have:</p> <ol> <li>Obelisk installed: The RAG system is part of Obelisk</li> <li>Ollama running: The system requires Ollama for LLMs and embeddings</li> <li>Documentation in your vault: Some markdown files to index</li> </ol>"},{"location":"chatbot/rag/getting-started/#step-1-start-ollama","title":"Step 1: Start Ollama","text":"<p>The RAG system requires Ollama for generating embeddings and providing LLM capabilities. You can run Ollama using Docker:</p> <pre><code>docker-compose up ollama -d\n</code></pre> <p>Wait for Ollama to start up (this might take a minute).</p>"},{"location":"chatbot/rag/getting-started/#step-2-pull-required-models","title":"Step 2: Pull Required Models","text":"<p>The RAG system needs models for embedding generation and text generation. You can pull them using:</p> <pre><code># Pull the LLM model (llama3 is recommended)\ndocker exec -it ollama ollama pull llama3\n\n# Pull the embedding model\ndocker exec -it ollama ollama pull mxbai-embed-large\n</code></pre> <p>This step will download the required models. The embedding model is optimized for generating high-quality embeddings for document retrieval.</p>"},{"location":"chatbot/rag/getting-started/#step-3-configure-the-rag-system","title":"Step 3: Configure the RAG System","text":"<p>The default configuration should work for most users, but you can customize it if needed:</p> <pre><code># View current configuration\nobelisk-rag config --show\n\n# Set a different vault directory if needed\nobelisk-rag config --set \"vault_dir=/path/to/your/docs\"\n\n# Set different Ollama URL if needed\nobelisk-rag config --set \"ollama_url=http://ollama:11434\"\n</code></pre>"},{"location":"chatbot/rag/getting-started/#step-4-index-your-documentation","title":"Step 4: Index Your Documentation","text":"<p>Before you can query your documentation, you need to index it:</p> <pre><code>obelisk-rag index\n</code></pre> <p>This process will: 1. Read all markdown files in your vault 2. Extract the content and metadata 3. Split the content into chunks 4. Generate embeddings for each chunk 5. Store everything in a vector database</p> <p>You should see a progress report in the console as files are processed.</p>"},{"location":"chatbot/rag/getting-started/#step-5-make-your-first-query","title":"Step 5: Make Your First Query","text":"<p>Now you can query your documentation:</p> <pre><code>obelisk-rag query \"What is Obelisk?\"\n</code></pre> <p>The system will: 1. Convert your query to an embedding 2. Find the most relevant document chunks 3. Use those chunks as context for the LLM 4. Generate a response based on your documentation</p> <p>You should see a response that's specifically informed by your documentation.</p>"},{"location":"chatbot/rag/getting-started/#step-6-start-the-api-server-optional","title":"Step 6: Start the API Server (Optional)","text":"<p>If you want to integrate with other applications or want the real-time document watching feature, you can start the API server:</p> <pre><code>obelisk-rag serve --watch\n</code></pre> <p>This will: 1. Start a REST API server (default: http://0.0.0.0:8000) 2. Provide endpoints for querying and stats 3. Watch for changes to documentation files and update the index automatically</p>"},{"location":"chatbot/rag/getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chatbot/rag/getting-started/#common-issues","title":"Common Issues","text":"<ol> <li>Connection errors with Ollama:</li> </ol> <pre><code>Error: Failed to connect to Ollama service at http://localhost:11434\n</code></pre> <p>Ensure Ollama is running and accessible at the configured URL. You may need to adjust the URL with:</p> <pre><code>obelisk-rag config --set \"ollama_url=http://ollama:11434\"\n</code></pre> <ol> <li>No results when querying:</li> </ol> <pre><code>No documents found for query: What is Obelisk?\n</code></pre> <p>Check that your documentation has been indexed successfully. Run <code>obelisk-rag stats</code> to see how many documents are in the database.</p> <ol> <li>Model not found errors:</li> </ol> <pre><code>Error: Model 'llama3' not found\n</code></pre> <p>Ensure you have pulled the required models using Ollama.</p>"},{"location":"chatbot/rag/getting-started/#enabling-debug-mode","title":"Enabling Debug Mode","text":"<p>If you're encountering issues, you can enable debug mode for more detailed logs:</p> <pre><code>export RAG_DEBUG=1\nobelisk-rag query \"What is Obelisk?\"\n</code></pre>"},{"location":"chatbot/rag/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have the RAG system up and running, you can:</p> <ol> <li>Learn about advanced configuration options</li> <li>Integrate with Open WebUI for a chat interface</li> <li>View the complete system architecture diagram</li> <li>Explore the RAG architecture details in depth</li> <li>Read about the implementation details if you want to customize the system</li> <li>Review evaluation techniques to measure and improve performance</li> </ol> <p>For more detailed usage information, see Using the RAG System.</p>"},{"location":"chatbot/rag/implementation-status/","title":"RAG Implementation Status","text":"<p>The Retrieval Augmented Generation (RAG) system has reached Minimum Viable Product (MVP) status! This document summarizes the implementation journey, current status, and future plans.</p>"},{"location":"chatbot/rag/implementation-status/#implementation-roadmap-overview","title":"Implementation Roadmap Overview","text":"<p>Based on research and requirements, the implementation focused on building a local-first RAG system that integrates with Obsidian vault and Ollama setup. This approach prioritized key functionality while setting a foundation for future enhancements.</p>"},{"location":"chatbot/rag/implementation-status/#prerequisites","title":"Prerequisites","text":"<ul> <li> Pull required embedding and LLM models:   <pre><code>ollama pull llama3\nollama pull mxbai-embed-large\n</code></pre> <p>Completed on 2025-04-11. Models are available via Ollama Docker container. The embedding model is 669MB, and the LLM is 4.7GB.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-1-project-setup-dependencies","title":"Phase 1: Project Setup &amp; Dependencies","text":"<ul> <li> Create module structure in <code>obelisk/rag/</code> <p>Created and implemented complete file structure on 2025-04-11 with all required modules.</p> </li> <li> Update <code>pyproject.toml</code> with RAG dependencies <p>Added dependencies on 2025-04-11 and updated with Poetry. Successfully installed langchain, langchain-community, langchain-ollama, chromadb, watchdog, fastapi, uvicorn, and pydantic.</p> </li> <li> Create basic configuration system for RAG settings <p>Implemented robust configuration system with environment variable support, defaults, and validation. Configuration can be modified via CLI and serialized to JSON.</p> </li> <li> Add initial unit tests structure <p>Implemented comprehensive test suite covering all RAG components with both unit and integration tests.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-2-document-processing-pipeline","title":"Phase 2: Document Processing Pipeline","text":"<ul> <li> Implement document loader for Markdown files <p>Created robust DocumentProcessor class that handles Markdown files with proper error handling and logging.</p> </li> <li> Create text splitter with appropriate chunk sizing <p>Implemented RecursiveCharacterTextSplitter with configurable chunk size and overlap parameters.</p> </li> <li> Develop file change monitoring system <p>Added real-time file watching using Watchdog with event handlers for file creation and modification.</p> </li> <li> Set up metadata extraction from documents <p>Implemented YAML frontmatter extraction with proper error handling and metadata filtering.</p> </li> <li> Test document processing with sample files <p>Validated document processing with real Obelisk documentation files, ensuring proper chunking and metadata extraction.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-3-embedding-vector-storage","title":"Phase 3: Embedding &amp; Vector Storage","text":"<ul> <li> Implement Ollama embedding integration <p>Successfully integrated Ollama embedding service using the mxbai-embed-large model with optimized error handling.</p> </li> <li> Configure ChromaDB for vector storage <p>Configured ChromaDB with proper persistence, filtering, and retrieval mechanisms.</p> </li> <li> Create persistence mechanism for embeddings <p>Implemented persistence to disk with configurable directory location and automatic backup.</p> </li> <li> Develop document indexing pipeline <p>Created efficient indexing pipeline with progress reporting and multi-threaded processing.</p> </li> <li> Build retrieval system for querying vectors <p>Implemented similarity search with configurable k parameter and metadata filtering capabilities.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-4-rag-pipeline-llm-integration","title":"Phase 4: RAG Pipeline &amp; LLM Integration","text":"<ul> <li> Create prompt templates for RAG <p>Developed optimized prompt templates for context insertion with proper formatting and instructions.</p> </li> <li> Implement Ollama LLM integration <p>Integrated Ollama LLM service with proper connection handling, retry mechanisms, and configurable parameters.</p> </li> <li> Develop RAG chain with context injection <p>Created RAG service that properly retrieves context and injects it into prompts for enhanced responses.</p> </li> <li> Add configuration options for the pipeline <p>Implemented comprehensive configuration options for all aspects of the RAG pipeline, including model parameters.</p> </li> <li> Test end-to-end query with retrieved context <p>Successfully tested end-to-end query processing with real documentation, validating context retrieval and response quality.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-5-user-interfaces","title":"Phase 5: User Interfaces","text":"<ul> <li> Build command-line interface <p>Implemented comprehensive CLI with commands for indexing, querying, configuration, and statistics.</p> </li> <li> Develop simple API with FastAPI <p>Created FastAPI application with proper endpoint definitions, validation, and error handling.</p> </li> <li> Create basic documentation for usage <p>Wrote detailed usage documentation for both CLI and API interfaces with examples.</p> </li> <li> Implement endpoints for querying and reindexing <p>Added endpoints for querying, reindexing, file watching, and system statistics.</p> </li> <li> Test interfaces with real documents <p>Validated both interfaces with real-world usage scenarios and sample queries.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#phase-6-docker-integration","title":"Phase 6: Docker &amp; Integration","text":"<ul> <li> Create Dockerfile for RAG service <p>Developed optimized Dockerfile with proper layer caching and minimal dependencies.</p> </li> <li> Update docker-compose.yml to include RAG service <p>Updated docker-compose configuration to include the RAG service with proper dependencies.</p> </li> <li> Configure volumes and environment variables <p>Set up appropriate volume mounts for data persistence and environment variables for configuration.</p> </li> <li> Test integration with existing Obelisk services <p>Verified integration with Ollama and OpenWebUI, ensuring proper communication between services.</p> </li> <li> Verify end-to-end functionality in containers <p>Successfully tested complete end-to-end functionality in containerized environment.</p> </li> </ul>"},{"location":"chatbot/rag/implementation-status/#current-implementation-status","title":"Current Implementation Status","text":"<p>The RAG system is now fully operational with all core MVP features implemented:</p> <p>\u2705 Document Processing: - Markdown document loading from vault - YAML frontmatter extraction - Text chunking with configurable parameters - File system watching for real-time updates</p> <p>\u2705 Embedding Generation: - Integration with Ollama for embeddings - Document and query embedding generation - Error handling and logging</p> <p>\u2705 Vector Storage: - ChromaDB integration for vector storage - Document storage and retrieval - Similarity search with configurable k parameter</p> <p>\u2705 RAG Service: - Integration of all components - Context augmentation for LLM prompts - Proper prompt engineering for effective responses - Fallback handling for no-context scenarios</p> <p>\u2705 Command-Line Interface: - Document indexing - Query processing - Configuration management - System statistics</p> <p>\u2705 API Server: - REST API for integration - Query endpoint - Statistics endpoint - Real-time document watching</p>"},{"location":"chatbot/rag/implementation-status/#whats-working-now","title":"What's Working Now","text":"<p>With the current implementation, you can:</p> <ol> <li> <p>Index your documentation:    <pre><code>obelisk-rag index\n</code></pre></p> </li> <li> <p>Query your documentation:    <pre><code>obelisk-rag query \"What is Obelisk?\"\n</code></pre></p> </li> <li> <p>Start the API server:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>Configure the system:    <pre><code>obelisk-rag config --set \"retrieve_top_k=5\"\n</code></pre></p> </li> <li> <p>View system statistics:    <pre><code>obelisk-rag stats\n</code></pre></p> </li> </ol>"},{"location":"chatbot/rag/implementation-status/#engineering-notes-and-technical-achievements","title":"Engineering Notes and Technical Achievements","text":"<p>Several key engineering challenges were addressed during the MVP implementation:</p> <p>\u2705 Configuration Management: - Created a unified configuration system using environment variables with OBELISK_ prefix - Implemented config file persistence as JSON - Added validation with proper error messages - Created CLI-based configuration management</p> <p>\u2705 Error Handling and Resilience: - Added comprehensive error handling throughout the codebase - Implemented connection retry mechanisms for Ollama services - Added proper logging with configurable levels - Created meaningful error messages for users</p> <p>\u2705 Metadata Processing: - Solved YAML frontmatter extraction and parsing issues - Fixed serialization problems with complex data types in metadata - Implemented proper date handling in document metadata - Created metadata filtering for vector storage</p> <p>\u2705 Performance Considerations: - Optimized document chunking for better retrieval results - Added efficient file watching with debouncing - Implemented multi-threaded processing where appropriate - \ud83d\udd04 Planned but not yet implemented: Batch processing for embedding generation</p>"},{"location":"chatbot/rag/implementation-status/#technical-decisions-for-mvp","title":"Technical Decisions for MVP","text":"<ol> <li>Embedding Model: mxbai-embed-large via Ollama</li> <li>Rationale: Already integrated with Ollama, good performance, simple setup</li> <li> <p>Implementation Note: Successfully integrated with 768-dimensional embeddings, handling ~50 docs/second on standard hardware.</p> </li> <li> <p>Vector Database: Chroma</p> </li> <li>Rationale: Lowest complexity, well-integrated with LangChain, sufficient for thousands of documents</li> <li> <p>Implementation Note: Working well with SQLite backend, efficient for up to 100,000 chunks. Filtering by metadata working as expected.</p> </li> <li> <p>LLM: Llama3 (8B variant) via Ollama</p> </li> <li>Rationale: Good balance of quality and performance on average hardware</li> <li> <p>Implementation Note: Response quality excellent with context, response time averaging 2-5 seconds depending on query complexity.</p> </li> <li> <p>Framework: LangChain core components</p> </li> <li>Rationale: Reduces custom code, well-tested integration patterns</li> <li> <p>Implementation Note: Updated to latest LangChain patterns, avoiding deprecated components. Custom components created where needed.</p> </li> <li> <p>UI Approach: CLI first, simple API for integration</p> </li> <li>Rationale: Fastest path to functional system, defer UI complexity</li> <li>Implementation Note: Both CLI and API implemented with full feature parity. API endpoints documented with OpenAPI.</li> </ol>"},{"location":"chatbot/rag/implementation-status/#next-steps","title":"Next Steps","text":"<p>The next development priorities are:</p> <ol> <li>Web UI Integration: Create tight integration with Open WebUI</li> <li>Develop custom plugin for OpenWebUI integration</li> <li>Add document source display in responses</li> <li> <p>Create admin interface for monitoring and management</p> </li> <li> <p>Enhanced Evaluation: Implement evaluation tools for measuring RAG quality</p> </li> <li>Develop benchmark datasets for testing retrieval quality</li> <li>Add automated testing framework for RAG metrics</li> <li> <p>Create evaluation dashboard for monitoring performance</p> </li> <li> <p>Advanced Retrieval: Add re-ranking and hybrid retrieval capabilities</p> </li> <li>Implement hybrid search with keywords and vectors</li> <li>Add re-ranking with cross-encoders for improved relevance</li> <li> <p>Create filtering mechanisms based on document metadata</p> </li> <li> <p>User Feedback Loop: Add mechanisms to incorporate user feedback</p> </li> <li>Implement thumbs up/down feedback collection</li> <li>Create feedback database for training improvements</li> <li>Develop tools for analyzing feedback patterns</li> </ol>"},{"location":"chatbot/rag/implementation-status/#areas-for-future-enhancement","title":"Areas for Future Enhancement","text":"<p>While the MVP is functional and production-ready, several areas could be enhanced in future iterations:</p> <p>\ud83d\udd04 Advanced Chunking: - Semantic chunking based on content meaning - Heading-based chunking - Improved handling of code blocks and tables</p> <p>\ud83d\udd04 Enhanced Retrieval: - Hybrid retrieval (keywords + vectors) - Re-ranking of retrieved documents - Additional filtering options based on metadata</p> <p>\ud83d\udd04 Advanced LLM Integration: - Support for more models - Improved streaming responses - Model parameter customization through UI</p> <p>\ud83d\udd04 Web UI Integration: - Dedicated Web UI components - Visualization of retrieved contexts - Search highlighting</p> <p>\ud83d\udd04 Performance Optimization: - Caching for frequent queries - Additional batch processing optimizations - Benchmarking and optimization</p>"},{"location":"chatbot/rag/implementation-status/#conclusion","title":"Conclusion","text":"<p>The RAG MVP has been successfully implemented and is now production-ready! All core components are functioning as expected:</p> <ul> <li>\u2705 Document processing pipeline with YAML frontmatter handling</li> <li>\u2705 Embedding generation using Ollama and mxbai-embed-large</li> <li>\u2705 Vector storage with ChromaDB</li> <li>\u2705 RAG integration with context augmentation</li> <li>\u2705 CLI and API interfaces for user interaction</li> <li>\u2705 Docker containerization and integration</li> </ul> <p>The implementation provides a solid foundation for document retrieval and generation in Obelisk. It enables users to interact with their documentation through natural language queries and receive contextually relevant responses using their own local infrastructure.</p> <p>We've overcome several technical challenges related to metadata handling, error resilience, and system integration. The result is a robust system that can be easily deployed and used in production environments.</p> <p>As we move forward, we'll continue to enhance and expand the RAG capabilities based on user feedback and emerging best practices in the field of retrieval augmented generation.</p>"},{"location":"chatbot/rag/implementation/","title":"Implementation Guide","text":"<p>This guide provides details on the RAG implementation in Obelisk, covering both the current implementation and future development considerations.</p>"},{"location":"chatbot/rag/implementation/#architecture-integration","title":"Architecture Integration","text":"<p>The RAG pipeline will be integrated into Obelisk's architecture:</p> <pre><code>graph TD\n    A[Obsidian Vault] --&gt;|Convert| B[MkDocs Site]\n    A --&gt;|Process| C[Document Processor]\n    C --&gt;|Embed| D[Vector Database]\n    E[User Query] --&gt;|via WebUI| F[Query Processor]\n    F --&gt;|Search| D\n    D --&gt;|Retrieved Chunks| G[Context Builder]\n    G --&gt;|Enhanced Prompt| H[Ollama API]\n    H --&gt;|Response| I[Web Interface]</code></pre>"},{"location":"chatbot/rag/implementation/#core-components","title":"Core Components","text":"<p>Note: The following sections include both currently implemented features and future planned enhancements. The code examples marked \"Current implementation\" reflect the actual implemented code, while examples marked \"Future implementation\" represent planned features.</p>"},{"location":"chatbot/rag/implementation/#1-document-processor","title":"1. Document Processor","text":"<p>Responsible for parsing Markdown files, chunking content, and handling metadata:</p> <pre><code># Current implementation\nclass DocumentProcessor:\n    def __init__(self, config):\n        \"\"\"Initialize the document processor.\"\"\"\n        self.config = config\n        # RecursiveCharacterTextSplitter is used for intelligent document chunking\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            # These values come from configuration\n            chunk_size=config.get(\"chunk_size\"),\n            chunk_overlap=config.get(\"chunk_overlap\"),\n            # Separators define how text is split, prioritizing Markdown headers\n            # to ensure chunks maintain semantic relevance\n            separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n        )\n\n        # References to other services (set via register_services)\n        self.embedding_service = None\n        self.storage_service = None\n\n    def process_file(self, file_path):\n        \"\"\"Process a single markdown file.\"\"\"\n        # Read file and create document with source metadata\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\"source\": file_path}\n        )\n\n        # Extract YAML frontmatter as metadata\n        self._extract_metadata(doc)\n\n        # Split document into chunks\n        chunks = self.text_splitter.split_documents([doc])\n\n        # Process with embedding and storage services if available\n        if self.embedding_service and self.storage_service:\n            embedded_docs = self.embedding_service.embed_documents(chunks)\n            self.storage_service.add_documents(embedded_docs)\n\n        return chunks\n\n    def process_directory(self, directory=None):\n        \"\"\"Process all markdown files in a directory recursively.\"\"\"\n        directory = directory or self.config.get(\"vault_dir\")\n        all_chunks = []\n\n        # Use glob to find all markdown files\n        for md_file in glob.glob(f\"{directory}/**/*.md\", recursive=True):\n            chunks = self.process_file(md_file)\n            all_chunks.extend(chunks)\n\n        return all_chunks\n</code></pre>"},{"location":"chatbot/rag/implementation/#document-chunking-details","title":"Document Chunking Details","text":"<p>The document chunking process uses LangChain's <code>RecursiveCharacterTextSplitter</code>, which:</p> <ol> <li>Starts with the most granular separator (<code>\\n##</code> - Markdown h2 headers)</li> <li>If chunks are still too large, proceeds to the next separator (h3, h4, etc.)</li> <li>Ultimately splits on individual characters if necessary</li> <li>Maintains overlap between chunks to preserve context across chunk boundaries</li> </ol> <p>This approach ensures chunks align with semantic boundaries when possible, improving retrieval quality by keeping related content together.</p>"},{"location":"chatbot/rag/implementation/#configuration-options","title":"Configuration Options","text":"<p>Document chunking can be configured with:</p> Parameter Description Default <code>chunk_size</code> Target size of each chunk in characters 1000 <code>chunk_overlap</code> Number of characters to overlap between chunks 200 <p>These parameters balance: - Larger chunks: More context but less precise retrieval - Smaller chunks: More precise retrieval but less context - Chunk overlap: Ensures information spanning chunk boundaries isn't lost</p> <p>The document processor also includes real-time file watching capabilities using the <code>watchdog</code> library to detect changes to markdown files and automatically update the vector database.</p>"},{"location":"chatbot/rag/implementation/#2-vector-database-manager","title":"2. Vector Database Manager","text":"<p>Interface for vector database operations:</p> <pre><code># Future implementation example\nclass VectorDBManager:\n    def __init__(self, config):\n        self.config = config\n        self.db = self._initialize_db()\n\n    def _initialize_db(self):\n        \"\"\"Initialize the vector database based on configuration.\"\"\"\n        db_type = self.config.get(\"vector_db\", \"chroma\")\n        if db_type == \"chroma\":\n            return self._init_chroma()\n        elif db_type == \"faiss\":\n            return self._init_faiss()\n        # Other implementations\n\n    def add_documents(self, chunks, embeddings, metadata):\n        \"\"\"Add document chunks to the database.\"\"\"\n        # Implementation details\n\n    def search(self, query_embedding, filters=None, k=5):\n        \"\"\"Search for similar documents.\"\"\"\n        # Implementation details\n\n    def update_document(self, doc_id, new_embedding=None, new_metadata=None):\n        \"\"\"Update an existing document.\"\"\"\n        # Implementation details\n\n    def delete_document(self, doc_id):\n        \"\"\"Remove a document from the database.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#3-query-processor","title":"3. Query Processor","text":"<p>Handles user queries and retrieval:</p> <pre><code># Future implementation example\nclass QueryProcessor:\n    def __init__(self, vector_db, embedding_model, config):\n        self.vector_db = vector_db\n        self.embedding_model = embedding_model\n        self.config = config\n\n    async def process_query(self, query_text):\n        \"\"\"Process a user query and retrieve relevant context.\"\"\"\n        # Preprocess query\n        processed_query = self._preprocess_query(query_text)\n\n        # Generate embedding\n        query_embedding = self.embedding_model.embed(processed_query)\n\n        # Retrieve relevant chunks\n        results = self.vector_db.search(\n            query_embedding,\n            filters=processed_query.get(\"filters\"),\n            k=self.config.get(\"retrieve_top_k\", 5)\n        )\n\n        # Assemble context\n        context = self._assemble_context(results)\n\n        return {\n            \"original_query\": query_text,\n            \"processed_query\": processed_query,\n            \"retrieved_chunks\": results,\n            \"assembled_context\": context\n        }\n</code></pre>"},{"location":"chatbot/rag/implementation/#4-prompt-manager","title":"4. Prompt Manager","text":"<p>Handles prompt assembly and model interaction:</p> <pre><code># Future implementation example\nclass PromptManager:\n    def __init__(self, config):\n        self.config = config\n        self.templates = self._load_templates()\n\n    def _load_templates(self):\n        \"\"\"Load prompt templates from configuration.\"\"\"\n        # Implementation details\n\n    def create_prompt(self, query, context):\n        \"\"\"Create a prompt with retrieved context.\"\"\"\n        template = self.templates.get(\"default_rag\")\n        return template.format(\n            retrieved_context=self._format_context(context),\n            user_question=query[\"original_query\"]\n        )\n\n    def _format_context(self, context_items):\n        \"\"\"Format retrieved context items for the prompt.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#integration-with-ollama","title":"Integration with Ollama","text":"<p>The RAG pipeline integrates with Ollama for both embedding generation and LLM response generation:</p> <pre><code># Current implementation (simplified)\nfrom langchain_ollama import ChatOllama\nfrom langchain_ollama import OllamaEmbeddings\n\nclass EmbeddingService:\n    def __init__(self, config):\n        \"\"\"Initialize the embedding service.\"\"\"\n        self.config = config\n        self.embedding_model = OllamaEmbeddings(\n            model=config.get(\"embedding_model\"),\n            base_url=config.get(\"ollama_url\")\n        )\n\n    def embed_documents(self, documents):\n        \"\"\"Generate embeddings for a list of documents.\"\"\"\n        try:\n            # Current implementation processes each document individually\n            # Future enhancement: Add batch processing for better performance\n            # with documents processed in configurable batch sizes\n            for doc in documents:\n                doc.embedding = self.embedding_model.embed_query(doc.page_content)\n            return documents\n        except Exception as e:\n            logger.error(f\"Error embedding documents: {e}\")\n            return []\n\nclass RAGService:\n    def __init__(self, config):\n        \"\"\"Initialize with all necessary components.\"\"\"\n        self.config = config\n        self.llm = ChatOllama(\n            model=config.get(\"ollama_model\"),\n            base_url=config.get(\"ollama_url\"),\n            temperature=0.7,\n        )\n\n    def query(self, query_text):\n        \"\"\"Process a query using RAG.\"\"\"\n        try:\n            # Get relevant documents\n            relevant_docs = self.storage_service.query(query_text)\n\n            # Format prompt with context\n            context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n            prompt = f\"\"\"Use the following information to answer the question.\n\nInformation:\n{context}\n\nQuestion: {query_text}\n\nAnswer:\"\"\"\n\n            # Get response from Ollama\n            response = self.llm.invoke(prompt)\n\n            return {\n                \"query\": query_text,\n                \"response\": response.content,\n                \"context\": relevant_docs,\n                \"no_context\": len(relevant_docs) == 0\n            }\n        except Exception as e:\n            logger.error(f\"Error processing query: {e}\")\n            # Fallback to direct LLM query\n            response = self.llm.invoke(f\"Question: {query_text}\\n\\nAnswer:\")\n            return {\n                \"query\": query_text,\n                \"response\": response.content,\n                \"context\": [],\n                \"no_context\": True\n            }\n</code></pre>"},{"location":"chatbot/rag/implementation/#error-handling-architecture","title":"Error Handling Architecture","text":"<p>The RAG system implements a comprehensive error handling strategy to ensure reliability:</p>"},{"location":"chatbot/rag/implementation/#1-layered-error-handling","title":"1. Layered Error Handling","text":"<p>Each component implements its own error handling appropriate to its context:</p> <ul> <li>Document Processor: Handles I/O errors, parsing errors, and invalid documents</li> <li>Embedding Service: Manages embedding generation failures</li> <li>Vector Storage: Handles database errors and metadata type compatibility</li> <li>API Layer: Converts exceptions to proper HTTP responses</li> </ul>"},{"location":"chatbot/rag/implementation/#2-error-recovery-strategies","title":"2. Error Recovery Strategies","text":"<p>The system uses various strategies to recover from errors:</p> <ul> <li>Graceful Degradation: If document retrieval fails, the system falls back to direct LLM queries</li> <li>Default Values: Configuration system provides sensible defaults for all settings</li> <li>Filtering: Invalid documents or metadata are filtered rather than causing failures</li> <li>Persistence: Database operations include safeguards against corruption</li> </ul>"},{"location":"chatbot/rag/implementation/#3-logging-system","title":"3. Logging System","text":"<p>A centralized logging system provides visibility into errors:</p> <pre><code># Logging configuration (from cli.py)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler()\n    ]\n)\n# Specific logging for external libraries\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nlogging.getLogger(\"chromadb\").setLevel(logging.WARNING)\n</code></pre>"},{"location":"chatbot/rag/implementation/#4-debug-mode","title":"4. Debug Mode","text":"<p>A debug mode can be enabled for detailed error information:</p> <pre><code># Debug mode handling (from cli.py)\ntry:\n    # Operation code\nexcept Exception as e:\n    logger.error(f\"Error: {e}\")\n    if os.environ.get(\"RAG_DEBUG\"):\n        # In debug mode, show the full traceback\n        import traceback\n        traceback.print_exc()\n    else:\n        # In normal mode, show a user-friendly message\n        print(f\"Error: {e}\")\n        print(\"For detailed error output, set the RAG_DEBUG environment variable\")\n</code></pre>"},{"location":"chatbot/rag/implementation/#web-ui-integration","title":"Web UI Integration","text":"<p>Connection to the Open WebUI interface:</p> <pre><code># Future implementation example\nclass WebUIIntegration:\n    def __init__(self, config):\n        self.config = config\n\n    def register_endpoints(self, app):\n        \"\"\"Register RAG endpoints with the web application.\"\"\"\n        app.add_route(\"/api/rag/query\", self.handle_query)\n\n    async def handle_query(self, request):\n        \"\"\"Handle RAG query requests.\"\"\"\n        # Implementation details\n</code></pre>"},{"location":"chatbot/rag/implementation/#configuration-system","title":"Configuration System","text":"<p>RAG features will be configurable through MkDocs configuration:</p> <pre><code># Example future configuration\nplugins:\n  - obelisk-rag:\n      # Document processing\n      chunk_size: 512\n      chunk_overlap: 50\n      chunk_strategy: \"fixed\"  # fixed, semantic, recursive\n\n      # Embedding\n      embedding_model: \"nomic-embed-text\"\n      embedding_dimension: 768\n\n      # Vector database\n      vector_db: \"chroma\"\n      vector_db_path: \"./.obelisk/vectordb\"\n\n      # Query processing\n      retrieve_top_k: 5\n      reranking_enabled: true\n      hybrid_search: true\n\n      # Integration\n      ollama_url: \"http://ollama:11434\"\n      ollama_model: \"mistral\"\n\n      # Templates\n      prompt_template: \"default_rag\"\n      custom_templates:\n        my_template: \"path/to/template.txt\"\n</code></pre>"},{"location":"chatbot/rag/implementation/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"chatbot/rag/implementation/#resource-requirements","title":"Resource Requirements","text":"Deployment Size Documents Vector DB Size RAM Storage Small (&lt;100 docs) &lt;1,000 chunks ~100MB 2GB 1GB Medium (~500 docs) ~5,000 chunks ~500MB 4GB 5GB Large (1000+ docs) 10,000+ chunks 1GB+ 8GB+ 10GB+"},{"location":"chatbot/rag/implementation/#docker-configuration","title":"Docker Configuration","text":"<p>Additional container configuration for RAG:</p> <pre><code># Future docker-compose additions\nservices:\n  obelisk:\n    # Existing configuration...\n    environment:\n      - OBELISK_RAG_ENABLED=true\n      - OBELISK_VECTOR_DB_PATH=/data/vectordb\n    volumes:\n      - vectordb_data:/data/vectordb\n\nvolumes:\n  vectordb_data:\n</code></pre>"},{"location":"chatbot/rag/implementation/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<p>The RAG system will include:</p> <ol> <li>Embedding updates: Trigger on content changes</li> <li>Index optimization: Scheduled maintenance tasks</li> <li>Performance metrics: Track latency and quality</li> <li>Usage statistics: Monitor query patterns</li> <li>Content gap analysis: Identify missing documentation</li> </ol>"},{"location":"chatbot/rag/ollama-integration/","title":"Ollama Integration","text":"<p>This page details how the RAG pipeline will integrate with Ollama to provide context-aware responses.</p>"},{"location":"chatbot/rag/ollama-integration/#integration-architecture","title":"Integration Architecture","text":"<p>The RAG pipeline will interact with Ollama through its API:</p> <pre><code>sequenceDiagram\n    participant User\n    participant OpenWebUI as Open WebUI\n    participant RAGMiddleware as RAG Middleware\n    participant VectorDB as Vector Database\n    participant Ollama\n\n    User-&gt;&gt;OpenWebUI: Ask question\n    OpenWebUI-&gt;&gt;RAGMiddleware: Forward query\n    RAGMiddleware-&gt;&gt;VectorDB: Retrieve context\n    VectorDB--&gt;&gt;RAGMiddleware: Return relevant docs\n    RAGMiddleware-&gt;&gt;Ollama: Send query + context\n    Ollama--&gt;&gt;OpenWebUI: Return enhanced response\n    OpenWebUI--&gt;&gt;User: Display response</code></pre>"},{"location":"chatbot/rag/ollama-integration/#ollama-api-interaction","title":"Ollama API Interaction","text":""},{"location":"chatbot/rag/ollama-integration/#model-generation-endpoint","title":"Model Generation Endpoint","text":"<p>The RAG pipeline will use Ollama's generation API:</p> <pre><code>POST /api/generate HTTP/1.1\nHost: localhost:11434\nContent-Type: application/json\n\n{\n  \"model\": \"mistral\",\n  \"prompt\": \"System: You are a helpful assistant. Use the following context to answer the question.\\n\\nContext: {retrieved_context}\\n\\nQuestion: {user_question}\",\n  \"stream\": false,\n  \"options\": {\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"num_ctx\": 4096\n  }\n}\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#context-formatting","title":"Context Formatting","text":"<p>The context will be formatted before being sent to Ollama:</p> <pre><code># Future implementation example\ndef format_context_for_ollama(retrieved_chunks):\n    \"\"\"Format retrieved chunks into a context string for Ollama.\"\"\"\n    formatted_chunks = []\n\n    for i, chunk in enumerate(retrieved_chunks):\n        formatted_chunks.append(\n            f\"[Document {i+1}] {chunk['metadata']['source']}\\n\"\n            f\"---\\n\"\n            f\"{chunk['text']}\\n\"\n        )\n\n    return \"\\n\\n\".join(formatted_chunks)\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#custom-modelfiles","title":"Custom Modelfiles","text":"<p>Obelisk will provide custom Modelfiles optimized for RAG:</p> <pre><code>FROM mistral:latest\n\n# Optimize for RAG\nPARAMETER num_ctx 8192\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER top_k 40\n\n# System instruction for RAG\nSYSTEM You are a helpful documentation assistant for the Obelisk documentation system.\nSYSTEM When given context from the documentation, use this information to answer the user's question.\nSYSTEM Always attribute your sources and only provide information contained in the given context.\nSYSTEM If the information can't be found in the context, acknowledge this and suggest where the user might find more information.\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#embedding-models","title":"Embedding Models","text":"<p>The RAG pipeline will use embedding models via Ollama:</p> Model Description Size Performance nomic-embed-text General text embeddings 137M High quality mxbai-embed-large Multilingual embeddings 137M Multilingual support all-mxbai-embed-large Specialized code embeddings 137M Code-focused <p>Example embedding request:</p> <pre><code>POST /api/embeddings HTTP/1.1\nHost: localhost:11434\nContent-Type: application/json\n\n{\n  \"model\": \"nomic-embed-text\",\n  \"prompt\": \"The text to be embedded\"\n}\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#memory-management","title":"Memory Management","text":"<p>When dealing with limited resources:</p> <ol> <li>Context pruning: Dynamically adjust context size based on available memory</li> <li>Quantization selection: Use higher quantization levels (Q4_0) for large models</li> <li>Context batching: Process chunks in batches for large documents</li> <li>Model swapping: Automatically switch between models based on query complexity</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#response-processing","title":"Response Processing","text":"<p>After receiving responses from Ollama:</p> <ol> <li>Citation extraction: Identify and format source citations</li> <li>Response validation: Verify that the response uses the provided context</li> <li>Confidence scoring: Assess confidence in the generated response</li> <li>Metadata enrichment: Add metadata about sources used</li> </ol> <p>Example response processing:</p> <pre><code># Future implementation example\ndef process_ollama_response(response, retrieved_chunks):\n    \"\"\"Process and enhance response from Ollama.\"\"\"\n    text = response[\"response\"]\n\n    # Extract and verify citations\n    citations = extract_citations(text)\n    valid_citations = verify_citations(citations, retrieved_chunks)\n\n    # Format citations and append source information\n    enhanced_response = format_response_with_citations(text, valid_citations)\n\n    # Add metadata\n    metadata = {\n        \"sources_used\": [chunk[\"metadata\"][\"source\"] for chunk in retrieved_chunks],\n        \"confidence_score\": calculate_confidence(text, retrieved_chunks),\n        \"model_used\": response[\"model\"],\n        \"response_time\": response[\"total_duration\"] / 1000000000  # Convert ns to s\n    }\n\n    return {\n        \"response\": enhanced_response,\n        \"metadata\": metadata\n    }\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#performance-optimization","title":"Performance Optimization","text":"<p>To optimize performance with Ollama:</p> <ol> <li>Batched embeddings: Process multiple chunks in a single API call</li> <li>Connection pooling: Maintain persistent connections</li> <li>Response streaming: Stream responses for faster initial display</li> <li>Caching layer: Cache common queries and embeddings</li> <li>Load balancing: Support multiple Ollama instances for high demand</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#fallback-mechanisms","title":"Fallback Mechanisms","text":"<p>The system will include fallbacks when Ollama cannot provide good answers:</p> <ol> <li>General knowledge fallback: Use model's general knowledge when no context is found</li> <li>Search suggestions: Recommend search terms for more relevant results</li> <li>Documentation navigation: Suggest navigation paths in the documentation</li> <li>Human handoff: Provide contact information for human assistance</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#security-considerations","title":"Security Considerations","text":"<p>Important security aspects of the Ollama integration:</p> <ol> <li>Input validation: Sanitize all inputs to prevent prompt injection</li> <li>Rate limiting: Prevent abuse with rate limits</li> <li>Output filtering: Filter sensitive information from responses</li> <li>Network isolation: Restrict network access to the Ollama API</li> <li>Authentication: Add optional authentication for API access</li> </ol>"},{"location":"chatbot/rag/ollama-integration/#advanced-models","title":"Advanced Models","text":"<p>Obelisk's RAG system supports various advanced models through Ollama integration, each offering different capabilities and performance characteristics.</p>"},{"location":"chatbot/rag/ollama-integration/#recommended-models","title":"Recommended Models","text":"Model Size Use Case Performance Llama3 8B General purpose, balanced Good general performance Phi-3-mini 3.8B Lightweight, efficient Excellent for resource-constrained systems Mistral 7B Technical content Strong reasoning capabilities Mixtral 8x7B Complex questions High quality with MoE architecture DeepSeek Coder 6.7B Code-focused Excellent for developer documentation"},{"location":"chatbot/rag/ollama-integration/#model-configuration","title":"Model Configuration","text":"<p>Advanced models can be configured in various ways:</p> <pre><code># Pull recommended models\nollama pull llama3\nollama pull phi:latest\nollama pull mistral\nollama pull mixtral\nollama pull deepseek-coder\n\n# Configure Obelisk to use a specific model\nexport OLLAMA_MODEL=\"mixtral\"\nobelisk-rag config --set \"ollama_model=mixtral\"\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#custom-model-parameters","title":"Custom Model Parameters","text":"<p>For advanced users, Obelisk allows customizing model parameters:</p> <pre><code># Example future configuration\nrag:\n  ollama:\n    model: \"llama3\"\n    parameters:\n      temperature: 0.1\n      top_p: 0.9\n      top_k: 40\n      num_ctx: 8192\n      repeat_penalty: 1.1\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#multi-model-strategy","title":"Multi-Model Strategy","text":"<p>The RAG system can use different models for different types of queries:</p> <ol> <li>Technical questions: Use models like Mistral or DeepSeek Coder</li> <li>General questions: Use Llama3 or Phi-3</li> <li>Complex reasoning: Use Mixtral or larger variants</li> </ol> <p>To implement a multi-model strategy, you can use the model selection feature:</p> <pre><code># Different models for different queries\nobelisk-rag query \"How do I use Docker with Obelisk?\" --model deepseek-coder\nobelisk-rag query \"Explain the architecture of Obelisk\" --model mixtral\n</code></pre>"},{"location":"chatbot/rag/ollama-integration/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For optimal performance with advanced models, GPU acceleration is recommended:</p> <ol> <li>NVIDIA GPUs: Fully supported through CUDA</li> <li>AMD GPUs: Experimental support through ROCm</li> <li>Metal (Apple Silicon): Native support on Mac</li> </ol> <p>Configure GPU usage with:</p> <pre><code># Enable GPU acceleration in docker-compose.yml\nservices:\n  ollama:\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n</code></pre>"},{"location":"chatbot/rag/openwebui-integration/","title":"Integrating RAG with Open WebUI","text":"<p>This document provides comprehensive instructions for integrating Obelisk's RAG system with Open WebUI, an advanced interface for interacting with large language models.</p>"},{"location":"chatbot/rag/openwebui-integration/#overview","title":"Overview","text":"<p>Open WebUI can leverage Obelisk's RAG capabilities to provide context-aware responses based on your documentation. The integration works through an OpenAI-compatible API provided by the Obelisk RAG service, allowing Open WebUI to seamlessly retrieve and use relevant documentation chunks when answering questions.</p>"},{"location":"chatbot/rag/openwebui-integration/#integration-architecture","title":"Integration Architecture","text":"<pre><code>graph LR\n    User([User]) --&gt; OpenWebUI\n    OpenWebUI --&gt; RAG_API[Obelisk RAG API]\n    OpenWebUI --&gt; Ollama\n    RAG_API --&gt; VectorDB[(Vector Database)]\n    RAG_API --&gt; Ollama[Ollama]\n    VectorDB --- Docs[(Documentation)]\n\n    style OpenWebUI fill:#f9f,stroke:#333,stroke-width:2px\n    style RAG_API fill:#bbf,stroke:#333,stroke-width:2px\n    style Ollama fill:#bfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"chatbot/rag/openwebui-integration/#configuration","title":"Configuration","text":""},{"location":"chatbot/rag/openwebui-integration/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>To integrate Open WebUI with the RAG service, configure the following environment variables in your <code>docker-compose.yaml</code> file:</p> <pre><code>services:\n  open-webui:\n    # ... other configuration ...\n    environment:\n      # ... other environment variables ...\n      # RAG configuration\n      - RAG_ENABLED=true\n      - RAG_SERVICE_TYPE=custom\n      - RAG_SERVICE_URL=http://obelisk-rag:8000/v1\n      - \"RAG_TEMPLATE=You are a helpful assistant. Use the following pieces of retrieved context to answer the user's question. If you don't know the answer, just say that you don't know.\\n\\nContext:\\n{{context}}\\n\\nUser question: {{query}}\"\n    depends_on:\n      - ollama\n      - obelisk-rag\n</code></pre>"},{"location":"chatbot/rag/openwebui-integration/#environment-variables","title":"Environment Variables","text":"Variable Description Example Value <code>RAG_ENABLED</code> Enable RAG functionality <code>true</code> <code>RAG_SERVICE_TYPE</code> Type of RAG service <code>custom</code> <code>RAG_SERVICE_URL</code> URL to the RAG API endpoint <code>http://obelisk-rag:8000/v1</code> <code>RAG_TEMPLATE</code> Prompt template for context injection See example below"},{"location":"chatbot/rag/openwebui-integration/#prompt-template","title":"Prompt Template","text":"<p>The <code>RAG_TEMPLATE</code> defines how retrieved context and user queries are presented to the language model. Here's the default template:</p> <pre><code>You are a helpful assistant. Use the following pieces of retrieved context to answer the user's question. If you don't know the answer, just say that you don't know.\n\nContext:\n{{context}}\n\nUser question: {{query}}\n</code></pre> <p>This template includes two placeholders: - <code>{{context}}</code> - Where retrieved document chunks will be inserted - <code>{{query}}</code> - Where the user's original question will be inserted</p>"},{"location":"chatbot/rag/openwebui-integration/#how-it-works","title":"How It Works","text":"<p>When a user asks a question through Open WebUI, the following process occurs:</p> <ol> <li>Open WebUI sends the query to the Obelisk RAG API using the OpenAI-compatible endpoint</li> <li>The RAG service:</li> <li>Embeds the query into a vector representation</li> <li>Searches the vector database for similar document chunks</li> <li>Retrieves the most relevant document chunks</li> <li>Formats these chunks as context using the RAG_TEMPLATE</li> <li>The formatted query with context is sent to the language model (via Ollama)</li> <li>The language model generates a response using the provided context</li> <li>The response is returned to Open WebUI and displayed to the user</li> </ol>"},{"location":"chatbot/rag/openwebui-integration/#customizing-the-rag-experience","title":"Customizing the RAG Experience","text":""},{"location":"chatbot/rag/openwebui-integration/#adjusting-retrieved-context","title":"Adjusting Retrieved Context","text":"<p>To modify the number of context chunks retrieved for each query, adjust the <code>RETRIEVE_TOP_K</code> environment variable in the obelisk-rag service:</p> <pre><code>services:\n  obelisk-rag:\n    # ... other configuration ...\n    environment:\n      # ... other environment variables ...\n      - RETRIEVE_TOP_K=5  # Retrieve 5 chunks instead of default 3\n</code></pre> <p>Higher values provide more context but may dilute relevance. Lower values are more focused but might miss important information.</p>"},{"location":"chatbot/rag/openwebui-integration/#customizing-the-prompt-template","title":"Customizing the Prompt Template","text":"<p>You can customize the RAG_TEMPLATE to change how the assistant uses retrieved context:</p> <pre><code>- 'RAG_TEMPLATE=You are an expert on Obelisk documentation. When answering, cite your sources using [Document Name] notation. Use these documentation excerpts to answer:\\n\\n{{context}}\\n\\nQuestion: {{query}}'\n</code></pre> <p>Effective customizations include: - Adjusting the assistant's persona - Requesting specific citation formats - Changing the response style (brief, detailed, technical, simplified) - Adding instructions for handling unanswerable questions</p>"},{"location":"chatbot/rag/openwebui-integration/#selecting-different-models","title":"Selecting Different Models","text":"<p>Configure the language model used for generating responses by modifying the <code>OLLAMA_MODEL</code> environment variable:</p> <pre><code>services:\n  obelisk-rag:\n    # ... other configuration ...\n    environment:\n      # ... other environment variables ...\n      - OLLAMA_MODEL=llama3  # Or other models like mistral, phi-3-mini, etc.\n</code></pre> <p>Similarly, you can change the embedding model with <code>EMBEDDING_MODEL</code>:</p> <pre><code>- EMBEDDING_MODEL=mxbai-embed-large  # Default embedding model\n</code></pre>"},{"location":"chatbot/rag/openwebui-integration/#testing-the-integration","title":"Testing the Integration","text":"<p>To verify the RAG integration is working correctly:</p> <ol> <li>Ensure all services are running with <code>docker-compose ps</code></li> <li>Open the Open WebUI interface at <code>http://localhost:8080</code></li> <li>Log in with your credentials</li> <li>Select a model with RAG capabilities</li> <li>Ask a question related to your documentation, for example:    <pre><code>How do I configure the RAG service?\n</code></pre></li> <li>The response should include information from your documentation</li> </ol>"},{"location":"chatbot/rag/openwebui-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chatbot/rag/openwebui-integration/#common-issues","title":"Common Issues","text":"Issue Possible Cause Solution No context retrieved Documents not indexed Run <code>docker exec obelisk-rag python -m obelisk.rag.cli index</code> Incorrect responses Irrelevant context Increase context quality by improving document chunking or adjust RETRIEVE_TOP_K RAG not being used Configuration issue Check RAG_ENABLED is true and RAG_SERVICE_URL is correct Slow responses Large document corpus Optimize embeddings or reduce context size"},{"location":"chatbot/rag/openwebui-integration/#checking-rag-status","title":"Checking RAG Status","text":"<p>To verify the RAG system status:</p> <pre><code>curl http://localhost:8001/stats\n</code></pre> <p>This will return information about indexed documents and the current configuration.</p>"},{"location":"chatbot/rag/openwebui-integration/#viewing-rag-logs","title":"Viewing RAG Logs","text":"<p>To view the RAG service logs for diagnostic information:</p> <pre><code>docker logs obelisk-rag\n</code></pre>"},{"location":"chatbot/rag/openwebui-integration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"chatbot/rag/openwebui-integration/#using-multiple-rag-models","title":"Using Multiple RAG Models","text":"<p>Open WebUI allows creating multiple models with different RAG configurations. To set up specialized models:</p> <ol> <li>In Open WebUI, go to \"Models\"</li> <li>Click \"Create New Model\"</li> <li>Select an Ollama model as the base</li> <li>Enable \"Use with RAG\"</li> <li>Configure RAG settings specific to this model</li> <li>Save the model</li> </ol> <p>This allows having specialized models for different documentation sections or query types.</p>"},{"location":"chatbot/rag/openwebui-integration/#hybrid-search","title":"Hybrid Search","text":"<p>While not implemented in the current version, future updates may support hybrid search combining vector similarity with keyword matching for improved retrieval quality.</p>"},{"location":"chatbot/rag/openwebui-integration/#example-full-integration","title":"Example: Full Integration","text":"<p>Below is a complete example of docker-compose configuration for integrating Open WebUI with the Obelisk RAG system:</p> <pre><code>services:\n  open-webui:\n    container_name: open-webui\n    image: ghcr.io/open-webui/open-webui:main\n    environment:\n      - MODEL_DOWNLOAD_DIR=/models\n      - OLLAMA_API_BASE_URL=http://ollama:11434\n      - OLLAMA_URL=http://ollama:11434\n      - LOG_LEVEL=debug\n      # RAG configuration\n      - RAG_ENABLED=true\n      - RAG_SERVICE_TYPE=custom\n      - RAG_SERVICE_URL=http://obelisk-rag:8000/v1\n      - \"RAG_TEMPLATE=You are a helpful assistant. Use the following pieces of retrieved context to answer the user's question. If you don't know the answer, just say that you don't know.\\n\\nContext:\\n{{context}}\\n\\nUser question: {{query}}\"\n    volumes:\n      - data:/data\n      - models:/models\n      - open-webui:/config\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - ollama\n      - obelisk-rag\n    networks:\n      - ollama-net\n\n  ollama:\n    container_name: ollama\n    image: ollama/ollama:latest\n    # ... ollama configuration ...\n\n  obelisk-rag:\n    container_name: obelisk-rag\n    build:\n      context: .\n      dockerfile: Dockerfile.rag\n    volumes:\n      - rag-data:/app/data\n      - rag-vault:/app/vault\n    ports:\n      - \"8001:8000\"\n    environment:\n      - VAULT_DIR=/app/vault\n      - CHROMA_DIR=/app/data/chroma_db\n      - OLLAMA_URL=http://ollama:11434\n      - OLLAMA_MODEL=llama3\n      - EMBEDDING_MODEL=mxbai-embed-large\n      - RETRIEVE_TOP_K=3\n      - API_HOST=0.0.0.0\n      - API_PORT=8000\n      - LOG_LEVEL=INFO\n    depends_on:\n      - ollama\n    networks:\n      - ollama-net\n</code></pre>"},{"location":"chatbot/rag/openwebui-integration/#further-reading","title":"Further Reading","text":"<ul> <li>Using RAG - Complete documentation on Obelisk's RAG capabilities</li> <li>Ollama Integration - Details on Ollama integration</li> <li>Open WebUI Documentation - Official Open WebUI documentation</li> </ul>"},{"location":"chatbot/rag/query-pipeline/","title":"Query Pipeline","text":"<p>The query pipeline is a critical component of the Obelisk RAG system, processing user questions and retrieving relevant content to enhance AI responses.</p>"},{"location":"chatbot/rag/query-pipeline/#query-flow-architecture","title":"Query Flow Architecture","text":"<p>The query pipeline follows these steps:</p> <pre><code>sequenceDiagram\n    participant User\n    participant QueryProcessor\n    participant VectorDB\n    participant PromptEngine\n    participant Ollama\n\n    User-&gt;&gt;QueryProcessor: Ask question\n    QueryProcessor-&gt;&gt;QueryProcessor: Preprocess query\n    QueryProcessor-&gt;&gt;VectorDB: Generate embedding\n    VectorDB-&gt;&gt;QueryProcessor: Return similar chunks\n    QueryProcessor-&gt;&gt;PromptEngine: Pass query + chunks\n    PromptEngine-&gt;&gt;Ollama: Send enhanced prompt\n    Ollama-&gt;&gt;User: Return augmented response</code></pre>"},{"location":"chatbot/rag/query-pipeline/#query-preprocessing","title":"Query Preprocessing","text":"<p>Before retrieval, queries undergo several preprocessing steps:</p> <ol> <li>Query expansion: Enhance queries with related terms</li> <li>Intent recognition: Identify the type of question</li> <li>Metadata extraction: Extract filters from the query</li> <li>Language detection: Handle multilingual queries</li> <li>Query rewriting: Optimize for retrieval performance</li> </ol> <p>Example implementation:</p> <pre><code># Future implementation example\ndef preprocess_query(query_text):\n    \"\"\"Preprocess user query for optimal retrieval.\"\"\"\n    # Clean and normalize text\n    cleaned_query = clean_text(query_text)\n\n    # Extract potential filters\n    filters = extract_metadata_filters(cleaned_query)\n\n    # Expand query with related terms\n    expanded_query = query_expansion(cleaned_query)\n\n    return {\n        \"original_query\": query_text,\n        \"processed_query\": expanded_query,\n        \"filters\": filters,\n        \"detected_intent\": detect_intent(cleaned_query)\n    }\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#retrieval-strategies","title":"Retrieval Strategies","text":"<p>The pipeline will implement multiple retrieval strategies:</p>"},{"location":"chatbot/rag/query-pipeline/#1-dense-retrieval","title":"1. Dense Retrieval","text":"<p>Using vector similarity to find relevant content:</p> <ul> <li>Embedding space: Convert query to the same embedding space as documents</li> <li>Similarity metrics: Cosine similarity, dot product, or Euclidean distance</li> <li>Top-k retrieval: Return the k most similar chunks</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#2-hybrid-search","title":"2. Hybrid Search","text":"<p>Combining multiple search techniques:</p> <ul> <li>BM25 keyword search: Traditional information retrieval</li> <li>Dense vector search: Semantic similarity</li> <li>Fusion methods: Reciprocal rank fusion or weighted combinations</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#3-multi-stage-retrieval","title":"3. Multi-stage Retrieval","text":"<p>A two-step process for better results:</p> <ul> <li>Initial retrieval: Get a larger set of potentially relevant chunks</li> <li>Re-ranking: Apply more complex models to re-rank results</li> <li>Diversity optimization: Ensure varied context</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#context-assembly","title":"Context Assembly","text":"<p>Retrieved chunks are assembled into a coherent context:</p> <ol> <li>Chunk sorting: Order by relevance and document structure</li> <li>Deduplication: Remove redundant information</li> <li>Context limitation: Fit within model context window</li> <li>Metadata inclusion: Add source information</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#prompt-engineering","title":"Prompt Engineering","text":"<p>Crafting effective prompts is essential for quality responses:</p>"},{"location":"chatbot/rag/query-pipeline/#basic-rag-prompt-template","title":"Basic RAG Prompt Template","text":"<pre><code>You are an assistant for the Obelisk documentation.\nAnswer the question based ONLY on the following context:\n\n{retrieved_context}\n\nQuestion: {user_question}\n\nAnswer:\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#advanced-rag-prompt-template","title":"Advanced RAG Prompt Template","text":"<pre><code>You are an assistant for the Obelisk documentation system.\nUse ONLY the following retrieved documentation to answer the user's question.\nIf the information is not in the retrieved docs, acknowledge that and suggest where they might find the information.\n\nRetrieved documentation:\n{retrieved_chunks}\n\nUser question: {user_question}\n\nRespond in a helpful, concise manner. Include code examples if relevant.\nAlways cite your sources using the document names provided in the retrieved chunks.\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#customizing-prompts","title":"Customizing Prompts","text":"<p>The Obelisk RAG system allows for prompt template customization to better fit specific use cases:</p>"},{"location":"chatbot/rag/query-pipeline/#template-variables","title":"Template Variables","text":"<p>You can use the following variables in your prompt templates:</p> <ul> <li><code>{user_question}</code>: The original question asked by the user</li> <li><code>{retrieved_context}</code>: The full context assembled from retrieved chunks</li> <li><code>{retrieved_chunks}</code>: An array of individual content chunks with metadata</li> <li><code>{chunk_count}</code>: The number of chunks retrieved</li> <li><code>{confidence_score}</code>: The confidence score of the retrieval</li> </ul>"},{"location":"chatbot/rag/query-pipeline/#custom-prompt-configuration","title":"Custom Prompt Configuration","text":"<p>Prompt templates can be customized through environment variables or the configuration API:</p> <pre><code># Set a custom prompt template\nexport OBELISK_PROMPT_TEMPLATE=\"You are an Obelisk expert. Use the following information to answer the question:\\n\\n{retrieved_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n\n# Or using the config API\nobelisk-rag config --set \"prompt_template=You are an Obelisk expert. Use the following information to answer the question:\\n\\n{retrieved_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n</code></pre>"},{"location":"chatbot/rag/query-pipeline/#advanced-prompt-engineering-techniques","title":"Advanced Prompt Engineering Techniques","text":"<p>For optimal RAG performance, consider these prompt engineering practices:</p> <ol> <li>Clear instructions: Include specific instructions on how to use the context</li> <li>Context formatting: Format the context for better readability by the model</li> <li>Response formatting: Specify desired response format (bullets, paragraphs, etc.)</li> <li>Source attribution: Instruct the model to cite sources from the retrieved chunks</li> <li>Fallback handling: Guide how to respond when information is not in the context</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#response-generation","title":"Response Generation","text":"<p>The final step involves:</p> <ol> <li>Model invocation: Send the assembled prompt to Ollama</li> <li>Parameter optimization: Adjust temperature, top_p, etc.</li> <li>Citation tracking: Maintain source references</li> <li>Response validation: Ensure factuality and relevance</li> <li>Fallback strategies: Handle cases with no relevant context</li> </ol>"},{"location":"chatbot/rag/query-pipeline/#measuring-effectiveness","title":"Measuring Effectiveness","text":"<p>The RAG pipeline will include evaluation metrics:</p> <ul> <li>Retrieval precision/recall: Measure retrieval quality</li> <li>Answer relevance: Assess response relevance</li> <li>Factual accuracy: Verify factual correctness</li> <li>Citation accuracy: Check if sources are properly cited</li> <li>User satisfaction: Collect user feedback</li> </ul>"},{"location":"chatbot/rag/using-rag/","title":"Using the Obelisk RAG System","text":"<p>This guide provides step-by-step instructions for setting up and using the Retrieval Augmented Generation (RAG) system in Obelisk. The RAG system enhances your chatbot by providing it with contextual information from your documentation.</p>"},{"location":"chatbot/rag/using-rag/#quick-start","title":"Quick Start","text":"<p>The RAG system is accessible through the <code>obelisk-rag</code> command-line tool. Here's how to get started:</p> <pre><code># Index your documentation\nobelisk-rag index\n\n# Query the system\nobelisk-rag query \"What is Obelisk?\"\n\n# Start the API server\nobelisk-rag serve --watch\n</code></pre>"},{"location":"chatbot/rag/using-rag/#installation","title":"Installation","text":"<p>The RAG system is included with Obelisk. If you've already installed Obelisk using Poetry, you have everything you need:</p> <pre><code># Verify installation\npoetry run obelisk-rag --help\n</code></pre> <p>If you're using Docker, the RAG system is available in the Obelisk container:</p> <pre><code>docker-compose up obelisk\ndocker exec -it obelisk obelisk-rag --help\n</code></pre>"},{"location":"chatbot/rag/using-rag/#setup-and-configuration","title":"Setup and Configuration","text":""},{"location":"chatbot/rag/using-rag/#basic-configuration","title":"Basic Configuration","text":"<p>By default, the RAG system is configured to:</p> <ol> <li>Read documentation from the <code>./vault</code> directory</li> <li>Store vector embeddings in <code>./.obelisk/vectordb</code></li> <li>Connect to Ollama at <code>http://localhost:11434</code></li> <li>Use <code>llama3</code> as the LLM and <code>mxbai-embed-large</code> as the embedding model</li> </ol> <p>You can view the current configuration with:</p> <pre><code>obelisk-rag config --show\n</code></pre>"},{"location":"chatbot/rag/using-rag/#custom-configuration","title":"Custom Configuration","text":"<p>You can customize the configuration using:</p> <ol> <li>Environment variables:</li> </ol> <pre><code># Set the vault directory\nexport VAULT_DIR=\"/path/to/your/docs\"\n\n# Set the Ollama URL\nexport OLLAMA_URL=\"http://ollama:11434\"\n\n# Set the models\nexport OLLAMA_MODEL=\"mistral\"\nexport EMBEDDING_MODEL=\"mxbai-embed-large\"\n\n# Set chunking parameters\nexport CHUNK_SIZE=\"1500\"\nexport CHUNK_OVERLAP=\"200\"\n\n# Set retrieval parameters\nexport RETRIEVE_TOP_K=\"5\"\n\n# Set API settings\nexport API_HOST=\"0.0.0.0\"\nexport API_PORT=\"8000\"\n</code></pre> <ol> <li>Command-line configuration:</li> </ol> <pre><code># Set a configuration value\nobelisk-rag config --set \"vault_dir=/path/to/your/docs\"\nobelisk-rag config --set \"ollama_model=mistral\"\n</code></pre> <ol> <li>Command-specific options:</li> </ol> <pre><code># Specify vault directory for indexing\nobelisk-rag index --vault /path/to/your/docs\n\n# Specify API host and port\nobelisk-rag serve --host 0.0.0.0 --port 9000\n</code></pre>"},{"location":"chatbot/rag/using-rag/#indexing-your-documentation","title":"Indexing Your Documentation","text":"<p>Before you can query your documentation, you need to index it. This process:</p> <ol> <li>Reads all markdown files in your vault</li> <li>Extracts content and metadata</li> <li>Chunks the content into appropriate segments</li> <li>Generates embeddings for each chunk</li> <li>Stores the embeddings in a vector database</li> </ol> <p>To index your documentation:</p> <pre><code># Index using the default vault directory\nobelisk-rag index\n\n# Index a specific directory\nobelisk-rag index --vault /path/to/your/docs\n</code></pre> <p>The indexing process might take some time depending on the size of your documentation. Progress will be displayed in the console.</p>"},{"location":"chatbot/rag/using-rag/#querying-the-system","title":"Querying the System","text":"<p>Once your documentation is indexed, you can query it:</p> <pre><code># Ask a question\nobelisk-rag query \"How do I customize the theme?\"\n\n# Get JSON output\nobelisk-rag query \"What is the configuration format?\" --json\n</code></pre> <p>The system will:</p> <ol> <li>Convert your query to an embedding</li> <li>Find the most relevant document chunks</li> <li>Include those chunks as context for the LLM</li> <li>Generate a response based on the documentation</li> </ol> <p>The output includes: - The query - The generated response - The sources of information used</p>"},{"location":"chatbot/rag/using-rag/#starting-the-api-server","title":"Starting the API Server","text":"<p>For integration with applications, you can start the RAG API server:</p> <pre><code># Start the server\nobelisk-rag serve\n\n# Start the server with document watching\nobelisk-rag serve --watch\n\n# Specify host and port\nobelisk-rag serve --host 0.0.0.0 --port 9000\n</code></pre> <p>The <code>--watch</code> flag enables real-time document monitoring, so changes to your documentation will be automatically indexed.</p>"},{"location":"chatbot/rag/using-rag/#api-endpoints","title":"API Endpoints","text":"<p>The API server provides the following endpoints:</p> <ol> <li>GET /stats</li> <li>Returns statistics about the RAG system</li> <li> <p>Example: <code>curl http://localhost:8000/stats</code></p> </li> <li> <p>POST /v1/chat/completions</p> </li> <li>OpenAI-compatible endpoint for chat completions</li> <li>Enables integration with tools expecting OpenAI API format</li> <li>Example:      <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"obelisk-rag\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is Obelisk?\"}\n    ],\n    \"temperature\": 0.7\n  }'\n</code></pre></li> </ol>"},{"location":"chatbot/rag/using-rag/#openai-compatible-api-details","title":"OpenAI-Compatible API Details","text":"<p>The <code>/v1/chat/completions</code> endpoint implements the OpenAI Chat API format, allowing seamless integration with applications that support OpenAI's API.</p> <p>Request Format: <pre><code>{\n  \"model\": \"string\",\n  \"messages\": [\n    {\n      \"role\": \"user|system|assistant\",\n      \"content\": \"string\"\n    }\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": null,\n  \"stream\": false\n}\n</code></pre></p> <p>Field descriptions: - <code>model</code>: Identifier for the model (can be any string, used for tracking) - <code>messages</code>: Array of message objects with roles and content - <code>temperature</code>: Controls randomness in response generation (0-1) - <code>max_tokens</code>: Maximum tokens to generate (optional) - <code>stream</code>: Whether to stream the response (default: false)</p> <p>Response Format: <pre><code>{\n  \"id\": \"rag-chat-completion-12345\",\n  \"object\": \"chat.completion\",\n  \"created\": 1683494100,\n  \"model\": \"obelisk-rag\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Obelisk is a tool that transforms Obsidian vaults into MkDocs Material Theme sites with AI integration.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  },\n  \"sources\": [\n    {\n      \"content\": \"Obelisk is a tool that transforms Obsidian vaults...\",\n      \"source\": \"index.md\"\n    }\n  ]\n}\n</code></pre></p> <p>Note that the response includes a <code>sources</code> field that contains information about the document sources used for generating the response, when context is found from the RAG system.</p> <p>Note that the RAG system extracts the query from the last user message in the <code>messages</code> array and processes it through the RAG pipeline before generating a response.</p>"},{"location":"chatbot/rag/using-rag/#integration-with-open-webui","title":"Integration with Open WebUI","text":"<p>You can integrate the RAG API with Open WebUI using the OpenAI-compatible endpoint for a more seamless experience.</p>"},{"location":"chatbot/rag/using-rag/#setup","title":"Setup","text":"<ol> <li> <p>Ensure the RAG API server is running:    <pre><code>obelisk-rag serve --watch\n</code></pre></p> </li> <li> <p>In your docker-compose.yaml, configure Open WebUI to use the RAG service:    <pre><code>services:\n  open-webui:\n    # ... other configuration ...\n    environment:\n      # ... other environment variables ...\n      - RAG_ENABLED=true\n      - RAG_SERVICE_TYPE=custom\n      - RAG_SERVICE_URL=http://obelisk-rag:8000/v1\n      - \"RAG_TEMPLATE=You are a helpful assistant. Use the following pieces of retrieved context to answer the user's question.\\n\\nContext:\\n{{context}}\\n\\nUser question: {{query}}\"\n</code></pre></p> </li> <li> <p>Select any model in the Open WebUI interface</p> </li> </ol> <p>For more detailed integration instructions, see OpenWebUI Integration.</p> <p>Now your chat interface will provide responses based on your documentation!</p>"},{"location":"chatbot/rag/using-rag/#advanced-features","title":"Advanced Features","text":""},{"location":"chatbot/rag/using-rag/#vector-database-management","title":"Vector Database Management","text":"<p>You can view statistics about the vector database:</p> <pre><code># View database stats\nobelisk-rag stats\n\n# View stats in JSON format\nobelisk-rag stats --json\n</code></pre>"},{"location":"chatbot/rag/using-rag/#document-watching","title":"Document Watching","text":"<p>The RAG system can watch for changes to your documentation and update the index in real-time:</p> <pre><code># Start the API server with document watching\nobelisk-rag serve --watch\n</code></pre> <p>This is useful during development when you're actively updating your documentation.</p>"},{"location":"chatbot/rag/using-rag/#troubleshooting-and-debugging","title":"Troubleshooting and Debugging","text":"<p>The RAG system includes built-in debugging capabilities to help diagnose issues. When you encounter problems, you can enable debug mode for detailed error information:</p> <pre><code># Enable debug mode\nexport RAG_DEBUG=1\nobelisk-rag query \"Why isn't this working?\"\n</code></pre>"},{"location":"chatbot/rag/using-rag/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"chatbot/rag/using-rag/#connection-issues-with-ollama","title":"Connection Issues with Ollama","text":"<p>If you see errors connecting to Ollama:</p> <pre><code>Error: HTTPConnectionError: Connection error when connecting to Ollama service\n</code></pre> <p>Debug with:</p> <pre><code># Check Ollama connection\nexport RAG_DEBUG=1\ncurl -v http://localhost:11434/api/embeddings\n</code></pre> <p>Solution: Ensure Ollama is running and properly configured: <pre><code># Check if Ollama is running\ndocker ps | grep ollama\n\n# If not running, start it\ndocker-compose up -d ollama\n\n# Update configuration with correct URL\nobelisk-rag config --set \"ollama_url=http://localhost:11434\"\n</code></pre></p>"},{"location":"chatbot/rag/using-rag/#vector-database-errors","title":"Vector Database Errors","text":"<p>If you experience vector database issues:</p> <pre><code>Error: Failed to connect to vector database\n</code></pre> <p>Debug with:</p> <pre><code># Enable debug mode and check stats\nexport RAG_DEBUG=1\nobelisk-rag stats\n</code></pre> <p>Solution: Verify the vector database directory exists and has proper permissions: <pre><code># Check vector database directory\nls -la ./.obelisk/vectordb\n\n# If missing, create it\nmkdir -p ./.obelisk/vectordb\n</code></pre></p>"},{"location":"chatbot/rag/using-rag/#document-processing-issues","title":"Document Processing Issues","text":"<p>If documents aren't being indexed properly:</p> <pre><code>Error: No chunks generated from document processing\n</code></pre> <p>Debug with:</p> <pre><code># Process a single file with debug mode\nexport RAG_DEBUG=1\nobelisk-rag index --vault /path/to/specific/file.md\n</code></pre> <p>Solution: Check the file format and ensure it's valid markdown: <pre><code># Validate markdown syntax\nnpx markdownlint /path/to/specific/file.md\n</code></pre></p>"},{"location":"chatbot/rag/using-rag/#using-debug-traces-for-advanced-troubleshooting","title":"Using Debug Traces for Advanced Troubleshooting","text":"<p>When debug mode is enabled, the system provides full tracebacks for errors. These tracebacks can help identify the root cause of issues:</p> <pre><code># Run with debug mode\nexport RAG_DEBUG=1\nobelisk-rag serve\n\n# Example traceback output:\n# Traceback (most recent call last):\n#   File \"/app/obelisk/rag/cli.py\", line 334, in main\n#     handle_serve(args)\n#   File \"/app/obelisk/rag/cli.py\", line 224, in handle_serve\n#     service = RAGService(RAGConfig(config))\n#   ...\n</code></pre>"},{"location":"chatbot/rag/using-rag/#checking-component-status","title":"Checking Component Status","text":"<p>Debug integration issues by examining each component:</p> <pre><code># Check embedding service\nexport RAG_DEBUG=1\nobelisk-rag query \"test\" --json | grep embedding\n\n# Check document processor \nexport RAG_DEBUG=1\nobelisk-rag stats\n\n# Examine API endpoints\nexport RAG_DEBUG=1\nobelisk-rag serve\n# Look for \"Available route:\" messages in the output\n</code></pre>"},{"location":"chatbot/rag/using-rag/#debug-mode-in-docker-environment","title":"Debug Mode in Docker Environment","text":"<p>If running in Docker, use:</p> <pre><code># Set debug environment variable in docker-compose.yaml\nservices:\n  obelisk:\n    environment:\n      - RAG_DEBUG=1\n\n# Or set it when running a command in the container\ndocker exec -it obelisk bash -c \"export RAG_DEBUG=1 &amp;&amp; obelisk-rag query 'test query'\"\n</code></pre>"},{"location":"chatbot/rag/using-rag/#architecture","title":"Architecture","text":"<p>The Obelisk RAG system consists of several components:</p> <ol> <li>Document Processor: Handles parsing and chunking of markdown files</li> <li>Embedding Service: Generates vector embeddings using Ollama</li> <li>Vector Storage: Stores and retrieves embeddings using ChromaDB</li> <li>RAG Service: Integrates all components for document retrieval and generation</li> <li>CLI Interface: Provides command-line access to the system</li> </ol> <p>For more details on the architecture, see Architecture Draft and Implementation.</p>"},{"location":"chatbot/rag/using-rag/#next-steps","title":"Next Steps","text":"<p>Now that you have the RAG system set up, you might want to:</p> <ol> <li>Customize the prompt template</li> <li>Integrate with other vector databases</li> <li>Evaluate and improve retrieval quality</li> <li>Explore advanced Ollama models</li> </ol>"},{"location":"chatbot/rag/vector-database/","title":"Vector Database Integration","text":"<p>The Obelisk RAG pipeline uses a vector database to store and query document embeddings. This page documents the current implementation and future options.</p>"},{"location":"chatbot/rag/vector-database/#current-implementation-chromadb","title":"Current Implementation: ChromaDB","text":"<p>Obelisk's RAG system currently uses ChromaDB as its vector database. ChromaDB was chosen for its:</p> <ul> <li>Lightweight embeddable architecture</li> <li>Ease of integration with LangChain</li> <li>Simple persistence model</li> <li>Good performance for small to medium document collections</li> <li>No additional services required (runs in-process)</li> </ul>"},{"location":"chatbot/rag/vector-database/#configuration","title":"Configuration","text":"<p>ChromaDB is configured through environment variables or the RAG configuration system:</p> <pre><code># Configure ChromaDB storage location\nexport CHROMA_DIR=\"/path/to/vectordb\"\n</code></pre> <p>The default storage location is <code>./.obelisk/vectordb/</code> in development and <code>/app/data/chroma_db</code> in Docker.</p>"},{"location":"chatbot/rag/vector-database/#implementation-details","title":"Implementation Details","text":"<p>The current ChromaDB implementation:</p> <ul> <li>Uses persistent storage at the configured CHROMA_DIR location</li> <li>Stores document content, metadata, and embeddings</li> <li>Utilizes the default ChromaDB collection</li> <li>Filters metadata to ensure compatibility (only primitive types)</li> <li>Provides similarity search with configurable k value (default: 3)</li> <li>Integrates with Ollama's embedding models (default: mxbai-embed-large)</li> </ul>"},{"location":"chatbot/rag/vector-database/#limitations","title":"Limitations","text":"<p>The current implementation has some limitations:</p> <ul> <li>Limited metadata filtering capabilities</li> <li>No batch optimization for large document sets</li> <li>Basic error handling for database corruption</li> <li>No custom ChromaDB settings for advanced use cases</li> </ul>"},{"location":"chatbot/rag/vector-database/#future-vector-database-options","title":"Future Vector Database Options","text":"<p>As Obelisk's RAG system evolves, several alternative vector databases are being evaluated for different use cases and scale requirements:</p> Database Description Use Case Milvus Distributed vector database Large-scale deployments FAISS Meta's efficient similarity search High-performance requirements Qdrant Scalable vector search engine Advanced filtering needs Weaviate Knowledge graph + vector search Complex semantic relationships"},{"location":"chatbot/rag/vector-database/#current-embedding-implementation","title":"Current Embedding Implementation","text":"<p>The current implementation uses:</p> <ul> <li>Local embedding models via Ollama (default: <code>mxbai-embed-large</code>)</li> <li>1024-dimension vectors optimized for semantic similarity</li> <li>Synchronous embedding generation through Ollama's API</li> </ul>"},{"location":"chatbot/rag/vector-database/#database-schema","title":"Database Schema","text":"<p>The ChromaDB instance currently stores:</p> <ol> <li>Document embeddings: Vector representations of content chunks (1024-dimensional)</li> <li>Metadata: Basic information about each chunk (source, title, etc.)</li> <li>Document content: The original text for retrieval</li> </ol> <p>Current metadata schema includes:</p> <pre><code>{\n  \"metadata\": {\n    \"source\": \"development/docker.md\",\n    \"title\": \"Docker Configuration\",\n    \"chunk_id\": \"dev-docker-compose-001\"\n  },\n  \"text\": \"The `docker-compose.yaml` file orchestrates the complete Obelisk stack, including optional AI components...\"\n}\n</code></pre> <p>Note: Only primitive data types (string, number, boolean) are supported in metadata to ensure ChromaDB compatibility.</p>"},{"location":"chatbot/rag/vector-database/#storage-persistence","title":"Storage &amp; Persistence","text":"<p>The current ChromaDB implementation uses persistent storage:</p> <ul> <li>Default location: <code>./.obelisk/vectordb/</code> (development) or <code>/app/data/chroma_db</code> (Docker)</li> <li>Configurable via <code>CHROMA_DIR</code> environment variable</li> <li>Mounted as a Docker volume in containerized deployments for persistence</li> <li>Uses ChromaDB's built-in persistence mechanism</li> </ul>"},{"location":"chatbot/rag/vector-database/#current-query-implementation","title":"Current Query Implementation","text":"<p>The RAG query process:</p> <ol> <li>Embed the user query using the same embedding model</li> <li>Perform similarity search to find the top-k most relevant chunks (default k=3)</li> <li>Extract and format the retrieved chunks for context</li> <li>Generate a response using the retrieved context</li> </ol>"},{"location":"chatbot/rag/vector-database/#integration-points","title":"Integration Points","text":"<p>The current vector database integrates with:</p> <ul> <li>RAG CLI: Direct interface for indexing and querying</li> <li>Document watcher: Monitors file changes for real-time updates</li> <li>Ollama API: Uses Ollama for embeddings and generation</li> <li>OpenAI-compatible API: Provides an endpoint for tools like Open WebUI</li> </ul>"},{"location":"chatbot/rag/vector-database/#current-configuration-options","title":"Current Configuration Options","text":"<p>Configure the vector database through environment variables:</p> <pre><code># Vector database location\nexport CHROMA_DIR=\"/path/to/db\"\n\n# Number of results to retrieve\nexport RETRIEVE_TOP_K=5\n\n# Embedding model to use\nexport EMBEDDING_MODEL=\"mxbai-embed-large\" \n</code></pre>"},{"location":"chatbot/rag/vector-database/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements to the vector database implementation:</p> <ol> <li>Advanced Filtering: Enhanced metadata filtering capabilities</li> <li>Hybrid Search: Combining vector search with keyword search</li> <li>Batch Processing: Optimized handling of large document collections</li> <li>Milvus Integration: Support for Milvus as an alternative backend</li> <li>Custom Collection Management: Multiple collections for different document types</li> </ol>"},{"location":"chatbot/rag/vector-database/#alternative-databases","title":"Alternative Databases","text":"<p>While ChromaDB is the default vector database for Obelisk's RAG implementation, several alternatives can be considered for different use cases:</p>"},{"location":"chatbot/rag/vector-database/#qdrant","title":"Qdrant","text":"<p>Benefits for Obelisk: - High-performance search with HNSW algorithm - Powerful filtering capabilities - Cloud-hosted or self-hosted options - Strong scaling capabilities</p> <p>Integration Example: <pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.http import models\n\n# Initialize Qdrant client\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Create collection for Obelisk embeddings\nclient.create_collection(\n    collection_name=\"obelisk_docs\",\n    vectors_config=models.VectorParams(\n        size=768,  # Embedding dimensions\n        distance=models.Distance.COSINE\n    )\n)\n\n# Store embeddings\nclient.upload_points(\n    collection_name=\"obelisk_docs\",\n    points=[\n        models.PointStruct(\n            id=chunk_id,\n            vector=embedding,\n            payload={\"text\": text, \"metadata\": metadata}\n        )\n        for chunk_id, embedding, text, metadata in zip(ids, embeddings, texts, metadatas)\n    ]\n)\n</code></pre></p>"},{"location":"chatbot/rag/vector-database/#milvus","title":"Milvus","text":"<p>Benefits for Obelisk: - Cloud-native architecture - Handles billions of vectors - Excellent for large documentation sites - Advanced query capabilities</p> <p>When to choose Milvus: - Your documentation exceeds 100,000 pages - You need multi-tenant isolation - You require complex metadata filtering - Enterprise deployment with high availability requirements</p>"},{"location":"chatbot/rag/vector-database/#faiss-with-sqlite","title":"FAISS (with SQLite)","text":"<p>Benefits for Obelisk: - Extremely lightweight - Optimized for in-memory performance - No additional services required - Perfect for small to medium documentation</p> <p>Integration approach: - Store vectors in FAISS - Use SQLite for metadata and text storage - Join results using document IDs</p>"},{"location":"chatbot/rag/vector-database/#configuring-alternative-databases","title":"Configuring Alternative Databases","text":"<p>To use an alternative vector database with Obelisk:</p> <pre><code># In a future configuration file\nrag:\n  vector_db:\n    type: \"qdrant\"  # Options: chroma, qdrant, milvus, faiss\n    connection:\n      host: \"localhost\"\n      port: 6333\n    collection: \"obelisk_docs\"\n    embedding_dimensions: 768\n</code></pre>"},{"location":"chatbot/rag/milvus/integration/","title":"Milvus Vector Database Integration","text":"<p>This document describes the integration of Milvus vector database into the Obelisk project to enhance RAG (Retrieval-Augmented Generation) capabilities.</p>"},{"location":"chatbot/rag/milvus/integration/#overview","title":"Overview","text":"<p>Milvus is an open-source vector database designed for embedding similarity search and AI applications. It provides:</p> <ul> <li>High-performance vector similarity search</li> <li>Scalable architecture</li> <li>Flexible index types for different use cases</li> <li>Support for hybrid search (vector + scalar filtering)</li> </ul> <p>In the Obelisk project, Milvus serves as the vector database backend for storing and retrieving document embeddings used in the RAG pipeline.</p>"},{"location":"chatbot/rag/milvus/integration/#architecture","title":"Architecture","text":"<p>The Milvus integration consists of the following components:</p> <ol> <li>Milvus Standalone Service: A self-contained Milvus instance that includes:</li> <li>Milvus Server</li> <li>Metadata Management (etcd)</li> <li> <p>Storage Backend (MinIO)</p> </li> <li> <p>OpenWebUI Integration: Configuration to use Milvus as the vector store for OpenWebUI's RAG features.</p> </li> <li> <p>Obelisk-RAG Integration: Configuration to use Milvus as the vector store for Obelisk's RAG service.</p> </li> </ol>"},{"location":"chatbot/rag/milvus/integration/#deployment","title":"Deployment","text":"<p>The Milvus integration is defined in the <code>docker-compose.yaml</code> file and includes:</p>"},{"location":"chatbot/rag/milvus/integration/#milvus-ecosystem-services","title":"Milvus Ecosystem Services","text":"<ol> <li> <p>etcd: For metadata management    <pre><code>etcd:\n  container_name: milvus-etcd\n  image: quay.io/coreos/etcd:v3.5.18\n  # Configuration...\n</code></pre></p> </li> <li> <p>MinIO: For storage management    <pre><code>minio:\n  container_name: milvus-minio\n  image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n  # Configuration...\n</code></pre></p> </li> <li> <p>Milvus Server: The main Milvus service    <pre><code>milvus:\n  container_name: milvus-standalone\n  image: milvusdb/milvus:v2.5.10\n  # Configuration...\n</code></pre></p> </li> </ol>"},{"location":"chatbot/rag/milvus/integration/#openwebui-integration","title":"OpenWebUI Integration","text":"<p>OpenWebUI is configured to use Milvus for its RAG features:</p> <pre><code>open-webui:\n  # Other configuration...\n  environment:\n    # RAG configuration\n    - RETRIEVAL_ENABLED=true\n    - RETRIEVAL_VECTOR_STORE=milvus\n    - MILVUS_URI=http://milvus:19530\n    - MILVUS_HOST=milvus\n    - MILVUS_PORT=19530\n    # Other environment variables...\n</code></pre>"},{"location":"chatbot/rag/milvus/integration/#obelisk-rag-integration","title":"Obelisk-RAG Integration","text":"<p>The Obelisk-RAG service is configured to use Milvus:</p> <pre><code>obelisk-rag:\n  # Other configuration...\n  environment:\n    # Milvus configuration\n    - VECTOR_DB=milvus\n    - MILVUS_URI=http://milvus:19530\n    - MILVUS_HOST=milvus\n    - MILVUS_PORT=19530\n    # Other environment variables...\n</code></pre>"},{"location":"chatbot/rag/milvus/integration/#verification","title":"Verification","text":"<p>The Milvus integration has been verified using test scripts that:</p> <ol> <li>Connect to Milvus and create a collection</li> <li>Insert document embeddings</li> <li>Create an index for efficient searching</li> <li>Perform vector similarity searches</li> <li>Verify connectivity from OpenWebUI</li> </ol> <p>Test scripts can be found in the <code>/hack</code> directory: - <code>test_milvus.py</code>: Basic Milvus functionality test - <code>test_rag_milvus.py</code>: Verification of RAG configuration - <code>test_rag_e2e.py</code>: End-to-end RAG pipeline test</p>"},{"location":"chatbot/rag/milvus/integration/#usage","title":"Usage","text":"<p>When the Obelisk stack is deployed with <code>docker-compose up</code>, the Milvus integration is automatically configured and available. Users can:</p> <ol> <li>Upload documents via the OpenWebUI interface</li> <li>Embed documents into the Milvus vector database</li> <li>Use RAG for enhanced AI responses based on document context</li> </ol> <p>No additional configuration is required for basic usage.</p>"},{"location":"chatbot/rag/milvus/integration/#advanced-configuration","title":"Advanced Configuration","text":"<p>For advanced configurations, you can modify the following environment variables:</p>"},{"location":"chatbot/rag/milvus/integration/#milvus-server-configuration","title":"Milvus Server Configuration","text":"<ul> <li><code>ETCD_ENDPOINTS</code>: etcd endpoint for metadata storage</li> <li><code>MINIO_ADDRESS</code>: MinIO address for object storage</li> </ul>"},{"location":"chatbot/rag/milvus/integration/#openwebui-rag-configuration","title":"OpenWebUI RAG Configuration","text":"<ul> <li><code>RETRIEVAL_ENABLED</code>: Enable/disable RAG features</li> <li><code>RETRIEVAL_VECTOR_STORE</code>: Vector database type (milvus)</li> <li><code>MILVUS_URI</code>/<code>MILVUS_HOST</code>/<code>MILVUS_PORT</code>: Connection parameters</li> </ul>"},{"location":"chatbot/rag/milvus/integration/#obelisk-rag-configuration","title":"Obelisk-RAG Configuration","text":"<ul> <li><code>VECTOR_DB</code>: Vector database type (milvus)</li> <li><code>MILVUS_URI</code>/<code>MILVUS_HOST</code>/<code>MILVUS_PORT</code>: Connection parameters</li> </ul>"},{"location":"chatbot/rag/milvus/integration/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li> <p>Cannot connect to Milvus: Ensure the Milvus service is running and network settings are correct    <pre><code>docker ps | grep milvus\ndocker logs milvus-standalone\n</code></pre></p> </li> <li> <p>Vector search returns no results: Verify collection exists and has data    <pre><code>from pymilvus import connections, utility\nconnections.connect(\"default\", host=\"localhost\", port=\"19530\")\nprint(utility.list_collections())\n</code></pre></p> </li> <li> <p>OpenWebUI RAG not working: Check OpenWebUI logs for connection issues    <pre><code>docker logs open-webui | grep -i milvus\n</code></pre></p> </li> </ol>"},{"location":"chatbot/rag/openai/integration/","title":"OpenAI Model Integration","text":"<p>This document covers integrating OpenAI models with Obelisk's RAG system.</p>"},{"location":"chatbot/rag/openai/integration/#supported-models","title":"Supported Models","text":"<p>Obelisk RAG supports the following OpenAI models for embedding and completion tasks:</p>"},{"location":"chatbot/rag/openai/integration/#model-datasheets","title":"Model Datasheets","text":"Feature GPT-4o GPT-4.1 text-embedding-3-large Description Fast, intelligent, flexible GPT model Flagship GPT model for complex tasks Most capable embedding model Use in Obelisk Completion generation Completion generation Document and query embedding Pricing (per 1M tokens) $2.50 input / $10.00 output $2.00 input / $8.00 output $0.13 Context Window 128,000 tokens 1,047,576 tokens N/A Max Output Tokens 16,384 32,768 N/A Knowledge Cutoff Sep 30, 2023 May 31, 2024 Not applicable Dimensions N/A N/A 3,072 Rate Limits (TPM) Tier 1: 30,000Tier 2: 450,000 Tier 1: 30,000Tier 2: 450,000 Tier 1: 1,000,000Tier 2: 1,000,000"},{"location":"chatbot/rag/openai/integration/#configuration","title":"Configuration","text":"<p>When an OpenAI API key is detected in the environment, Obelisk RAG can optionally use these models instead of local Ollama models, providing enhanced performance and capabilities especially for development environments without GPU access.</p>"},{"location":"chatbot/rag/openai/integration/#environment-variables","title":"Environment Variables","text":"<p>To enable OpenAI models, set the following environment variables:</p> <pre><code># Required\nOPENAI_API_KEY=your_openai_api_key\n\n# Optional\nOPENAI_ORG_ID=your_org_id                      # For enterprise users\nUSE_OPENAI=true                                # Force using OpenAI (defaults to true when key exists)\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large  # Default embedding model\nOPENAI_COMPLETION_MODEL=gpt-4o                 # Default completion model\nEMBEDDING_PROVIDER=openai                      # Set embedding provider (ollama or openai)\nCOMPLETION_PROVIDER=openai                     # Set completion provider (ollama or openai)\n</code></pre>"},{"location":"chatbot/rag/openai/integration/#docker-compose-usage","title":"Docker Compose Usage","text":"<p>When using docker-compose, you can provide the OpenAI API key:</p> <pre><code>OPENAI_API_KEY=your_api_key docker-compose up\n</code></pre>"},{"location":"chatbot/rag/openai/integration/#technical-implementation","title":"Technical Implementation","text":"<p>The OpenAI integration is implemented through several components:</p>"},{"location":"chatbot/rag/openai/integration/#1-litellm-configuration","title":"1. LiteLLM Configuration","text":"<p>LiteLLM acts as a middleware layer that can route requests to either OpenAI or Ollama models. The configuration in <code>litellm-config.yaml</code> includes:</p> <ul> <li>OpenAI model definitions (gpt-4o, text-embedding-3-large)</li> <li>Model aliases for simpler referencing</li> <li>Fallback mechanisms to ensure graceful degradation to Ollama models when needed</li> <li>Prioritized embedding model list</li> </ul> <pre><code># Model routing with fallbacks\nfallbacks: [\n  {\n    \"model\": \"openai/gpt-4o\",\n    \"fallback_model\": \"ollama/llama3\"\n  },\n  {\n    \"model\": \"openai/text-embedding-3-large\", \n    \"fallback_model\": \"ollama/mxbai-embed-large\"\n  }\n]\n</code></pre>"},{"location":"chatbot/rag/openai/integration/#2-initialization-process","title":"2. Initialization Process","text":"<p>During container initialization:</p> <ol> <li>The <code>generate-tokens.sh</code> script checks for an OpenAI API key and adds it to the shared tokens file</li> <li>The <code>configure-services.sh</code> script creates configurations for LiteLLM and Obelisk-RAG based on the presence of the API key</li> <li>Models are automatically prioritized based on availability</li> </ol>"},{"location":"chatbot/rag/openai/integration/#3-fallback-mechanism","title":"3. Fallback Mechanism","text":"<p>If the OpenAI API is unavailable or rate-limited:</p> <ol> <li>LiteLLM will automatically route requests to the specified fallback model (Ollama)</li> <li>This ensures system reliability even when external API services are unavailable</li> <li>No code changes are needed as the fallback is handled at the routing layer</li> </ol>"},{"location":"chatbot/rag/openai/integration/#testing-the-integration","title":"Testing the Integration","text":"<ol> <li> <p>Start the Obelisk stack with your OpenAI API key:    <pre><code>OPENAI_API_KEY=your_api_key docker-compose up\n</code></pre></p> </li> <li> <p>Access the OpenWebUI interface at http://localhost:8080</p> </li> <li> <p>Create a new chat and select LiteLLM as the provider</p> </li> <li> <p>In the model dropdown, you should see OpenAI models (gpt-4o)</p> </li> <li> <p>Test embedding functionality by creating a new RAG collection and uploading a document</p> </li> </ol>"},{"location":"chatbot/rag/openai/integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chatbot/rag/openai/integration/#api-key-issues","title":"API Key Issues","text":"<p>If your OpenAI API key is invalid or has insufficient permissions:</p> <ol> <li>Check the logs in the <code>litellm</code> container for API-related errors</li> <li>Verify the key works using our testing script:    <pre><code>poetry run python /workspaces/obelisk/hack/test_litellm_openai.py\n</code></pre></li> <li>Ensure your OpenAI account has billing enabled for API access</li> </ol>"},{"location":"chatbot/rag/openai/integration/#fallback-issues","title":"Fallback Issues","text":"<p>If fallback to Ollama models isn't working:</p> <ol> <li>Verify Ollama models are downloaded and available</li> <li>Check the fallback configuration in <code>config/litellm_config.yaml</code></li> <li>Examine the logs for routing-related errors</li> </ol>"},{"location":"chatbot/rag/openai/integration/#limitations","title":"Limitations","text":"<ul> <li>Embedding caching is not yet implemented (planned for future releases)</li> <li>Rate limiting for OpenAI API is not currently enforced</li> <li>Cost optimization features are not yet available</li> </ul>"},{"location":"cloud/Cloud%20Native%20Tooling/","title":"Cloud Native Tooling","text":"<p>A collection of cloud-native tools and resources.</p> <p>Categories: cloud \u2601\ufe0f \u2022 kubernetes \ud83d\udea2 \u2022 devops \ud83d\udd04 \u2022 infrastructure \ud83c\udfd7\ufe0f</p>"},{"location":"cloud/Cloud%20Native%20Tooling/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes</p> <p>Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts.</p>"},{"location":"cloud/Cloud%20Native%20Tooling/#key-components","title":"Key Components","text":"<ul> <li>Pods: The smallest deployable units in Kubernetes</li> <li>Services: Abstract way to expose applications running on pods</li> <li>Deployments: Declarative updates for pods and replica sets</li> <li>ConfigMaps: External configuration for applications</li> </ul>"},{"location":"cloud/Cloud%20Native%20Tooling/#infrastructure-as-code","title":"Infrastructure as Code","text":"<pre><code>graph TD\n    A[Infrastructure as Code] --&gt; B[Terraform]\n    A --&gt; C[CloudFormation]\n    A --&gt; D[Pulumi]\n    B --&gt; E[Providers]\n    E --&gt; F[AWS]\n    E --&gt; G[Azure]\n    E --&gt; H[GCP]</code></pre>"},{"location":"cloud/Cloud%20Native%20Tooling/#container-technologies","title":"Container Technologies","text":"Technology Description Use Case Docker Container platform Development, packaging Podman Daemonless container engine Security-focused environments Buildah Building OCI images CI/CD pipelines Containerd Container runtime Kubernetes environments"},{"location":"cloud/Cloud%20Native%20Tooling/#devops-practices","title":"DevOps Practices","text":"<ul> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Infrastructure as Code</li> <li>Observability</li> <li>Security as Code</li> </ul> <p>Best Practice</p> <p>Implement GitOps workflows to manage infrastructure and application deployments through Git repositories.</p>"},{"location":"customization/","title":"MkDocs Customization Guide","text":"<p>This section documents all customizations applied to the MkDocs Material theme in the Obelisk project.</p>"},{"location":"customization/#overview","title":"Overview","text":"<p>Obelisk uses MkDocs with the Material theme to generate a static site from Markdown files. Several customizations have been applied to enhance the appearance and functionality.</p>"},{"location":"customization/#customization-categories","title":"Customization Categories","text":"<ul> <li>CSS Styling - Custom CSS styles applied to the Material theme</li> <li>HTML Templates - Customized HTML template overrides</li> <li>JavaScript Enhancements - Custom JavaScript functionality</li> <li>Python Integration - Python scripts and utilities for MkDocs</li> <li>Versioning - Documentation versioning with Mike</li> </ul>"},{"location":"customization/#configuration","title":"Configuration","text":"<p>The primary configuration for MkDocs is in the <code>mkdocs.yml</code> file in the project root. This file controls theme settings, plugins, navigation, and more.</p> <p>See the detailed MkDocs configuration guide for a complete reference.</p>"},{"location":"customization/#directory-structure","title":"Directory Structure","text":"<p>Key directories related to MkDocs customization:</p> <pre><code>/workspaces/obelisk/\n\u251c\u2500\u2500 mkdocs.yml           # Main configuration file\n\u251c\u2500\u2500 vault/               # Content source (similar to \"docs\" in standard MkDocs)\n\u2502   \u251c\u2500\u2500 index.md         # Homepage\n\u2502   \u251c\u2500\u2500 stylesheets/     # Custom CSS\n\u2502   \u2502   \u2514\u2500\u2500 extra.css    # Primary custom styles\n\u2502   \u251c\u2500\u2500 javascripts/     # Custom JavaScript\n\u2502   \u2502   \u2514\u2500\u2500 extra.js     # Custom scripts\n\u2502   \u2514\u2500\u2500 overrides/       # HTML template overrides\n\u2502       \u2514\u2500\u2500 main.html    # Main template override\n\u2514\u2500\u2500 site/                # Output directory (generated)\n    \u2514\u2500\u2500 versions.json    # Versioning information\n</code></pre>"},{"location":"customization/mkdocs-configuration/","title":"MkDocs Configuration","text":"<p>The <code>mkdocs.yml</code> file is the central configuration file for the Obelisk documentation site. This file controls theme settings, plugins, navigation structure, and more.</p>"},{"location":"customization/mkdocs-configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>site_name: Obelisk\nsite_description: Obsidian vault to MkDocs Material Theme site generator with AI integration\nsite_author: Obelisk Team\nsite_url: https://usrbinkat.github.io/obelisk\n\nrepo_name: usrbinkat/obelisk\nrepo_url: https://github.com/usrbinkat/obelisk\nedit_uri: edit/main/vault/\n</code></pre> <p>These settings define: - site_name: The name displayed in the header and browser title - site_description: Used for SEO and metadata - site_author: The author metadata - site_url: The base URL where the site is hosted - repo_name: The name of the GitHub repository  - repo_url: Link to the GitHub repository - edit_uri: Path for the \"edit this page\" functionality</p>"},{"location":"customization/mkdocs-configuration/#theme-configuration","title":"Theme Configuration","text":"<pre><code>theme:\n  name: material\n  custom_dir: vault/overrides\n  features:\n    # Navigation\n    - navigation.instant\n    - navigation.tracking\n    - navigation.tabs\n    - navigation.tabs.sticky\n    - navigation.sections\n    - navigation.expand\n    - navigation.indexes\n    - navigation.top\n    - navigation.footer\n    - navigation.path\n\n    # Table of contents\n    - toc.follow\n    - toc.integrate\n\n    # Search\n    - search.suggest\n    - search.highlight\n    - search.share\n\n    # Content\n    - content.tabs.link\n    - content.code.annotation\n    - content.code.copy\n    - content.action.edit\n    - content.action.view\n\n    # Header anchors and tooltips\n    - header.autohide\n\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n      primary: deep purple\n      accent: deep orange\n    # Dark mode  \n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n      primary: deep purple\n      accent: deep orange\n  font:\n    text: Roboto\n    code: Roboto Mono\n  favicon: assets/favicon.png\n  icon:\n    logo: material/book-open-page-variant\n    repo: fontawesome/brands/github\n  language: en\n</code></pre> <p>Key theme settings: - name: Specifies the Material theme - custom_dir: Directory for template overrides - features: Enables specific theme features - palette: Defines color schemes for light and dark mode - font: Specifies the fonts for text and code - favicon: Path to the favicon - icon: Icons for logo and repository - language: Default language</p>"},{"location":"customization/mkdocs-configuration/#markdown-extensions","title":"Markdown Extensions","text":"<pre><code>markdown_extensions:\n  - admonition\n  - attr_list\n  - def_list\n  - footnotes\n  - md_in_html\n  - toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.caret\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: usrbinkat\n      repo: obelisk\n  - pymdownx.mark\n  - pymdownx.smartsymbols\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n</code></pre> <p>This section enables various Markdown extensions that enhance the standard Markdown syntax: - admonition: Adds note, warning, tip blocks - attr_list: Allows adding HTML attributes to elements - toc: Table of contents with permalinks - pymdownx.emoji: Adds emoji support - pymdownx.highlight: Code syntax highlighting - pymdownx.superfences: Fenced code blocks with support for Mermaid diagrams - pymdownx.tabbed: Tabbed content - pymdownx.tasklist: Task lists with custom checkboxes</p>"},{"location":"customization/mkdocs-configuration/#plugins","title":"Plugins","text":"<pre><code>plugins:\n  - search\n  - git-revision-date-localized:\n      enable_creation_date: true\n      fallback_to_build_date: true\n      type: date\n  - minify:\n      minify_html: true\n      minify_js: true\n      minify_css: true\n  - awesome-pages\n  - glightbox:\n      touchNavigation: true\n      loop: false\n      effect: zoom\n      width: 100%\n      height: auto\n      zoomable: true\n      draggable: true\n</code></pre> <p>Plugins add additional functionality: - search: Adds search functionality - git-revision-date-localized: Shows last update date based on git - minify: Reduces file sizes - awesome-pages: Simplifies navigation configuration - glightbox: Enhanced image viewing</p>"},{"location":"customization/mkdocs-configuration/#extra-configuration","title":"Extra Configuration","text":"<pre><code>extra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/usrbinkat\n      name: GitHub\n  version:\n    provider: mike\n    default: 0.1.0\n  consent:\n    title: Cookie consent\n    description: &gt;- \n      We use cookies to recognize your repeated visits and preferences, as well\n      as to analyze traffic and understand where our visitors are coming from.\n  generator: false\n\nextra_css:\n  - stylesheets/extra.css\nextra_javascript:\n  - javascripts/extra.js\n</code></pre> <p>Additional configuration options: - social: Social media links for the footer - version: Documentation versioning configuration - consent: Cookie consent configuration - generator: Hides the \"Made with Material for MkDocs\" notice - extra_css/extra_javascript: Paths to custom CSS and JavaScript files</p>"},{"location":"customization/mkdocs-configuration/#navigation","title":"Navigation","text":"<pre><code>docs_dir: vault\n\nnav:\n  - Home: index.md\n  - Cloud: \n    - Overview: cloud/Cloud Native Tooling.md\n</code></pre> <ul> <li>docs_dir: Specifies the directory containing documentation files</li> <li>nav: Defines the navigation structure</li> <li>Can contain nested sections</li> <li>Each entry has a title and a path to a markdown file</li> </ul>"},{"location":"customization/css/","title":"CSS Customization","text":"<p>The Obelisk project includes extensive CSS customizations to enhance the appearance and user experience of the default Material theme.</p>"},{"location":"customization/css/#implementation","title":"Implementation","text":"<p>Custom CSS is applied through the <code>extra_css</code> setting in <code>mkdocs.yml</code>:</p> <pre><code>extra_css:\n  - stylesheets/extra.css\n</code></pre> <p>The primary CSS file is located at <code>vault/stylesheets/extra.css</code>.</p>"},{"location":"customization/css/#key-customizations","title":"Key Customizations","text":""},{"location":"customization/css/#theme-colors","title":"Theme Colors","text":"<p>Custom color variables are defined in the <code>:root</code> selector:</p> <pre><code>:root {\n  /* Primary theme colors - modern purple-blue gradient */\n  --md-primary-fg-color: #5e35b1;\n  --md-primary-fg-color--light: #9575cd;\n  --md-primary-fg-color--dark: #4527a0;\n\n  /* Accent color - complementary orange for highlights */\n  --md-accent-fg-color: #ff8a65; \n  --md-accent-fg-color--transparent: rgba(255, 138, 101, 0.1);\n}\n</code></pre>"},{"location":"customization/css/#dark-mode-support","title":"Dark Mode Support","text":"<p>Dark mode specific styles ensure proper contrast and readability:</p> <pre><code>/* Dark mode custom variables */\n[data-md-color-scheme=\"slate\"] {\n  --obelisk-box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3), 0 1px 3px rgba(0, 0, 0, 0.2);\n}\n\n[data-md-color-scheme=\"slate\"] .md-content {\n  background-color: var(--md-default-bg-color);\n}\n</code></pre>"},{"location":"customization/css/#announcement-banner","title":"Announcement Banner","text":"<p>A customized announcement banner provides an eye-catching, yet unobtrusive notice at the top of each page:</p> <pre><code>/* Announcement bar styling */\n.md-banner {\n  background-color: #ff8a65; /* Orange background */\n  color: #212121; /* Dark text */\n  padding: 0; /* Remove padding, we'll control this through height */\n  font-size: 0.8rem; /* Slightly larger font size */\n  line-height: 1; /* Minimal line height */\n  height: 36px; /* Balanced height */\n  display: flex;\n  align-items: center; /* Vertical centering */\n  justify-content: center; /* Horizontal centering */\n}\n</code></pre>"},{"location":"customization/css/#layout-enhancements","title":"Layout Enhancements","text":"<p>Content areas use subtle shadows and rounded corners for a modern look:</p> <pre><code>.md-content {\n  background-color: white;\n  border-radius: var(--obelisk-border-radius);\n  box-shadow: var(--obelisk-box-shadow);\n  padding: 1.5rem;\n  margin-bottom: 2rem;\n}\n</code></pre>"},{"location":"customization/css/#typography-improvements","title":"Typography Improvements","text":"<p>Font styles and spacing are optimized for readability:</p> <pre><code>/* Heading styles */\n.md-content h1, \n.md-content h2, \n.md-content h3 {\n  font-weight: 500;\n  margin-top: 2rem;\n}\n\n.md-content h1 {\n  font-size: 2rem;\n  margin-bottom: 1rem;\n  color: var(--md-primary-fg-color--dark);\n}\n</code></pre>"},{"location":"customization/css/#admonition-callout-styling","title":"Admonition &amp; Callout Styling","text":"<p>Enhanced styling for admonitions and custom callouts:</p> <pre><code>/* Special styling for Obsidian compatibility */\n.admonition {\n  border-radius: var(--obelisk-border-radius);\n  margin: 1.5em 0;\n  box-shadow: var(--obelisk-box-shadow);\n  border: none;\n  transition: var(--obelisk-transition);\n}\n\n/* Custom callouts */\n.callout {\n  border-left: 4px solid var(--md-primary-fg-color);\n  padding: 1em 1.5em;\n  margin: 1.2em 0;\n  background-color: var(--md-accent-fg-color--transparent);\n  border-radius: 0 var(--obelisk-border-radius) var(--obelisk-border-radius) 0;\n  box-shadow: var(--obelisk-box-shadow);\n}\n</code></pre>"},{"location":"customization/css/#responsive-design","title":"Responsive Design","text":"<p>Media queries ensure the design works well on all screen sizes:</p> <pre><code>/* Mobile Responsiveness Improvements */\n@media screen and (max-width: 76.1875em) {\n  .md-nav--primary .md-nav__title {\n    background-color: var(--md-primary-fg-color);\n    color: white;\n  }\n\n  .md-sidebar--primary {\n    background-color: white;\n  }\n\n  [data-md-color-scheme=\"slate\"] .md-sidebar--primary {\n    background-color: var(--md-default-bg-color);\n  }\n}\n</code></pre>"},{"location":"customization/css/#full-css-reference","title":"Full CSS Reference","text":"<p>For the complete set of CSS customizations, see the extra.css file.</p>"},{"location":"customization/html/","title":"HTML Template Customization","text":"<p>The Material theme in MkDocs can be customized by overriding specific HTML templates. Obelisk uses this feature to customize key parts of the interface without modifying the theme itself.</p>"},{"location":"customization/html/#implementation","title":"Implementation","text":"<p>Template overrides are enabled by setting the <code>custom_dir</code> property in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  name: material\n  custom_dir: vault/overrides\n</code></pre> <p>All custom templates are placed in the <code>vault/overrides</code> directory.</p>"},{"location":"customization/html/#main-template-override","title":"Main Template Override","text":"<p>The primary template override is <code>main.html</code>, which extends the base template and replaces specific blocks:</p> <pre><code>{% extends \"base.html\" %}\n\n{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n\n{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#template-blocks","title":"Template Blocks","text":"<p>The Material theme provides several blocks that can be overridden:</p> Block Name Description <code>announce</code> Content for the announcement bar at the top of the page <code>outdated</code> Message displayed when viewing an outdated version of the documentation <code>content</code> Main content area of the page <code>extrahead</code> Additional content for the HTML head section <code>footer</code> Footer content"},{"location":"customization/html/#key-customizations","title":"Key Customizations","text":""},{"location":"customization/html/#announcement-banner","title":"Announcement Banner","text":"<p>The announcement banner is customized to display the current Obelisk version with a link to the GitHub repository:</p> <pre><code>{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#outdated-version-message","title":"Outdated Version Message","text":"<p>The message for outdated versions is customized to provide clear navigation to the latest version:</p> <pre><code>{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#adding-new-template-overrides","title":"Adding New Template Overrides","text":"<p>To add a new template override:</p> <ol> <li>Identify the template file you want to override from the Material theme</li> <li>Create a file with the same name in the <code>vault/overrides</code> directory</li> <li>Either extend the base template and override specific blocks, or provide a complete replacement</li> </ol> <p>For example, to override the footer:</p> <pre><code>{% extends \"base.html\" %}\n\n{% block footer %}\n  &lt;footer class=\"md-footer\"&gt;\n    &lt;div class=\"md-footer-meta md-typeset\"&gt;\n      &lt;div class=\"md-footer-meta__inner md-grid\"&gt;\n        &lt;div class=\"md-footer-copyright\"&gt;\n          &lt;div class=\"md-footer-copyright__highlight\"&gt;\n            Powered by Obelisk {{ config.extra.version.default }}\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/footer&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/html/#full-template-reference","title":"Full Template Reference","text":"<p>For more information on template customization, refer to the Material theme documentation.</p>"},{"location":"customization/javascript/","title":"JavaScript Customization","text":"<p>Obelisk includes custom JavaScript to enhance the user experience beyond what's provided by the standard Material theme.</p>"},{"location":"customization/javascript/#implementation","title":"Implementation","text":"<p>JavaScript is added through the <code>extra_javascript</code> setting in <code>mkdocs.yml</code>:</p> <pre><code>extra_javascript:\n  - javascripts/extra.js\n</code></pre> <p>The custom JavaScript file is located at <code>vault/javascripts/extra.js</code>.</p>"},{"location":"customization/javascript/#key-features","title":"Key Features","text":""},{"location":"customization/javascript/#smooth-scrolling","title":"Smooth Scrolling","text":"<p>Implements smooth scrolling for anchor links within the same page:</p> <pre><code>// Add smooth scroll behavior\ndocument.querySelectorAll('a[href^=\"#\"]').forEach(anchor =&gt; {\n  anchor.addEventListener('click', function (e) {\n    e.preventDefault();\n\n    const target = document.querySelector(this.getAttribute('href'));\n    if (target) {\n      target.scrollIntoView({\n        behavior: 'smooth'\n      });\n    }\n  });\n});\n</code></pre>"},{"location":"customization/javascript/#version-footer","title":"Version Footer","text":"<p>Adds a version indicator to the footer:</p> <pre><code>// Add version text to footer\nconst footer = document.querySelector('.md-footer-meta');\nif (footer) {\n  const versionDiv = document.createElement('div');\n  versionDiv.className = 'md-footer-version';\n  versionDiv.innerHTML = '&lt;span&gt;Obelisk v0.1.0&lt;/span&gt;';\n  footer.querySelector('.md-footer-meta__inner').appendChild(versionDiv);\n}\n</code></pre>"},{"location":"customization/javascript/#cookie-consent-handling","title":"Cookie Consent Handling","text":"<p>Automatically handles cookie consent in development environments:</p> <pre><code>// Accept cookie consent by default in development\nconst acceptButtons = document.querySelectorAll('.md-dialog__accept');\nif (acceptButtons.length &gt; 0) {\n  setTimeout(() =&gt; {\n    acceptButtons[0].click();\n  }, 500);\n}\n</code></pre>"},{"location":"customization/javascript/#adding-custom-javascript","title":"Adding Custom JavaScript","text":"<p>To add new JavaScript functionality:</p> <ol> <li>Edit the <code>extra.js</code> file or create a new JavaScript file</li> <li>Add the file to the <code>extra_javascript</code> list in <code>mkdocs.yml</code> if needed</li> <li>Make sure your code is wrapped in a DOM ready event listener:</li> </ol> <pre><code>document.addEventListener('DOMContentLoaded', function() {\n  // Your code here\n});\n</code></pre>"},{"location":"customization/javascript/#integrating-with-material-theme","title":"Integrating with Material Theme","text":"<p>The Material theme provides several JavaScript hooks and events that can be used for customization. Common integration points include:</p> <ul> <li>Search functionality - Customizing search behavior</li> <li>Navigation - Enhancing navigation interactions</li> <li>Color scheme switching - Adding custom handling for light/dark mode</li> <li>Content tabs - Extending tabbed content behavior</li> </ul>"},{"location":"customization/javascript/#full-javascript-reference","title":"Full JavaScript Reference","text":"<p>For the complete set of JavaScript customizations, see the extra.js file.</p>"},{"location":"customization/python/","title":"Python Integration","text":"<p>Obelisk uses Python for extending MkDocs functionality and providing utilities for managing the documentation. This section details the Python components related to MkDocs customization.</p>"},{"location":"customization/python/#project-structure","title":"Project Structure","text":"<p>The Python code for Obelisk is organized in the <code>/workspaces/obelisk/obelisk</code> directory:</p> <pre><code>/workspaces/obelisk/obelisk/\n\u251c\u2500\u2500 __init__.py          # Package initialization with version info\n\u251c\u2500\u2500 cli.py               # Command-line interface \n\u251c\u2500\u2500 config.py            # Configuration utilities\n\u2514\u2500\u2500 convert.py           # Conversion utilities for Obsidian to MkDocs\n</code></pre>"},{"location":"customization/python/#package-initialization","title":"Package Initialization","text":"<p>The <code>__init__.py</code> file defines basic package information:</p> <pre><code>\"\"\"\nObelisk - Obsidian vault to MkDocs Material Theme site generator.\n\"\"\"\n\n__version__ = \"0.1.0\"\n__author__ = \"Obelisk Team\"\n</code></pre>"},{"location":"customization/python/#command-line-interface","title":"Command-Line Interface","text":"<p>The <code>cli.py</code> module provides a command-line interface for using Obelisk:</p> <pre><code>\"\"\"\nObelisk CLI tool to convert Obsidian vaults to MkDocs sites.\n\"\"\"\n\nimport argparse\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nfrom . import __version__\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Convert Obsidian vault to MkDocs Material Theme site.\"\n    )\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"Obelisk {__version__}\"\n    )\n    parser.add_argument(\n        \"--vault\", \n        type=str, \n        help=\"Path to Obsidian vault directory\",\n        default=\"vault\"\n    )\n    parser.add_argument(\n        \"--output\", \n        type=str, \n        help=\"Output directory for the generated site\",\n        default=\"site\"\n    )\n    parser.add_argument(\n        \"--serve\", \n        action=\"store_true\", \n        help=\"Start a development server after building\"\n    )\n\n    # Command processing logic...\n</code></pre>"},{"location":"customization/python/#configuration-utilities","title":"Configuration Utilities","text":"<p>The <code>config.py</code> module contains utilities for handling MkDocs configuration:</p> <pre><code>\"\"\"\nConfiguration utilities for Obelisk.\n\"\"\"\n\nimport yaml\n\ndef get_default_config():\n    \"\"\"Return the default Obelisk configuration.\"\"\"\n    return {\n        \"site_name\": \"Obelisk\",\n        \"site_description\": \"Obsidian vault to MkDocs Material Theme site generator\",\n        \"theme\": {\n            \"name\": \"material\",\n            \"features\": [\n                \"navigation.instant\",\n                \"navigation.tracking\",\n                \"navigation.tabs\",\n                # Other features...\n            ],\n            \"palette\": [\n                {\n                    \"scheme\": \"default\",\n                    \"primary\": \"deep purple\",\n                    \"accent\": \"deep orange\"\n                },\n                {\n                    \"scheme\": \"slate\",\n                    \"primary\": \"deep purple\",\n                    \"accent\": \"deep orange\"\n                }\n            ]\n        },\n        # Other configuration options...\n    }\n</code></pre>"},{"location":"customization/python/#conversion-utilities","title":"Conversion Utilities","text":"<p>The <code>convert.py</code> module handles conversion from Obsidian to MkDocs format:</p> <pre><code>\"\"\"\nConversion utilities for transforming Obsidian files to MkDocs-compatible format.\n\"\"\"\n\nimport re\nimport shutil\nfrom pathlib import Path\n\ndef process_obsidian_vault(vault_path, output_path=\"vault\"):\n    \"\"\"\n    Process an Obsidian vault directory and prepare it for MkDocs.\n    \"\"\"\n    # Process all markdown files...\n\ndef convert_file(input_path, output_path):\n    \"\"\"\n    Convert an Obsidian markdown file to MkDocs compatible format.\n    \"\"\"\n    # Convert wiki links to markdown links\n    # Convert Obsidian callouts to admonitions\n    # Convert Obsidian comments to HTML comments\n</code></pre>"},{"location":"customization/python/#integration-with-mkdocs","title":"Integration with MkDocs","text":"<p>The Python code integrates with MkDocs through:</p> <ol> <li>Command execution - Running MkDocs commands via <code>subprocess</code></li> <li>Configuration generation - Creating and modifying <code>mkdocs.yml</code></li> <li>Content processing - Transforming Obsidian syntax to MkDocs/Material compatible format</li> </ol>"},{"location":"customization/python/#using-the-python-cli","title":"Using the Python CLI","text":"<p>The CLI is registered as an entry point in <code>pyproject.toml</code>:</p> <pre><code>[project.scripts]\nobelisk = \"obelisk.cli:main\"\n</code></pre> <p>This allows running Obelisk as a command:</p> <pre><code>obelisk --vault /path/to/obsidian/vault --serve\n</code></pre>"},{"location":"customization/python/#extending-with-mkdocs-plugins","title":"Extending with MkDocs Plugins","text":"<p>For more complex customizations, Python can be used to create custom MkDocs plugins. These would be placed in a <code>plugins</code> subdirectory and registered in <code>mkdocs.yml</code>.</p>"},{"location":"customization/versioning/","title":"Documentation Versioning","text":"<p>Obelisk implements documentation versioning using Mike, which allows maintaining multiple versions of the documentation simultaneously.</p>"},{"location":"customization/versioning/#configuration","title":"Configuration","text":"<p>Versioning is configured in the <code>mkdocs.yml</code> file:</p> <pre><code>extra:\n  version:\n    provider: mike\n    default: 0.1.0\n</code></pre>"},{"location":"customization/versioning/#version-files","title":"Version Files","text":"<p>The versioning information is stored in two places:</p> <ol> <li>versions.json - Lists all available versions</li> <li>Site deployment directories - Each version has its own directory</li> </ol> <p>The <code>versions.json</code> file format:</p> <pre><code>[\n  {\n    \"version\": \"0.1.0\",\n    \"title\": \"0.1.0\",\n    \"aliases\": [\n      \"latest\"\n    ]\n  }\n]\n</code></pre>"},{"location":"customization/versioning/#deployment-commands","title":"Deployment Commands","text":"<p>Version deployment is managed through Taskfile commands:</p> <pre><code># From Taskfile.yaml\nversion-deploy:\n  desc: \"Deploy a new version (requires version number and description)\"\n  cmds:\n    - poetry run mike deploy --push --update-aliases {{.CLI_ARGS}}\n\nversion-set-default:\n  desc: \"Set the default version (requires version number)\"\n  cmds:\n    - poetry run mike set-default --push {{.CLI_ARGS}}\n</code></pre> <p>Usage:</p> <pre><code># Deploy new version\ntask version-deploy -- 0.1.0 \"Initial boilerplate template release\"\n\n# Set as default\ntask version-set-default -- 0.1.0\n</code></pre>"},{"location":"customization/versioning/#version-selection-ui","title":"Version Selection UI","text":"<p>The Material theme displays a version selection dropdown when versioning is enabled. The styling for this dropdown is customized in <code>extra.css</code>:</p> <pre><code>/* Version selector styling */\n.md-version {\n  background-color: rgba(0, 0, 0, 0.1);\n  border-radius: 4px;\n  padding: 0 0.5rem;\n}\n</code></pre>"},{"location":"customization/versioning/#outdated-version-notice","title":"Outdated Version Notice","text":"<p>When viewing an outdated version, a notice is displayed at the top of the page. This notice is customized in <code>main.html</code>:</p> <pre><code>{% block outdated %}\n  You're viewing an outdated version of this documentation.\n  &lt;a href=\"{{ '../' ~ base_url }}\"&gt;\n    &lt;strong&gt;Click here to go to the latest version.&lt;/strong&gt;\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/versioning/#version-integration-with-announcement-banner","title":"Version Integration with Announcement Banner","text":"<p>The current version is displayed in the announcement banner:</p> <pre><code>{% block announce %}\n  &lt;a href=\"https://github.com/usrbinkat/obelisk\"&gt;\n    &lt;span class=\"twemoji\"&gt;{% include \".icons/fontawesome/brands/github.svg\" %}&lt;/span&gt;\n    &lt;strong&gt;Obelisk v0.1.0 &lt;/strong&gt;&amp;nbsp;released - Start your Obelisk now\n  &lt;/a&gt;\n{% endblock %}\n</code></pre>"},{"location":"customization/versioning/#managing-multiple-versions","title":"Managing Multiple Versions","text":"<p>To maintain multiple versions:</p> <ol> <li>Make changes to the documentation</li> <li>Deploy a new version with an appropriate version number and description</li> <li>Optionally set the new version as the default</li> <li>Users can switch between versions using the dropdown in the UI</li> </ol>"},{"location":"customization/versioning/#versioning-best-practices","title":"Versioning Best Practices","text":"<ol> <li>Semantic Versioning - Use SemVer format (MAJOR.MINOR.PATCH)</li> <li>Clear Descriptions - Provide meaningful descriptions for each version</li> <li>Default Version - Typically set the latest stable version as default</li> <li>Aliases - Use aliases like \"latest\" or \"stable\" for important versions</li> </ol>"},{"location":"deployment/container-initialization/","title":"Obelisk Container Initialization and Testing Guide","text":"<p>This comprehensive guide details the initialization process for the Obelisk container stack and provides a step-by-step testing procedure to ensure your deployment is functioning correctly. Whether you're a junior developer setting up Obelisk for the first time or a seasoned architect deploying in a production environment, this document will guide you through the entire process.</p> <p>\u26a0\ufe0f IMPORTANT NOTE FOR AI-ASSISTED DEPLOYMENTS \u26a0\ufe0f When using Claude Code or similar AI assistants to help with deployment:</p> <p>Several commands in this initialization workflow exceed the 2-minute execution time limit imposed by AI assistants: - <code>task clean-all-purge</code> (or equivalent container cleanup) - <code>docker-compose build</code> (can take 5+ minutes) - <code>docker-compose pull</code> (can take 5+ minutes) - The Ollama model download step (can take 5-15 minutes depending on models and connection)</p> <p>Recommendation for AI-assisted workflows: 1. Run these long-running commands manually BEFORE asking the AI to assist:    <pre><code># Run these commands manually first\ntask clean-all-purge  # or equivalent cleanup commands\ndocker-compose build\ndocker-compose pull\n\n# Optionally pre-pull models (helpful but not required)\ndocker-compose up -d ollama\ndocker-compose exec ollama ollama pull llama3\ndocker-compose exec ollama ollama pull mxbai-embed-large\n</code></pre> 2. Once these steps complete, you can engage the AI assistant for the remaining initialization and testing steps.</p> <p>This approach prevents timeout issues during AI-assisted deployments and ensures a smoother experience.</p>"},{"location":"deployment/container-initialization/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>System Architecture</li> <li>Prerequisites</li> <li>Initialization Sequence</li> <li>Step-by-Step Setup</li> <li>Validation and Testing</li> <li>Troubleshooting</li> <li>Advanced Configuration</li> <li>Technical Reference</li> </ul>"},{"location":"deployment/container-initialization/#overview","title":"Overview","text":"<p>Obelisk is a system that transforms Obsidian vaults into MkDocs Material Theme sites with AI integration capabilities through Ollama and Open WebUI. The container architecture includes several interdependent services that require a specific initialization sequence to function correctly. This document explains the technical aspects of this process and provides a reliable testing procedure.</p>"},{"location":"deployment/container-initialization/#key-components","title":"Key Components","text":"<p>The Obelisk stack consists of:</p> <ol> <li>Documentation System</li> <li><code>obelisk</code>: MkDocs documentation server</li> <li> <p>Documentation transformation pipeline</p> </li> <li> <p>AI Services</p> </li> <li><code>ollama</code>: Local LLM inference service</li> <li><code>open-webui</code>: Web interface for AI interaction</li> <li> <p><code>litellm</code>: API proxy for model management</p> </li> <li> <p>RAG (Retrieval-Augmented Generation) Pipeline</p> </li> <li><code>obelisk-rag</code>: RAG API service</li> <li>Vector embedding and storage</li> <li> <p>Document processing</p> </li> <li> <p>Vector Database</p> </li> <li><code>milvus</code>: Vector database for embeddings</li> <li><code>etcd</code>: Configuration store for Milvus</li> <li> <p><code>minio</code>: Object storage for Milvus</p> </li> <li> <p>Support Services</p> </li> <li><code>litellm_db</code>: PostgreSQL database for LiteLLM</li> <li><code>tika</code>: Document processing for OpenWebUI</li> <li><code>init-service</code>: Container initialization manager</li> </ol>"},{"location":"deployment/container-initialization/#system-architecture","title":"System Architecture","text":"<p>The system follows a microservices architecture where each component fulfills a specific role. Understanding the interactions between these services is crucial for successful deployment and troubleshooting.</p>"},{"location":"deployment/container-initialization/#service-dependency-graph","title":"Service Dependency Graph","text":"<pre><code>graph TD\n    subgraph \"Infrastructure Layer\"\n        A[etcd] --&gt; D[milvus]\n        B[minio] --&gt; D\n    end\n\n    subgraph \"AI Layer\"\n        D --&gt; E[ollama]\n        F[litellm_db] --&gt; G[litellm]\n        E --&gt; G\n    end\n\n    subgraph \"Initialization\"\n        H[init-service]\n        G --&gt; H\n        E --&gt; H\n        D --&gt; H\n    end\n\n    subgraph \"Application Layer\"\n        H --&gt; I[obelisk-rag]\n        H --&gt; J[open-webui]\n        H --&gt; K[obelisk]\n        G --&gt; I\n        G --&gt; J\n        I --&gt; J\n    end</code></pre>"},{"location":"deployment/container-initialization/#data-flow","title":"Data Flow","text":"<ol> <li>Initialization Flow: </li> <li>Token generation and distribution</li> <li>Model downloading and registration</li> <li> <p>Service configuration</p> </li> <li> <p>Document Processing Flow:</p> </li> <li>Documents stored in vault directory</li> <li>Processed into vector embeddings</li> <li> <p>Stored in Milvus vector database</p> </li> <li> <p>Query Flow:</p> </li> <li>User query received via OpenWebUI or API</li> <li>Query embedded and similar documents retrieved</li> <li>LLM generates response with document context</li> <li>Response returned to user</li> </ol>"},{"location":"deployment/container-initialization/#prerequisites","title":"Prerequisites","text":"<p>Before starting the initialization process, ensure you have:</p> <ul> <li>Docker Engine (version 20.10.0+)</li> <li>Docker Compose (version 2.0.0+)</li> <li>Git (for repository access)</li> <li>16GB RAM minimum (32GB recommended)</li> <li>50GB available disk space</li> <li>NVIDIA GPU with CUDA support (optional but recommended)</li> </ul>"},{"location":"deployment/container-initialization/#nvidia-setup-optional","title":"NVIDIA Setup (Optional)","text":"<p>If using GPU acceleration:</p> <ol> <li> <p>Install the NVIDIA Container Toolkit:    <pre><code># For Ubuntu\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt update &amp;&amp; sudo apt install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre></p> </li> <li> <p>Verify GPU availability:    <pre><code>docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n</code></pre></p> </li> </ol>"},{"location":"deployment/container-initialization/#initialization-sequence","title":"Initialization Sequence","text":"<p>The initialization process follows a specific sequence to ensure all dependencies are properly established. This section explains the technical reasons behind this sequence.</p>"},{"location":"deployment/container-initialization/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant I as Infrastructure\n    participant AI as AI Services\n    participant Init as Initialization\n    participant App as Applications\n\n    I-&gt;&gt;I: Start etcd &amp; minio\n    I-&gt;&gt;I: Start milvus (depends on etcd &amp; minio)\n    I-&gt;&gt;AI: Start ollama\n    I-&gt;&gt;AI: Start litellm_db\n    AI-&gt;&gt;AI: Start litellm (depends on litellm_db)\n    AI-&gt;&gt;Init: Run init-service\n    Init-&gt;&gt;Init: Generate authentication tokens\n    Init-&gt;&gt;Init: Register tokens with LiteLLM API\n    Init-&gt;&gt;Init: Download and pull required models\n    Init-&gt;&gt;Init: Register models with LiteLLM API\n    Init-&gt;&gt;Init: Configure all services\n    Init-&gt;&gt;Init: Verify initialization success\n    Init-&gt;&gt;App: Start obelisk-rag (depends on successful init)\n    Init-&gt;&gt;App: Start open-webui (depends on successful init)\n    Init-&gt;&gt;App: Start obelisk (depends on successful init)</code></pre>"},{"location":"deployment/container-initialization/#initialization-steps-explained","title":"Initialization Steps Explained","text":"<ol> <li>Infrastructure Setup (etcd, minio, milvus)</li> <li>etcd: Provides configuration store for Milvus</li> <li>minio: Provides object storage for Milvus</li> <li>milvus: Provides vector database capabilities</li> </ol> <p>These services must start first as they constitute the fundamental data infrastructure.</p> <ol> <li>AI Service Setup (ollama, litellm_db, litellm)</li> <li>ollama: Provides model serving capabilities</li> <li>litellm_db: Stores API keys and configuration</li> <li>litellm: Provides API proxy for models</li> </ol> <p>These services depend on the infrastructure but must be available before initialization.</p> <ol> <li>Initialization (init-service)</li> <li>Handles token generation and registration</li> <li>Downloads required models</li> <li>Configures services with appropriate settings</li> <li>Verifies successful initialization</li> </ol> <p>This service orchestrates the preparation of the environment.</p> <ol> <li>Application Services (obelisk-rag, open-webui, obelisk)</li> <li>obelisk-rag: Provides RAG capabilities</li> <li>open-webui: Provides user interface</li> <li>obelisk: Serves documentation</li> </ol> <p>These services depend on successful initialization.</p>"},{"location":"deployment/container-initialization/#step-by-step-setup","title":"Step-by-Step Setup","text":"<p>This section provides a detailed walkthrough of the initialization process, from repository setup to running all services.</p>"},{"location":"deployment/container-initialization/#1-environment-setup","title":"1. Environment Setup","text":"<p>Ensure you are in the Obelisk project directory before proceeding with the deployment steps.</p>"},{"location":"deployment/container-initialization/#2-clean-environment","title":"2. Clean Environment","text":"<p>Before starting, ensure a clean environment:</p> <pre><code># Clean all containers, volumes, and cached data\ntask clean-all-purge\n\n# Alternatively, if task is not installed\ndocker-compose down -v --remove-orphans\ndocker-compose rm -fsv\ndocker-compose down --rmi all\n</code></pre>"},{"location":"deployment/container-initialization/#3-build-containers","title":"3. Build Containers","text":"<p>Build all service containers:</p> <pre><code>docker-compose build\n</code></pre> <p>This step compiles the container images for all services defined in the docker-compose.yaml file. It may take several minutes depending on your internet connection and machine performance.</p>"},{"location":"deployment/container-initialization/#4-start-infrastructure-services","title":"4. Start Infrastructure Services","text":"<p>Start the foundational infrastructure services in order:</p> <pre><code># Start etcd, minio, and milvus\ndocker-compose up -d etcd minio milvus\n\n# Verify services are running\ndocker-compose ps etcd minio milvus\n</code></pre> <p>Key technical points: - etcd must be fully operational before milvus can initialize - minio provides the object storage layer for milvus - milvus creates collections automatically upon first startup</p>"},{"location":"deployment/container-initialization/#5-start-ollama-service","title":"5. Start Ollama Service","text":"<pre><code># Start Ollama\ndocker-compose up -d ollama\n\n# Verify Ollama is running\ndocker-compose ps ollama\n</code></pre> <p>Ollama provides the foundation for LLM serving. It must be running before LiteLLM can register models.</p>"},{"location":"deployment/container-initialization/#6-start-litellm-database-and-api","title":"6. Start LiteLLM Database and API","text":"<pre><code># Start LiteLLM database and API service\ndocker-compose up -d litellm_db litellm\n\n# Verify services are running\ndocker-compose ps litellm_db litellm\n</code></pre> <p>Technical notes: - LiteLLM requires a PostgreSQL database for API key storage and tracking - The service initializes tables on first startup - It provides an OpenAI-compatible API for model access</p>"},{"location":"deployment/container-initialization/#7-run-the-initialization-service","title":"7. Run the Initialization Service","text":"<pre><code># Run the initialization service in foreground mode to observe the initialization process\n# IMPORTANT: This command will run and exit when complete - don't interrupt it\ndocker-compose up init-service\n</code></pre> <p>This is a critical step where: 1. Authentication tokens are generated with cryptographic security 2. Tokens are registered with the LiteLLM API using the master key 3. Required models are downloaded with retry capability:    - mxbai-embed-large: For vector embeddings (~2GB)    - llama3: For text generation (~4GB) 4. Models are registered with LiteLLM for proxy access 5. Configuration files are created for all services 6. Verification confirms successful initialization</p> <p>Wait for this service to complete before proceeding. The process takes approximately 5-10 minutes on first run, as it downloads models.</p>"},{"location":"deployment/container-initialization/#verifying-init-service-success","title":"Verifying init-service Success","text":"<p>Before proceeding to the next step, verify that the initialization service completed successfully:</p> <pre><code># Check the init-service logs - look for \"Initialization complete\" at the end\ndocker-compose logs init-service | tail -n 20\n\n# Verify token file was created\ndocker-compose exec litellm test -f /app/tokens/api_tokens.env &amp;&amp; echo \"Token file exists\" || echo \"Token file missing\"\n\n# Check if Ollama models were successfully downloaded\ndocker-compose exec ollama ollama list\n</code></pre> <p>You should see both <code>llama3</code> and <code>mxbai-embed-large</code> models in the Ollama list output. If any of these checks fail, review the full init-service logs for errors:</p> <pre><code>docker-compose logs init-service\n</code></pre> <p>Common causes of initialization failure include: - Network connectivity issues during model downloads - LiteLLM API not ready when tokens are being registered - Insufficient disk space for model downloads - Token registration issues with LiteLLM</p> <p>If you see <code>Error: LiteLLM API authentication failed</code> in the logs, you may need to manually register the token:</p> <pre><code># Get the generated token\nLITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2 | tr -d ' \\t\\n\\r')\n\n# Register it with LiteLLM\ncurl -s -X POST \"http://localhost:4000/key/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_MASTER_KEY:-sk-1234}\" \\\n  -d \"{\\\"key\\\": \\\"$LITELLM_TOKEN\\\", \\\"metadata\\\": {\\\"description\\\": \\\"Manual registration\\\"}}\"\n</code></pre>"},{"location":"deployment/container-initialization/#8-start-application-services","title":"8. Start Application Services","text":"<p>After initialization completes successfully:</p> <pre><code># Start application services\ndocker-compose up -d obelisk-rag open-webui obelisk\n\n# Verify all services are running\ndocker-compose ps\n</code></pre> <p>This starts the user-facing services that depend on successful initialization: - obelisk-rag: Provides the RAG API at http://localhost:8001/ - open-webui: Provides the chat interface at http://localhost:8080/ - obelisk: Provides the documentation at http://localhost:8000/</p>"},{"location":"deployment/container-initialization/#validation-and-testing","title":"Validation and Testing","text":"<p>After completing the setup, it's crucial to verify that all components are functioning correctly. This section provides a comprehensive testing procedure.</p>"},{"location":"deployment/container-initialization/#automated-validation","title":"Automated Validation","text":"<p>Obelisk includes an automated validation script that checks initialization status:</p> <pre><code># Run the validation script\ncd /workspaces/obelisk &amp;&amp; poetry run python hack/test_init.py\n</code></pre> <p>This script performs the following checks: - Service health verification - Token retrieval and authentication testing - Model availability verification - LiteLLM generation capability testing - OpenWebUI accessibility testing</p> <p>The script will output a detailed report of each test performed and its result.</p>"},{"location":"deployment/container-initialization/#manual-component-testing","title":"Manual Component Testing","text":"<p>For thorough validation, perform these manual checks:</p>"},{"location":"deployment/container-initialization/#1-verify-litellm-api-with-token-authentication","title":"1. Verify LiteLLM API with Token Authentication","text":"<pre><code># Retrieve the token\nLITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2)\n\n# Test the API\ncurl -s -H \"Authorization: Bearer ${LITELLM_TOKEN}\" http://localhost:4000/models | jq\n</code></pre> <p>Expected output: A JSON list of available models including <code>llama3</code> and <code>mxbai-embed-large</code>.</p>"},{"location":"deployment/container-initialization/#2-test-litellm-generation","title":"2. Test LiteLLM Generation","text":"<pre><code># Use the token to test generation\ncurl -X POST http://localhost:4000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_TOKEN}\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is Obelisk?\"}],\n    \"max_tokens\": 100\n  }'\n</code></pre> <p>Expected output: A JSON response containing generated text about Obelisk.</p>"},{"location":"deployment/container-initialization/#3-test-obelisk-rag-api","title":"3. Test Obelisk-RAG API","text":"<pre><code># Test the RAG API with the token\ncurl -X POST http://localhost:8001/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_TOKEN}\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is Obelisk?\"}]\n  }'\n</code></pre> <p>Expected output: A JSON response containing a generated answer, potentially with source references if documents are available.</p>"},{"location":"deployment/container-initialization/#4-test-web-interfaces","title":"4. Test Web Interfaces","text":"<p>Verify all web interfaces are accessible:</p> <pre><code># Test Obelisk MkDocs\ncurl -s -o /dev/null -w '%{http_code}\\n' http://localhost:8000/\n\n# Test OpenWebUI\ncurl -s -o /dev/null -w '%{http_code}\\n' http://localhost:8080/\n\n# Test Obelisk-RAG stats endpoint\ncurl -s -o /dev/null -w '%{http_code}\\n' http://localhost:8001/stats\n</code></pre> <p>All should return 200 (or 302 for redirects), indicating successful access.</p>"},{"location":"deployment/container-initialization/#5-browser-interface-testing","title":"5. Browser Interface Testing","text":"<p>For a complete end-to-end test, access these URLs in your browser:</p> <ul> <li>Documentation: http://localhost:8000/</li> <li>Chat Interface: http://localhost:8080/</li> <li>RAG Stats: http://localhost:8001/stats</li> </ul> <p>In the OpenWebUI interface: 1. Navigate to Settings &gt; LiteLLM 2. Verify the connection is working with the auto-generated token 3. Start a new chat and ask a question about Obelisk 4. Verify that responses are generated correctly</p>"},{"location":"deployment/container-initialization/#document-indexing-test","title":"Document Indexing Test","text":"<p>To test document indexing for RAG functionality:</p> <pre><code># Create a test document\nmkdir -p test-docs\necho -e \"# Test Document\\n\\nThis is a unique test document for RAG testing.\" &gt; test-docs/test.md\n\n# Copy to the RAG vault\ndocker cp test-docs/test.md obelisk-rag:/app/vault/\n\n# Wait for processing (about 10 seconds)\nsleep 10\n\n# Query for the content\ncurl -X POST http://localhost:8001/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_TOKEN}\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is in the test document?\"}]\n  }'\n</code></pre> <p>If functioning correctly, the response should reference content from the test document.</p>"},{"location":"deployment/container-initialization/#troubleshooting","title":"Troubleshooting","text":"<p>This section addresses common issues that may occur during the initialization process.</p>"},{"location":"deployment/container-initialization/#initialization-failures","title":"Initialization Failures","text":"<p>If the init-service fails, examine the logs for specific errors:</p> <pre><code>docker-compose logs init-service\n</code></pre> <p>Common initialization issues and solutions:</p>"},{"location":"deployment/container-initialization/#1-token-registration-failures","title":"1. Token Registration Failures","text":"<p>Symptoms: - Error message: <code>Token registration failed</code> - Authentication errors in LiteLLM logs</p> <p>Solutions: - Verify LiteLLM API is accessible: <code>curl -s http://localhost:4000/health</code> - Check master key configuration in docker-compose.yaml - Manually register token:   <pre><code>LITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2)\n\ncurl -s -X POST \"http://localhost:4000/key/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_MASTER_KEY:-sk-1234}\" \\\n  -d \"{\\\"key\\\": \\\"$LITELLM_TOKEN\\\", \\\"metadata\\\": {\\\"description\\\": \\\"Manual registration\\\"}}\"\n</code></pre></p>"},{"location":"deployment/container-initialization/#2-model-download-failures","title":"2. Model Download Failures","text":"<p>Symptoms: - Error message: <code>Failed to pull model 'xxx' after xx retries</code> - Missing models in <code>docker-compose exec ollama ollama list</code></p> <p>Solutions: - Check Ollama logs: <code>docker-compose logs ollama</code> - Verify network connectivity within containers - Manually pull models:   <pre><code>docker-compose exec ollama ollama pull mxbai-embed-large\ndocker-compose exec ollama ollama pull llama3\n</code></pre> - Check if model name has changed in recent Ollama releases</p>"},{"location":"deployment/container-initialization/#3-milvus-connection-issues","title":"3. Milvus Connection Issues","text":"<p>Symptoms: - Error connecting to Milvus in initialization logs - Milvus startup warnings</p> <p>Solutions: - Check Milvus logs: <code>docker-compose logs milvus</code> - Verify etcd and minio are running: <code>docker-compose ps etcd minio</code> - Restart Milvus: <code>docker-compose restart milvus</code> - Check network configuration between containers</p>"},{"location":"deployment/container-initialization/#service-specific-issues","title":"Service-Specific Issues","text":""},{"location":"deployment/container-initialization/#ollama-issues","title":"Ollama Issues","text":"<p>Symptom: Ollama fails to start or respond</p> <p>Solutions: - Check for GPU conflicts: <code>docker-compose logs ollama | grep -i cuda</code> - For non-GPU environments, edit docker-compose.yaml to remove GPU configuration - Check resource allocation and memory availability - Verify NVIDIA drivers are installed (for GPU usage)</p>"},{"location":"deployment/container-initialization/#litellm-issues","title":"LiteLLM Issues","text":"<p>Symptom: Authentication errors or model not found errors</p> <p>Solutions: - Check database connectivity: <code>docker-compose logs litellm_db</code> - Verify token registration: <code>curl -s -H \"Authorization: Bearer ${LITELLM_MASTER_KEY:-sk-1234}\" http://localhost:4000/key/info</code> - Check model registration: <code>curl -s -H \"Authorization: Bearer ${LITELLM_TOKEN}\" http://localhost:4000/models</code></p>"},{"location":"deployment/container-initialization/#rag-api-issues","title":"RAG API Issues","text":"<p>Symptom: No documents found, empty responses</p> <p>Solutions: - Check vector database connection: <code>docker-compose logs obelisk-rag | grep -i milvus</code> - Verify document processing: <code>docker-compose exec obelisk-rag ls -la /app/vault/</code> - Check embeddings: <code>curl -s http://localhost:8001/stats</code></p>"},{"location":"deployment/container-initialization/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"deployment/container-initialization/#environment-variable-reference","title":"Environment Variable Reference","text":"<p>The system can be customized through environment variables in docker-compose.yaml:</p> Variable Service Description Default <code>LITELLM_MASTER_KEY</code> LiteLLM Master key for token registration sk-1234 <code>LITELLM_API_TOKEN</code> Generated Token for service authentication (auto-generated) <code>OLLAMA_MODEL</code> Ollama Default model for generation llama3 <code>EMBEDDING_MODEL</code> Obelisk-RAG Model for vector embeddings mxbai-embed-large <code>MILVUS_HOST</code> Obelisk-RAG Milvus host milvus <code>MILVUS_PORT</code> Obelisk-RAG Milvus port 19530 <code>CHUNK_SIZE</code> Obelisk-RAG Document chunking size 2500 <code>CHUNK_OVERLAP</code> Obelisk-RAG Overlap between chunks 500 <code>RETRIEVE_TOP_K</code> Obelisk-RAG Number of chunks to retrieve 5 <code>VECTOR_DB</code> Obelisk-RAG Vector database type milvus"},{"location":"deployment/container-initialization/#custom-model-configuration","title":"Custom Model Configuration","text":"<p>To use different models:</p> <ol> <li> <p>Edit the <code>docker-compose.yaml</code> file to update environment variables:    <pre><code>obelisk-rag:\n  environment:\n    - OLLAMA_MODEL=mistral  # Change to desired model\n    - EMBEDDING_MODEL=nomic-embed-text  # Change to desired embedding model\n</code></pre></p> </li> <li> <p>Pull the model before starting services:    <pre><code>docker-compose up -d ollama\ndocker-compose exec ollama ollama pull mistral\ndocker-compose exec ollama ollama pull nomic-embed-text\n</code></pre></p> </li> <li> <p>Update the LiteLLM configuration:    <pre><code>docker-compose exec litellm bash -c \"cat &gt; /app/config/litellm_config.yaml &lt;&lt; EOF\nmodel_list:\n  - model_name: mistral\n    litellm_params:\n      model: ollama/mistral\n      api_base: http://ollama:11434\n  - model_name: nomic-embed-text\n    litellm_params:\n      model: ollama/nomic-embed-text\n      api_base: http://ollama:11434\n\n# Authentication\napi_key: ${LITELLM_API_TOKEN}\nEOF\"\n</code></pre></p> </li> </ol>"},{"location":"deployment/container-initialization/#persistent-document-storage","title":"Persistent Document Storage","text":"<p>For persistent document storage:</p> <ol> <li> <p>Mount a local directory to the RAG vault:    <pre><code>obelisk-rag:\n  volumes:\n    - ./your-local-docs:/app/vault:ro  # Read-only mount\n</code></pre></p> </li> <li> <p>Restart the service:    <pre><code>docker-compose restart obelisk-rag\n</code></pre></p> </li> </ol>"},{"location":"deployment/container-initialization/#custom-prompt-templates","title":"Custom Prompt Templates","text":"<p>Customize the RAG prompt template:</p> <ol> <li> <p>Create a custom template file:    <pre><code>cat &gt; rag_template.txt &lt;&lt; EOF\nAnswer the following question based on the provided documents.\nIf you don't know the answer, say so clearly.\n\nDocuments:\n{{context}}\n\nQuestion: {{query}}\n\nAnswer:\nEOF\n</code></pre></p> </li> <li> <p>Mount it to the container:    <pre><code>obelisk-rag:\n  volumes:\n    - ./rag_template.txt:/app/rag_template.txt\n  environment:\n    - RAG_TEMPLATE_PATH=/app/rag_template.txt\n</code></pre></p> </li> </ol>"},{"location":"deployment/container-initialization/#technical-reference","title":"Technical Reference","text":"<p>This section provides technical details about the internals of the initialization process.</p>"},{"location":"deployment/container-initialization/#initialization-scripts","title":"Initialization Scripts","text":"<p>The initialization process is managed by scripts in the <code>docker/init-scripts/</code> directory:</p> Script Purpose Key Technical Functions init-sequence.sh Master controller Orchestrates initialization sequence, performs verification generate-tokens.sh Token management Creates secure cryptographic tokens, registers with APIs download-models.sh Model management Checks model availability, pulls models, handles registration configure-services.sh Service configuration Creates configuration files for each service"},{"location":"deployment/container-initialization/#docker-container-specifications","title":"Docker Container Specifications","text":"Container Base Image Key Dependencies Function init-service python:3.12-slim curl, bash Performs initialization ollama ollama/ollama:latest CUDA (optional) Serves LLMs litellm ghcr.io/berriai/litellm:main-latest PostgreSQL API proxy milvus milvusdb/milvus:v2.5.10 etcd, minio Vector database obelisk-rag python:3.12-slim langchain, fastapi RAG service"},{"location":"deployment/container-initialization/#token-generation-mechanism","title":"Token Generation Mechanism","text":"<p>The token generation process uses OpenSSL to create cryptographically secure tokens:</p> <ul> <li>LiteLLM API tokens: 12 bytes of hexadecimal with <code>sk-</code> prefix</li> <li>OpenWebUI tokens: 32 bytes in base64 encoding</li> </ul> <p>These tokens are stored in a shared volume at <code>/app/tokens/api_tokens.env</code> and are accessible to all services that require authentication.</p>"},{"location":"deployment/container-initialization/#verification-process","title":"Verification Process","text":"<p>The initialization performs an exhaustive verification:</p> <ol> <li>Token file existence and format validation</li> <li>Ollama API accessibility and response validation</li> <li>LiteLLM token authentication verification</li> <li>Model download confirmation</li> <li>Model registration confirmation with LiteLLM API</li> </ol> <p>This multi-step verification ensures the system is fully operational before allowing application services to start.</p>"},{"location":"deployment/container-initialization/#networking-architecture","title":"Networking Architecture","text":"<p>The services communicate through a Docker bridge network:</p> <pre><code>networks:\n  ollama-net:\n    driver: bridge\n</code></pre> <p>Service discovery is handled through Docker's DNS resolution, allowing services to reference each other by container name (e.g., <code>http://ollama:11434</code>).</p>"},{"location":"deployment/container-initialization/#data-volumes","title":"Data Volumes","text":"<p>The system uses these persistent volumes:</p> Volume Purpose Services Using ollama Model storage ollama models Shared models ollama, open-webui milvus_data Vector database milvus rag-data RAG database obelisk-rag rag-vault Document storage obelisk-rag tokens Authentication tokens multiple config Service configurations multiple"},{"location":"deployment/container-initialization/#conclusion","title":"Conclusion","text":"<p>This comprehensive guide has walked you through the technical details of the Obelisk container initialization and testing process. By following this procedure, you can ensure a reliable and fully functional deployment of the Obelisk stack.</p> <p>The initialization process has been carefully designed to handle dependencies, ensure proper configuration, and validate the functionality of all components. By understanding the purpose and inner workings of each step, you can effectively troubleshoot issues and customize the system to your specific needs.</p> <p>For additional help or to report issues, please refer to the GitHub repository or submit a detailed bug report with logs and system information.</p>"},{"location":"deployment/testing-openai-integration/","title":"Testing OpenAI Integration in Obelisk","text":"<p>This guide provides a comprehensive testing methodology for validating the OpenAI integration feature in Obelisk. This feature allows the system to use OpenAI models when an API key is available, while gracefully falling back to local Ollama models when it's not.</p> <p>\u26a0\ufe0f IMPORTANT NOTE FOR AI-ASSISTED TESTING \u26a0\ufe0f When using Claude Code or similar AI assistants for testing procedures:</p> <p>Several commands in this testing workflow exceed the 2-minute execution time limit imposed by AI assistants: - <code>task clean-all-purge</code> (or alternative cleanup commands) - <code>docker-compose build</code> - <code>docker-compose pull</code> - Ollama model pulling (can take 5-10 minutes per model)</p> <p>Recommendation: Run these commands manually BEFORE asking the AI to assist with testing: <pre><code># Run these commands manually first\ntask clean-all-purge  # or alternative cleanup commands\ndocker-compose build\ndocker-compose pull\n\n# For quick model pull (optional but helpful)\ndocker-compose up -d ollama\ndocker-compose exec ollama ollama pull llama3\ndocker-compose exec ollama ollama pull mxbai-embed-large\n</code></pre></p> <p>After these commands complete, you can engage the AI assistant for the remaining testing steps.</p>"},{"location":"deployment/testing-openai-integration/#architecture-and-integration-points","title":"Architecture and Integration Points","text":"<p>The OpenAI integration allows Obelisk to use either OpenAI models or local Ollama models, dynamically selecting the appropriate option based on configuration and API key availability.</p>"},{"location":"deployment/testing-openai-integration/#key-components","title":"Key Components","text":"<ol> <li>LiteLLM Proxy: Routes requests to either OpenAI or Ollama based on:</li> <li>Environment variables (OPENAI_API_KEY, USE_OPENAI)</li> <li>Availability of models</li> <li> <p>Fallback configuration</p> </li> <li> <p>OpenWebUI: Provides user interface for interacting with models through either:</p> </li> <li>Direct connection to LiteLLM proxy</li> <li> <p>Connection to Ollama</p> </li> <li> <p>Initialization Scripts: Configure the system during startup:</p> </li> <li>Token generation and distribution</li> <li>Model configuration and registration</li> <li>Service configuration</li> </ol>"},{"location":"deployment/testing-openai-integration/#testing-prerequisites","title":"Testing Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>Git clone of the Obelisk repository</li> <li>At least 16GB RAM and 50GB disk space</li> <li>Internet connection for downloading models</li> <li>(Optional) Valid OpenAI API key for full testing</li> </ul>"},{"location":"deployment/testing-openai-integration/#verifying-container-versions","title":"Verifying Container Versions","text":"<p>Before testing, you can verify you're using the recommended container versions:</p> <pre><code># Run the container version checker script\ncd /workspaces/obelisk\nchmod +x hack/check_container_versions.sh\n./hack/check_container_versions.sh\n</code></pre> <p>This script checks the latest stable versions of all container images used in Obelisk, including: - PostgreSQL - MinIO - Ollama - Milvus - Apache Tika - LiteLLM - etcd - Open WebUI</p>"},{"location":"deployment/testing-openai-integration/#testing-methodology","title":"Testing Methodology","text":"<p>This methodology focuses on validating both functionality and fallback behavior, ensuring the system works correctly in all scenarios.</p>"},{"location":"deployment/testing-openai-integration/#1-environment-preparation","title":"1. Environment Preparation","text":"<p>There are two approaches for preparing the test environment: manual step-by-step or using the automated preparation script.</p>"},{"location":"deployment/testing-openai-integration/#option-a-using-the-automated-preparation-script","title":"Option A: Using the Automated Preparation Script","text":"<p>The repository includes a script to automate the preparation steps that would otherwise time out in AI assistants:</p> <pre><code># Run the test preparation task (will clean, build, pull images, and download models)\ntask test-prep-openai\n\n# (Optional) To test with OpenAI API key\nexport OPENAI_API_KEY=\"your-openai-api-key\" \nexport USE_OPENAI=true\n\n# (Optional) To test fallback behavior\n# export USE_OPENAI=true\n# Do not set OPENAI_API_KEY\n</code></pre> <p>This automated script handles: - Cleaning the environment via <code>task clean-all-purge</code> - Building all containers with <code>docker-compose build</code> - Pulling required container images with <code>docker-compose pull</code> - Starting Ollama and downloading required models</p>"},{"location":"deployment/testing-openai-integration/#option-b-manual-step-by-step-preparation","title":"Option B: Manual Step-by-Step Preparation","text":"<p>If you prefer to perform steps individually:</p> <pre><code># Clean any existing environment\ntask clean-all-purge  # or use alternative docker-compose commands\n\n# Build the containers\ndocker-compose build\n\n# Pull required images\ndocker-compose pull\n\n# Start ollama to download models\ndocker-compose up -d ollama\ndocker-compose exec ollama ollama pull llama3\ndocker-compose exec ollama ollama pull mxbai-embed-large\n\n# (Optional) To test with OpenAI API key\nexport OPENAI_API_KEY=\"your-openai-api-key\"\nexport USE_OPENAI=true\n\n# (Optional) To test fallback behavior\n# export USE_OPENAI=true\n# Do not set OPENAI_API_KEY\n</code></pre>"},{"location":"deployment/testing-openai-integration/#2-running-tests","title":"2. Running Tests","text":""},{"location":"deployment/testing-openai-integration/#starting-services-and-initialization","title":"Starting Services and Initialization","text":"<p>After preparing the environment, you need to start the services in the correct order and run the initialization process:</p> <pre><code># Start infrastructure services\ndocker-compose up -d etcd minio milvus\n\n# Wait for milvus to be fully initialized\nsleep 10\n\n# Start AI services\ndocker-compose up -d litellm_db litellm\n\n# Wait for litellm to be ready\nsleep 5\n\n# Run initialization service (this will run and exit when complete)\ndocker-compose up init-service\n\n# Verify initialization was successful\ndocker-compose logs init-service | grep \"Initialization complete\"\ndocker-compose exec ollama ollama list\n</code></pre> <p>Make sure the initialization was successful before proceeding. You should see \"Initialization complete\" in the logs and both <code>llama3</code> and <code>mxbai-embed-large</code> models in the Ollama list.</p>"},{"location":"deployment/testing-openai-integration/#running-the-test-script","title":"Running the Test Script","text":"<p>After successful initialization, run Obelisk's automated tests:</p> <pre><code># Run the comprehensive OpenAI integration test\n./hack/test_openai_integration.sh\n</code></pre> <p>This script automates the testing workflow: 1. Validates system initialization 2. Tests OpenAI API directly (if key available) 3. Retrieves LiteLLM token from container 4. Tests LiteLLM API with authentication 5. Tests generation capabilities with appropriate model selection 6. Tests OpenWebUI accessibility and API functionality 7. Provides a comprehensive test summary</p> <p>You can also run individual tests:</p> <pre><code># IMPORTANT: Always use 'poetry run' to execute Python scripts to ensure all dependencies are available\n# Direct execution with 'python' will likely fail with \"ModuleNotFoundError: No module named 'requests'\" or other missing dependencies\n\n# Test initialization\npoetry run python hack/test_init.py\n\n# Test LiteLLM integration with OpenAI\nLITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2 | tr -d ' \\t\\n\\r')\nOPENAI_API_KEY=\"$OPENAI_API_KEY\" \\\nLITELLM_API_TOKEN=\"$LITELLM_TOKEN\" \\\nLITELLM_API_URL=\"http://localhost:4000\" \\\npoetry run python hack/test_litellm_openai.py\n\n# Run end-to-end testing\nOPENAI_API_KEY=\"$OPENAI_API_KEY\" \\\nLITELLM_API_TOKEN=\"$LITELLM_TOKEN\" \\\nLITELLM_API_URL=\"http://localhost:4000\" \\\nOPENWEBUI_API_URL=\"http://localhost:8080\" \\\npoetry run python hack/test_e2e_openai.py\n</code></pre>"},{"location":"deployment/testing-openai-integration/#3-manual-verification","title":"3. Manual Verification","text":"<p>After automated testing, perform these manual validations:</p>"},{"location":"deployment/testing-openai-integration/#check-litellm-api-and-models","title":"Check LiteLLM API and Models","text":"<pre><code># Get LiteLLM token\nLITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2 | tr -d ' \\t\\n\\r')\n\n# List available models\ncurl -s -H \"Authorization: Bearer ${LITELLM_TOKEN}\" http://localhost:4000/models | jq\n\n# Test completion with appropriate model\nMODEL=\"gpt-4o\"  # If OpenAI API key available\n# MODEL=\"llama3\"  # If no OpenAI API key\n\ncurl -s -X POST http://localhost:4000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${LITELLM_TOKEN}\" \\\n  -d '{\n    \"model\": \"'\"$MODEL\"'\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one word\"}],\n    \"max_tokens\": 10\n  }' | jq\n</code></pre>"},{"location":"deployment/testing-openai-integration/#test-openwebui-integration","title":"Test OpenWebUI Integration","text":"<ul> <li>Access OpenWebUI in browser: http://localhost:8080/</li> <li>Navigate to Settings &gt; LiteLLM/OpenAI API Settings</li> <li>Verify the API URL is set to http://litellm:4000</li> <li>Start a new chat and test responses</li> <li>Ensure model selection includes appropriate models based on your configuration</li> </ul>"},{"location":"deployment/testing-openai-integration/#verify-fallback-behavior","title":"Verify Fallback Behavior","text":"<p>If you have an OpenAI API key, test both with and without the key:</p> <pre><code># Test with OpenAI API key\nexport OPENAI_API_KEY=\"your-key\"\nexport USE_OPENAI=true\n./hack/test_openai_integration.sh\n\n# Test fallback behavior\nunset OPENAI_API_KEY\nexport USE_OPENAI=true\n./hack/test_openai_integration.sh\n</code></pre>"},{"location":"deployment/testing-openai-integration/#key-testing-focus-areas","title":"Key Testing Focus Areas","text":"<p>When testing the OpenAI integration, pay particular attention to these areas:</p>"},{"location":"deployment/testing-openai-integration/#1-model-registration-and-availability","title":"1. Model Registration and Availability","text":"<ul> <li>With OpenAI API Key:</li> <li>Both OpenAI models (gpt-4o) and Ollama models (llama3) should be available</li> <li> <p>Model aliases should work correctly (e.g., both \"gpt-4o\" and \"openai/gpt-4o\")</p> </li> <li> <p>Without OpenAI API Key:</p> </li> <li>Only Ollama models should be available</li> <li>System should not show errors related to missing OpenAI models</li> </ul>"},{"location":"deployment/testing-openai-integration/#2-authentication-and-tokens","title":"2. Authentication and Tokens","text":"<ul> <li>Token format should be clean (no whitespace, newlines, or \"Bearer \" prefix)</li> <li>LiteLLM should accept the token and provide access to models</li> <li>Tokens should be properly shared between services</li> </ul>"},{"location":"deployment/testing-openai-integration/#3-api-functionality","title":"3. API Functionality","text":"<ul> <li>LiteLLM API should respond correctly to requests</li> <li>OpenWebUI should connect to LiteLLM API successfully</li> <li>Chat completions should work with authentication</li> </ul>"},{"location":"deployment/testing-openai-integration/#4-fallback-mechanism","title":"4. Fallback Mechanism","text":"<ul> <li>With OpenAI API key: OpenAI models should be used</li> <li>Without OpenAI API key: System should fallback to Ollama models</li> <li>No errors should occur during fallback</li> </ul>"},{"location":"deployment/testing-openai-integration/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"deployment/testing-openai-integration/#authentication-problems","title":"Authentication Problems","text":"<ul> <li>Symptom: \"Unauthorized\" or 401 errors</li> <li>Check:   <pre><code># Verify token format\nLITELLM_TOKEN=$(docker-compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2 | tr -d ' \\t\\n\\r')\necho \"Token: $LITELLM_TOKEN\"\n</code></pre></li> <li>Solution: If token has whitespace or \"Bearer \" prefix, clean it before using</li> </ul>"},{"location":"deployment/testing-openai-integration/#model-registration-issues","title":"Model Registration Issues","text":"<ul> <li>Symptom: OpenAI models not appearing in model list</li> <li>Check:   <pre><code># Check LiteLLM logs for model registration\ndocker-compose logs litellm | grep -i \"model registration\"\ndocker-compose exec litellm cat /app/config/litellm_config.yaml\n</code></pre></li> <li>Solution: Verify OpenAI API key is passed correctly in environment variables</li> </ul>"},{"location":"deployment/testing-openai-integration/#openwebui-api-authentication","title":"OpenWebUI API Authentication","text":"<ul> <li>Note: OpenWebUI's chat completions API requiring authentication (403/401) is normal</li> <li>Solution: Access through the web interface at http://localhost:8080</li> </ul>"},{"location":"deployment/testing-openai-integration/#initialization-failures","title":"Initialization Failures","text":"<ul> <li>Check:    <pre><code>docker-compose logs init-service\n</code></pre></li> <li>Solution: Look for specific errors in token generation, model pulling, or configuration</li> </ul>"},{"location":"deployment/testing-openai-integration/#conclusion","title":"Conclusion","text":"<p>By following this testing methodology, you can thoroughly validate the OpenAI integration in Obelisk. Remember to test both with and without an OpenAI API key to ensure the fallback mechanism works correctly.</p> <p>For additional help or to report issues, please refer to the main project documentation or submit a detailed issue report to the project repository.</p>"},{"location":"development/","title":"Development Configuration","text":"<p>This section documents the development environment configuration files and tools used in the Obelisk project.</p>"},{"location":"development/#project-structure","title":"Project Structure","text":"<p>The root level of the Obelisk project contains several configuration files and directories that control development, building, and deployment:</p> <pre><code>/workspaces/obelisk/\n\u251c\u2500\u2500 .devcontainer/                # Development container configuration\n\u2502   \u251c\u2500\u2500 devcontainer.json         # VS Code development container settings\n\u2502   \u2514\u2500\u2500 Dockerfile                # Development container image definition\n\u251c\u2500\u2500 .github/                      # GitHub-specific configurations\n\u2502   \u2514\u2500\u2500 dependabot.yml            # Dependabot configuration\n\u251c\u2500\u2500 obelisk/                      # Python package for Obelisk\n\u2502   \u251c\u2500\u2500 __init__.py               # Package initialization\n\u2502   \u251c\u2500\u2500 cli.py                    # Command-line interface\n\u2502   \u251c\u2500\u2500 config.py                 # Configuration utilities\n\u2502   \u2514\u2500\u2500 convert.py                # Conversion utilities\n\u251c\u2500\u2500 site/                         # Generated static site (output)\n\u251c\u2500\u2500 vault/                        # Documentation content (input)\n\u251c\u2500\u2500 .editorconfig                 # Editor configuration for consistency\n\u251c\u2500\u2500 .envrc                        # direnv configuration for environment setup\n\u251c\u2500\u2500 .gitattributes                # Git attributes configuration\n\u251c\u2500\u2500 .gitignore                    # Files ignored by Git\n\u251c\u2500\u2500 CLAUDE.md                     # Instructions for Claude AI assistant\n\u251c\u2500\u2500 CONTRIBUTING.md               # Contribution guidelines\n\u251c\u2500\u2500 docker-compose.yaml           # Docker Compose configuration\n\u251c\u2500\u2500 Dockerfile                    # Main Docker image definition\n\u251c\u2500\u2500 LICENSE                       # Project license\n\u251c\u2500\u2500 mkdocs.yml                    # MkDocs configuration\n\u251c\u2500\u2500 poetry.lock                   # Poetry locked dependencies\n\u251c\u2500\u2500 pyproject.toml                # Python project configuration\n\u251c\u2500\u2500 README.md                     # Project README\n\u251c\u2500\u2500 Taskfile.yaml                 # Task runner configuration\n\u2514\u2500\u2500 versions.json                 # Documentation version information\n</code></pre>"},{"location":"development/#development-environment-files","title":"Development Environment Files","text":"<p>The following sections document the various configuration files and their purposes:</p> <ul> <li>Docker Configuration - Docker and container configuration</li> <li>Task Runner - Task runner configuration and usage</li> <li>Python Configuration - Python project settings and dependencies</li> <li>Git Configuration - Git workflow and settings</li> <li>Editor Configuration - Code editor settings</li> <li>Documentation Files - Project documentation</li> </ul>"},{"location":"development/docker/","title":"Docker Configuration","text":"<p>Obelisk uses Docker for containerization of both the development environment and the production deployment.</p>"},{"location":"development/docker/#dockerfile","title":"Dockerfile","text":"<p>The main <code>Dockerfile</code> defines the container image for running Obelisk:</p> <pre><code># View the Dockerfile contents\ncat /workspaces/obelisk/Dockerfile\n</code></pre> <p>This file defines: - Base image selection - System dependencies installation - Python environment setup - Obelisk installation - Default command execution</p>"},{"location":"development/docker/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<p>The <code>docker-compose.yaml</code> file orchestrates the complete Obelisk stack, including optional AI components:</p> <pre><code># View the docker-compose.yaml contents\ncat /workspaces/obelisk/docker-compose.yaml\n</code></pre> <p>Key services include: - obelisk: The main Obelisk documentation server - ollama: (Optional) AI model server for local embedding and inference - openwebui: (Optional) Web interface for interacting with AI models</p>"},{"location":"development/docker/#development-container","title":"Development Container","text":"<p>The <code>.devcontainer</code> directory contains configuration for VS Code's Development Containers feature:</p>"},{"location":"development/docker/#devcontainerjson","title":"devcontainer.json","text":"<pre><code># View the devcontainer.json contents\ncat /workspaces/obelisk/.devcontainer/devcontainer.json\n</code></pre> <p>This file configures: - Development container settings - VS Code extensions to install - Port forwarding - Environment variables - Startup commands</p>"},{"location":"development/docker/#dockerfile-dev","title":"Dockerfile (Dev)","text":"<pre><code># View the development container Dockerfile\ncat /workspaces/obelisk/.devcontainer/Dockerfile\n</code></pre> <p>The development container Dockerfile includes: - Development-specific tools and dependencies - Debugging utilities - Additional build tools</p>"},{"location":"development/docker/#running-with-docker","title":"Running with Docker","text":"<p>To run Obelisk using Docker:</p> <ol> <li> <p>Build the image:    <pre><code>docker build -t obelisk .\n</code></pre></p> </li> <li> <p>Run with Docker:    <pre><code>docker run -p 8000:8000 obelisk\n</code></pre></p> </li> <li> <p>Run with Docker Compose:    <pre><code>docker-compose up obelisk\n</code></pre></p> </li> <li> <p>Run the full stack with AI:    <pre><code>docker-compose up\n</code></pre></p> </li> </ol>"},{"location":"development/docker/#task-runner-integration","title":"Task Runner Integration","text":"<p>Docker commands are also available through the Task runner:</p> <pre><code># Build Docker container\ntask docker-build\n\n# Run with local volumes mounted\ntask docker-run\n\n# Run Obelisk service only\ntask compose-obelisk\n\n# Run full stack with Ollama and OpenWebUI\ntask compose\n</code></pre>"},{"location":"development/documentation/","title":"Documentation Files","text":"<p>Obelisk includes several documentation files to guide developers and users.</p>"},{"location":"development/documentation/#readmemd","title":"README.md","text":"<p>The primary project README provides an overview of Obelisk:</p> <pre><code># View README.md contents\ncat /workspaces/obelisk/README.md\n</code></pre> <p>The README typically includes: - Project description and purpose - Quick start instructions - Key features - Requirements - Basic usage examples - Links to more detailed documentation</p>"},{"location":"development/documentation/#contributingmd","title":"CONTRIBUTING.md","text":"<p>Guidelines for contributing to the project:</p> <pre><code># View CONTRIBUTING.md contents\ncat /workspaces/obelisk/CONTRIBUTING.md\n</code></pre> <p>This file covers: - How to set up a development environment - Coding standards and conventions - Testing requirements - Pull request process - Issue reporting guidelines</p>"},{"location":"development/documentation/#claudemd","title":"CLAUDE.md","text":"<p>Special instructions for Claude AI assistants:</p> <pre><code># View CLAUDE.md contents\ncat /workspaces/obelisk/CLAUDE.md\n</code></pre> <p>This file provides: - Project context for Claude AI - Common commands and workflows - Code style guidelines - Specific instructions for AI-assisted development</p>"},{"location":"development/documentation/#license","title":"LICENSE","text":"<p>The project's license file:</p> <pre><code># View LICENSE contents\ncat /workspaces/obelisk/LICENSE\n</code></pre> <p>Obelisk is licensed under the MIT License, which: - Permits commercial use, modification, distribution, and private use - Requires license and copyright notice inclusion - Provides no warranty or liability</p>"},{"location":"development/documentation/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation content is organized in the <code>vault</code> directory, with additional content generated during the build process:</p> <pre><code>vault/\n\u251c\u2500\u2500 assets/                  # Static assets like images\n\u251c\u2500\u2500 cloud/                   # Cloud-related documentation\n\u251c\u2500\u2500 customization/           # Theme customization documentation\n\u251c\u2500\u2500 development/             # Development setup documentation\n\u251c\u2500\u2500 javascripts/             # Custom JavaScript files\n\u251c\u2500\u2500 overrides/               # Theme template overrides\n\u251c\u2500\u2500 stylesheets/             # Custom CSS styles\n\u2514\u2500\u2500 index.md                 # Home page\n</code></pre>"},{"location":"development/documentation/#generated-documentation","title":"Generated Documentation","text":"<p>The built documentation is generated to the <code>site</code> directory:</p> <pre><code>site/\n\u251c\u2500\u2500 404.html                 # Not found page\n\u251c\u2500\u2500 assets/                  # Processed assets\n\u251c\u2500\u2500 cloud/                   # Built cloud documentation\n\u251c\u2500\u2500 customization/           # Built customization documentation\n\u251c\u2500\u2500 development/             # Built development documentation\n\u251c\u2500\u2500 index.html               # Built home page\n\u251c\u2500\u2500 search/                  # Search index files\n\u251c\u2500\u2500 sitemap.xml              # Site map for search engines\n\u2514\u2500\u2500 versions.json            # Documentation version information\n</code></pre>"},{"location":"development/editor-config/","title":"Editor Configuration","text":"<p>Obelisk includes configuration files for consistent code formatting and editor behavior across development environments.</p>"},{"location":"development/editor-config/#editorconfig","title":".editorconfig","text":"<p>The <code>.editorconfig</code> file defines coding style preferences that many editors and IDEs support:</p> <pre><code># View .editorconfig contents\ncat /workspaces/obelisk/.editorconfig\n</code></pre> <p>Key settings include: - indent_style: Spaces vs tabs - indent_size: Number of spaces per indentation level - end_of_line: Line ending style (LF, CRLF) - charset: Character encoding - trim_trailing_whitespace: Remove trailing spaces - insert_final_newline: Ensure files end with a newline</p> <p>Different rules can be specified for different file types:</p> <pre><code># Python files\n[*.py]\nindent_style = space\nindent_size = 4\n\n# YAML files\n[*.{yml,yaml}]\nindent_style = space\nindent_size = 2\n</code></pre>"},{"location":"development/editor-config/#vs-code-integration","title":"VS Code Integration","text":"<p>The <code>.devcontainer/devcontainer.json</code> file includes VS Code-specific settings:</p> <pre><code>\"settings\": {\n    \"python.defaultInterpreterPath\": \"/usr/local/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true,\n    \"editor.rulers\": [88],\n    \"terminal.integrated.defaultProfile.linux\": \"bash\"\n}\n</code></pre> <p>These settings configure: - Python interpreter path - Linting tools and settings - Code formatting (Black) - Format on save behavior - Visual rulers for line length - Terminal configuration</p>"},{"location":"development/editor-config/#recommended-extensions","title":"Recommended Extensions","text":"<p>The development container configuration includes recommended VS Code extensions:</p> <pre><code>\"extensions\": [\n    \"ms-python.python\",\n    \"ms-python.vscode-pylance\",\n    \"batisteo.vscode-django\",\n    \"yzhang.markdown-all-in-one\"\n]\n</code></pre> <p>Popular extensions for Obelisk development include: - Python: Python language support - Pylance: Python language server - Django: Django framework support - Markdown All in One: Markdown editing features</p>"},{"location":"development/editor-config/#code-style-enforcement","title":"Code Style Enforcement","text":"<p>Obelisk uses several tools to enforce code style:</p> <ol> <li>Black: Code formatter with minimal configuration</li> <li>Ruff: Fast Python linter</li> <li>mypy: Optional static type checking</li> </ol> <p>Configuration for these tools is in <code>pyproject.toml</code>:</p> <pre><code>[tool.black]\nline-length = 88\ntarget-version = [\"py312\"]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\"E\", \"F\", \"I\", \"W\"]\nignore = []\n</code></pre>"},{"location":"development/git-config/","title":"Git Configuration","text":"<p>Obelisk uses Git for version control with several additional configuration files for managing repository behavior.</p>"},{"location":"development/git-config/#gitignore","title":".gitignore","text":"<p>The <code>.gitignore</code> file specifies which files and directories should be excluded from Git:</p> <pre><code># View .gitignore contents\ncat /workspaces/obelisk/.gitignore\n</code></pre> <p>Key exclusions include: - Built documentation (<code>/site/</code>) - Python cache files (<code>__pycache__/</code>, <code>*.py[cod]</code>) - Virtual environments (<code>.venv/</code>, <code>venv/</code>) - Environment files (<code>.env</code>) - System and editor files (<code>.DS_Store</code>, <code>.idea/</code>)</p>"},{"location":"development/git-config/#gitattributes","title":".gitattributes","text":"<p>The <code>.gitattributes</code> file defines attributes for paths in the repository:</p> <pre><code># View .gitattributes contents\ncat /workspaces/obelisk/.gitattributes\n</code></pre> <p>This file controls: - Line ending normalization - Diff behavior for specific file types - Merge strategies - Export settings</p>"},{"location":"development/git-config/#github-configuration","title":"GitHub Configuration","text":""},{"location":"development/git-config/#githubdependabotyml","title":".github/dependabot.yml","text":"<p>Dependabot configuration for automated dependency updates:</p> <pre><code># View dependabot.yml contents\ncat /workspaces/obelisk/.github/dependabot.yml\n</code></pre> <p>This configuration specifies: - Package ecosystems to monitor (e.g., pip, docker) - Update frequency - Target branches - Reviewers and assignees - Version update strategy</p>"},{"location":"development/git-config/#git-workflow","title":"Git Workflow","text":""},{"location":"development/git-config/#branching-strategy","title":"Branching Strategy","text":"<p>The project follows a feature branch workflow:</p> <ol> <li>The <code>main</code> branch contains stable releases</li> <li>Feature branches are created for new features or fixes</li> <li>Pull requests are used to merge changes back to main</li> <li>The <code>v4</code> branch is used for publishing docs with versioning</li> </ol>"},{"location":"development/git-config/#commit-conventions","title":"Commit Conventions","text":"<p>Commit messages follow the conventional commits format:</p> <pre><code>&lt;type&gt;: &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types include: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Formatting changes - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or updating tests - <code>chore</code>: Maintenance tasks</p>"},{"location":"development/git-config/#version-tags","title":"Version Tags","text":"<p>Git tags are used to mark version releases:</p> <pre><code># Create a version tag\ngit tag -a v0.1.0 -m \"Initial release\"\n\n# Push tags to remote\ngit push --tags\n</code></pre>"},{"location":"development/python-config/","title":"Python Configuration","text":"<p>Obelisk uses modern Python tooling for dependency management, packaging, and development.</p>"},{"location":"development/python-config/#pyprojecttoml","title":"pyproject.toml","text":"<p>The <code>pyproject.toml</code> file defines the project's metadata, dependencies, and build system:</p> <pre><code># View the pyproject.toml contents\ncat /workspaces/obelisk/pyproject.toml\n</code></pre> <p>Key sections include:</p>"},{"location":"development/python-config/#project-metadata","title":"Project Metadata","text":"<pre><code>[project]\nname = \"obelisk\"\nversion = \"0.1.0\"\ndescription = \"Obsidian vault to MkDocs Material Theme site generator\"\nauthors = [\n    {name = \"Obelisk Team\"}\n]\nreadme = \"README.md\"\nrequires-python = \"^3.12\"\nlicense = \"MIT\"\n</code></pre>"},{"location":"development/python-config/#dependencies","title":"Dependencies","text":"<pre><code>dependencies = [\n    \"mkdocs&gt;=1.6.0,&lt;2.0.0\",\n    \"mkdocs-material&gt;=9.6.11,&lt;10.0.0\",\n    \"mkdocs-material-extensions&gt;=1.3.1,&lt;2.0.0\",\n    \"mkdocs-git-revision-date-localized-plugin&gt;=1.4.5,&lt;2.0.0\",\n    # Other dependencies...\n]\n</code></pre>"},{"location":"development/python-config/#script-entrypoints","title":"Script Entrypoints","text":"<pre><code>[project.scripts]\nobelisk = \"obelisk.cli:main\"\n</code></pre>"},{"location":"development/python-config/#development-tools-configuration","title":"Development Tools Configuration","text":"<pre><code>[tool.black]\nline-length = 88\ntarget-version = [\"py312\"]\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\"E\", \"F\", \"I\", \"W\"]\nignore = []\n</code></pre>"},{"location":"development/python-config/#poetrylock","title":"poetry.lock","text":"<p>The <code>poetry.lock</code> file contains the exact versions of all dependencies:</p> <pre><code># View lock file summary\ncat /workspaces/obelisk/poetry.lock | head -n 20\n</code></pre> <p>This file ensures reproducible builds by pinning exact versions of: - Direct dependencies - Transitive dependencies - Platform-specific dependencies</p>"},{"location":"development/python-config/#poetry-usage","title":"Poetry Usage","text":"<p>Poetry is used for dependency management and packaging:</p> <pre><code># Install dependencies\npoetry install\n\n# Add a dependency\npoetry add package-name\n\n# Update dependencies\npoetry update\n\n# Run a command in the virtual environment\npoetry run command\n\n# Activate the virtual environment\npoetry shell\n</code></pre>"},{"location":"development/python-config/#direnv-integration","title":"direnv Integration","text":"<p>The <code>.envrc</code> file integrates with direnv for automatic environment activation:</p> <pre><code># View .envrc contents\ncat /workspaces/obelisk/.envrc\n</code></pre> <p>When entering the project directory, direnv: 1. Activates the Python virtual environment 2. Sets necessary environment variables 3. Adds development tools to the PATH</p>"},{"location":"development/task-runner/","title":"Task Runner","text":"<p>Obelisk uses Task (via Taskfile.yaml) as a task runner for development and deployment workflows.</p>"},{"location":"development/task-runner/#taskfileyaml","title":"Taskfile.yaml","text":"<p>The <code>Taskfile.yaml</code> file defines all project tasks and their dependencies:</p> <pre><code># View the Taskfile.yaml contents\ncat /workspaces/obelisk/Taskfile.yaml\n</code></pre>"},{"location":"development/task-runner/#available-tasks","title":"Available Tasks","text":""},{"location":"development/task-runner/#development-tasks","title":"Development Tasks","text":"<pre><code># Install dependencies (Poetry)\ntask install\n\n# Build static site\ntask build\n\n# Run strict build testing\ntask test\n\n# Fast development server with livereload\ntask run\n\n# Build and serve with browser opening\ntask serve\n\n# Remove build artifacts\ntask clean\n</code></pre>"},{"location":"development/task-runner/#versioning-tasks","title":"Versioning Tasks","text":"<pre><code># Deploy version (requires version number and description)\ntask version-deploy -- X.Y.Z \"Description\"\n\n# Set default version (requires version number)\ntask version-set-default -- X.Y.Z\n</code></pre>"},{"location":"development/task-runner/#docker-tasks","title":"Docker Tasks","text":"<pre><code># Build Docker container\ntask docker-build\n\n# Run with local volumes mounted\ntask docker-run\n\n# Run Obelisk service only\ntask compose-obelisk\n\n# Run full stack with Ollama and OpenWebUI\ntask compose\n</code></pre>"},{"location":"development/task-runner/#task-implementation-details","title":"Task Implementation Details","text":"<p>Each task in the Taskfile defines:</p> <ul> <li>desc: Description of what the task does</li> <li>deps: Tasks that must run before this one</li> <li>cmds: Commands to execute</li> <li>vars: Variables used by the task</li> <li>sources/generates: Files to watch for incremental builds</li> </ul>"},{"location":"development/task-runner/#task-variables","title":"Task Variables","text":"<p>The Taskfile also defines variables for configuration:</p> <pre><code>vars:\n  # Project settings\n  PACKAGE: obelisk\n  PYTHON_VERSION: \"3.12\"\n\n  # Docker settings\n  DOCKER_IMAGE: {{.PACKAGE}}\n  DOCKER_TAG: latest\n  DOCKER_RUN_PORT: 8000\n</code></pre>"},{"location":"development/task-runner/#example-usage","title":"Example Usage","text":"<pre><code># Install dependencies\ntask install\n\n# Start development server\ntask run\n\n# Deploy a new version\ntask version-deploy -- 0.1.0 \"Initial release\"\n</code></pre>"}]}
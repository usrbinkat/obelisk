#######################################################
# Obelisk Environment Configuration Example
#######################################################
# Copy this file to '.env' and modify values as needed
# This file documents all environment variables used across 
# the Obelisk container ecosystem
#######################################################

#======================================================
# GENERAL SETTINGS
#======================================================

# Determines whether to use OpenAI models (true) or local models only (false)
USE_OPENAI=false

# Docker Compose file path for use with Task commands
# For reorganized structure, use: deployments/docker/compose/dev.yaml
DOCKER_COMPOSE_FILE=deployments/docker/compose/dev.yaml

#======================================================
# LITELLM CONFIGURATION
#======================================================

# API base URL for accessing LiteLLM proxy
LITELLM_API_URL=http://litellm:4000

# Master key for LiteLLM admin operations - CHANGE THIS IN PRODUCTION!
LITELLM_MASTER_KEY=sk-mysecretkey1234

# Virtual key for API operations - CHANGE THIS IN PRODUCTION!
LITELLM_VIRTUAL_KEY=sk-virtual1234

# API token for client applications - generated automatically, don't set manually
# LITELLM_API_TOKEN=

# PostgreSQL database configuration for LiteLLM
LITELLM_POSTGRES_USER=postgres
LITELLM_POSTGRES_PASSWORD=postgres
LITELLM_POSTGRES_DATABASE=postgres
LITELLM_POSTGRES_HOST=litellm_db
LITELLM_POSTGRES_PORT=5432

# LiteLLM configuration path
LITELLM_MODEL_CONFIG=/app/config/litellm_config.yaml

# LiteLLM token UID for token identification
LITELLM_TOKEN_UID=obelisk-api-token

# Store models in database
STORE_MODEL_IN_DB=true

#======================================================
# OLLAMA CONFIGURATION
#======================================================

# API base URL for accessing Ollama
OLLAMA_API_URL=http://ollama:11434

# Models for Ollama - used for various services
MODEL_NAME_1=llama3
MODEL_NAME_2=mxbai-embed-large
MODEL_API_BASE_1=http://ollama:11434
MODEL_API_BASE_2=http://ollama:11434
MODEL_1=ollama/llama3
MODEL_2=ollama/mxbai-embed-large

# NVIDIA GPU settings (if using GPU acceleration)
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

#======================================================
# RAG SERVICE CONFIGURATION
#======================================================

# API host and port for RAG service
API_HOST=0.0.0.0
API_PORT=8000

# Vector database configuration
VECTOR_DB=milvus  # Options: chroma, milvus
CHROMA_DIR=/app/data/chroma_db

# Milvus configuration
MILVUS_HOST=milvus
MILVUS_PORT=19530
MILVUS_URI=http://milvus:19530

# Document processing settings
CHUNK_SIZE=2500
CHUNK_OVERLAP=500
RETRIEVE_TOP_K=5

# Embedding model settings
EMBEDDING_MODEL=mxbai-embed-large
EMBEDDING_PROVIDER=ollama  # Options: ollama, openai

# Completion model settings
COMPLETION_PROVIDER=ollama  # Options: ollama, openai

# Document watching for auto-update
WATCH_DOCS=true
VAULT_DIR=/app/vault

# Logging configuration
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

#======================================================
# OPENWEBUI CONFIGURATION
#======================================================

# Enable RAG integration with OpenWebUI
ENABLE_OPENWEBUI_RAG=false

# Authentication token for OpenWebUI - generated automatically, don't set manually
# OPENWEBUI_AUTH_TOKEN=

# OpenWebUI configuration
WEBUI_URL=http://localhost:8080
ENV=dev
DEFAULT_LOCALE="America/Los_Angeles"

# OpenWebUI models configuration
OLLAMA_MODELS=llama3,mxbai-embed-large

# OpenWebUI content extraction
CONTENT_EXTRACTION_ENGINE=tika
TIKA_SERVER_URL=http://tika:9998

# OpenWebUI retrieval configuration
RETRIEVAL_ENABLED=true
RETRIEVAL_VECTOR_STORE=milvus

#======================================================
# EXTERNAL API KEYS
#======================================================

# OpenAI API key and configuration
OPENAI_API_KEY=sk-openai-key-goes-here
OPENAI_COMPLETION_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
OPENAI_ORG_ID=

# Google Gemini API key
GEMINI_API_KEY=your-gemini-api-key-goes-here

# Anthropic API key
ANTHROPIC_API_KEY=sk-ant-api01-key-goes-here

#======================================================
# OPTIONAL SERVICES
#======================================================

# Langfuse Tracing (Optional)
# LANGFUSE_PROJECT_ID=your-project-id
# LANGFUSE_PUBLIC_KEY=your-public-key
# LANGFUSE_SECRET_KEY=your-secret-key

#======================================================
# ADVANCED CONFIGURATION
#======================================================

# Docker Compose related settings
# Uncomment to use - typically only needed in specific cases
# COMPOSE_PROJECT_NAME=obelisk
# COMPOSE_PROFILES=default
# COMPOSE_FILE_SEPARATOR=:
# DOCKER_BUILDKIT=1
# COMPOSE_DOCKER_CLI_BUILD=1

#======================================================
# RESOURCE LIMITS (PRODUCTION)
#======================================================

# Uncomment to limit resources for production environments
# RAG_SERVICE_MEMORY_LIMIT=2g
# RAG_SERVICE_CPU_LIMIT=1.0
# OLLAMA_MEMORY_LIMIT=8g
# OLLAMA_CPU_LIMIT=4.0
# MILVUS_MEMORY_LIMIT=4g
# MILVUS_CPU_LIMIT=2.0
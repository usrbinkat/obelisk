<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Obsidian vault to MkDocs Material Theme site generator with AI integration"><meta name=author content="Obelisk Team"><link href=https://usrbinkat.github.io/obelisk/chatbot/rag/architecture-draft-review/ rel=canonical><link href=../architecture-draft/ rel=prev><link href=../vector-database/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Architecture Review - Obelisk</title><link rel=stylesheet href=../../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-orange> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#retrieval-augmented-generation-rag-architecture-for-a-local-first-developer-setup class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <a href=https://github.com/usrbinkat/obelisk> <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span> <strong>Obelisk v0.1.0</strong> released - Start your Obelisk now </a> </div> </aside> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> You're viewing an outdated version of this documentation. <a href=../../../..> <strong>Click here to go to the latest version.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),base=new URL("../../.."),outdated=__md_get("__outdated",sessionStorage,base);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=Obelisk class="md-header__button md-logo" aria-label=Obelisk data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5zM6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Obelisk </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Architecture Review </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=deep-orange aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=deep-orange aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/usrbinkat/obelisk title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> usrbinkat/obelisk </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../cloud/Cloud%20Native%20Tooling/ class=md-tabs__link> Cloud </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Chatbot </a> </li> <li class=md-tabs__item> <a href=../../../customization/ class=md-tabs__link> Customization </a> </li> <li class=md-tabs__item> <a href=../../../development/ class=md-tabs__link> Development </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=Obelisk class="md-nav__button md-logo" aria-label=Obelisk data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5zM6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5"/></svg> </a> Obelisk </label> <div class=md-nav__source> <a href=https://github.com/usrbinkat/obelisk title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> usrbinkat/obelisk </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Cloud </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Cloud </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../cloud/Cloud%20Native%20Tooling/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chatbot </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Chatbot </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../models/ class=md-nav__link> <span class=md-ellipsis> Models Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../integration/ class=md-nav__link> <span class=md-ellipsis> Documentation Integration </span> </a> </li> <li class=md-nav__item> <a href=../../openwebui/ class=md-nav__link> <span class=md-ellipsis> Open WebUI Configuration </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_5 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> RAG Pipeline </span> </a> <label class="md-nav__link " for=__nav_3_5 id=__nav_3_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=true> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> RAG Pipeline </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../architecture-draft/ class=md-nav__link> <span class=md-ellipsis> Architecture Design </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Architecture Review </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Architecture Review </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview-and-key-requirements class=md-nav__link> <span class=md-ellipsis> Overview and Key Requirements </span> </a> </li> <li class=md-nav__item> <a href=#document-ingestion-and-indexing-pipeline class=md-nav__link> <span class=md-ellipsis> Document Ingestion and Indexing Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#embedding-generation-local-models-and-alternatives class=md-nav__link> <span class=md-ellipsis> Embedding Generation: Local Models and Alternatives </span> </a> </li> <li class=md-nav__item> <a href=#vector-database-local-deployment-options class=md-nav__link> <span class=md-ellipsis> Vector Database: Local Deployment Options </span> </a> <nav class=md-nav aria-label="Vector Database: Local Deployment Options"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#milvus-with-milvus-lite class=md-nav__link> <span class=md-ellipsis> Milvus (with Milvus Lite) </span> </a> </li> <li class=md-nav__item> <a href=#chroma class=md-nav__link> <span class=md-ellipsis> Chroma </span> </a> </li> <li class=md-nav__item> <a href=#postgres-pgvector class=md-nav__link> <span class=md-ellipsis> Postgres + pgvector </span> </a> </li> <li class=md-nav__item> <a href=#comparison-of-vector-db-options class=md-nav__link> <span class=md-ellipsis> Comparison of Vector DB Options </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langchain-rag-pipeline-design class=md-nav__link> <span class=md-ellipsis> LangChain RAG Pipeline Design </span> </a> <nav class=md-nav aria-label="LangChain RAG Pipeline Design"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#retrieval-step-embedding-query-and-similarity-search class=md-nav__link> <span class=md-ellipsis> Retrieval Step: Embedding Query and Similarity Search </span> </a> </li> <li class=md-nav__item> <a href=#prompt-construction-and-llm-query class=md-nav__link> <span class=md-ellipsis> Prompt Construction and LLM Query </span> </a> </li> <li class=md-nav__item> <a href=#end-to-end-query-flow class=md-nav__link> <span class=md-ellipsis> End-to-End Query Flow </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#integration-with-openwebui class=md-nav__link> <span class=md-ellipsis> Integration with OpenWebUI </span> </a> </li> <li class=md-nav__item> <a href=#performance-and-resource-considerations class=md-nav__link> <span class=md-ellipsis> Performance and Resource Considerations </span> </a> </li> <li class=md-nav__item> <a href=#implementation-references class=md-nav__link> <span class=md-ellipsis> Implementation References </span> </a> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> <span class=md-ellipsis> Conclusion </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../vector-database/ class=md-nav__link> <span class=md-ellipsis> Vector Database </span> </a> </li> <li class=md-nav__item> <a href=../query-pipeline/ class=md-nav__link> <span class=md-ellipsis> Query Pipeline </span> </a> </li> <li class=md-nav__item> <a href=../ollama-integration/ class=md-nav__link> <span class=md-ellipsis> Ollama Integration </span> </a> </li> <li class=md-nav__item> <a href=../implementation/ class=md-nav__link> <span class=md-ellipsis> Implementation Guide </span> </a> </li> <li class=md-nav__item> <a href=../evaluation/ class=md-nav__link> <span class=md-ellipsis> Evaluation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../../customization/ class="md-nav__link "> <span class=md-ellipsis> Customization </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Customization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../customization/mkdocs-configuration/ class=md-nav__link> <span class=md-ellipsis> MkDocs Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../customization/css/ class=md-nav__link> <span class=md-ellipsis> CSS Styling </span> </a> </li> <li class=md-nav__item> <a href=../../../customization/html/ class=md-nav__link> <span class=md-ellipsis> HTML Templates </span> </a> </li> <li class=md-nav__item> <a href=../../../customization/javascript/ class=md-nav__link> <span class=md-ellipsis> JavaScript </span> </a> </li> <li class=md-nav__item> <a href=../../../customization/python/ class=md-nav__link> <span class=md-ellipsis> Python Integration </span> </a> </li> <li class=md-nav__item> <a href=../../../customization/versioning/ class=md-nav__link> <span class=md-ellipsis> Versioning </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../../development/ class="md-nav__link "> <span class=md-ellipsis> Development </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Development </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../development/docker/ class=md-nav__link> <span class=md-ellipsis> Docker Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../development/task-runner/ class=md-nav__link> <span class=md-ellipsis> Task Runner </span> </a> </li> <li class=md-nav__item> <a href=../../../development/python-config/ class=md-nav__link> <span class=md-ellipsis> Python Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../development/git-config/ class=md-nav__link> <span class=md-ellipsis> Git Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../development/editor-config/ class=md-nav__link> <span class=md-ellipsis> Editor Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../development/documentation/ class=md-nav__link> <span class=md-ellipsis> Documentation Files </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/usrbinkat/obelisk/edit/main/vault/chatbot/rag/architecture-draft-review.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/usrbinkat/obelisk/raw/main/vault/chatbot/rag/architecture-draft-review.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=retrieval-augmented-generation-rag-architecture-for-a-local-first-developer-setup>Retrieval-Augmented Generation (RAG) Architecture for a Local-First Developer Setup<a class=headerlink href=#retrieval-augmented-generation-rag-architecture-for-a-local-first-developer-setup title="Permanent link">&para;</a></h1> <h2 id=overview-and-key-requirements>Overview and Key Requirements<a class=headerlink href=#overview-and-key-requirements title="Permanent link">&para;</a></h2> <p>This design outlines a <strong>local-first RAG system</strong> for developers that ingest Markdown documents and enables querying via local LLMs. The architecture emphasizes <strong>offline operation</strong>, dynamic updates from a Git-tracked <code>./vault</code> directory, and compatibility with developer hardware (NVIDIA GPUs and Apple M-series). Key requirements include:</p> <ul> <li><strong>Dynamic Document Ingestion:</strong> Automatically detect and process new or updated Markdown files in a vault (Git repository) into the RAG knowledge base.</li> <li><strong>Local Embeddings &amp; Storage:</strong> Generate embeddings on-premise (no cloud API) using state-of-the-art open models like <code>mxbai-embed-large</code>, and store vectors in a local vector database.</li> <li><strong>Efficient Retrieval Pipeline:</strong> Use a LangChain-based pipeline to retrieve relevant content for a query with modern retrieval techniques (e.g. similarity search, re-ranking) and feed it to a local LLM.</li> <li><strong>Local LLM Integration:</strong> Query resolution via local large language models (e.g. LLaMA 3, Phi-4, DeepSeek-R1) served through Ollama, ensuring all inference is offline.</li> <li><strong>Developer-Friendly Tools:</strong> Favor easy-to-deploy, open-source components (Milvus Lite, Chroma, or Postgres/pgvector) with minimal overhead and good performance on a single machine.</li> <li><strong>OpenWebUI Compatibility:</strong> Optionally integrate with OpenWebUI’s interface for a seamless chat experience, leveraging its knowledge base features for RAG.</li> </ul> <p>By meeting these requirements, the system will function as a self-contained “personal ChatGPT” that stays updated with local documentation and runs entirely on a laptop. Below we detail the architecture components and design decisions.</p> <h2 id=document-ingestion-and-indexing-pipeline>Document Ingestion and Indexing Pipeline<a class=headerlink href=#document-ingestion-and-indexing-pipeline title="Permanent link">&para;</a></h2> <p><strong>Ingestion pipeline</strong> refers to how documents are loaded, parsed, and indexed into the vector store. We use <strong>LangChain’s document loaders</strong> and text splitters, augmented with a filesystem watcher, to continuously ingest Markdown files from the <code>./vault</code> directory. This ensures that as soon as documentation is added or changed, the vector index is updated.</p> <ul> <li><strong>Markdown Document Loading:</strong> LangChain provides Markdown loaders (or generic text loaders) to read <code>.md</code> files. Each file is loaded and its content prepared for splitting. Metadata (such as filename, path, or commit ID) is attached to each document so that the origin of retrieved text is known. This metadata will later help format answers with source references.</li> <li><strong>Dynamic Watcher for Updates:</strong> LangChain’s core does not natively monitor filesystem changes in real-time. Instead, we integrate a Python filesystem watcher (e.g. the <code>watchdog</code> library) to track additions, deletions, or modifications in the vault. On detecting a change, the pipeline will load the new/updated file and update the vector store (<a href="https://github.com/hwchase17/langchain/issues/5252#:~:text=I%20don%27t%20feel%20that%20it%27s,achieve%20this%20feature%2C%20you%20can">how to monitoring the new files after directory loader class used · Issue #5252 · langchain-ai/langchain · GitHub</a>). This approach is recommended by LangChain contributors for auto-ingestion of new files (<a href="https://github.com/hwchase17/langchain/issues/5252#:~:text=I%20don%27t%20feel%20that%20it%27s,achieve%20this%20feature%2C%20you%20can">how to monitoring the new files after directory loader class used · Issue #5252 · langchain-ai/langchain · GitHub</a>). Alternatively, a scheduled batch job or Git hook could trigger re-ingestion for new commits.</li> <li><strong>Text Splitting:</strong> Each Markdown document is split into smaller chunks (e.g. 500-1000 tokens) using LangChain’s text splitters (such as <code>RecursiveCharacterTextSplitter</code>). Splitting by section or headings is ideal so that each chunk is a semantically coherent unit (e.g. a paragraph, bullet list, or code block). This yields better retrieval granularity and ensures the LLM can be given just the relevant portions of a document. </li> <li><strong>Indexing &amp; Embedding Ingestion:</strong> For each chunk, an embedding vector is computed and upserted into the vector database (with the chunk’s text and metadata). If a file was modified or deleted, the pipeline will delete or update the corresponding vectors to keep the index in sync. Using unique IDs (like stable document IDs + chunk index) for each chunk makes it possible to replace or remove them when the source changes.</li> </ul> <p>By maintaining this ingestion pipeline, the “knowledge base” is always up to date with the Markdown content in the Git vault. The pipeline is lightweight enough to run in the background on a developer laptop, thanks to efficient local embedding models and an embedded vector store.</p> <h2 id=embedding-generation-local-models-and-alternatives>Embedding Generation: Local Models and Alternatives<a class=headerlink href=#embedding-generation-local-models-and-alternatives title="Permanent link">&para;</a></h2> <p>To convert text into high-dimensional vectors for similarity search, the system uses a <strong>local embedding model</strong>. We prioritize <code>mxbai-embed-large-v1</code> (by Mixedbread AI) or a similar state-of-the-art open model due to their strong semantic performance. The embedding model choice critically affects downstream retrieval quality, so we consider the latest options in 2024–2025:</p> <ul> <li><strong><code>mxbai-embed-large-v1</code>:</strong> A 335M parameter English embedding model known for state-of-the-art results among efficiently-sized models (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=mxbai,002">mxbai-embed-large-v1 - Mixedbread</a>). It outperforms OpenAI’s text-embedding-ada-002 in accuracy (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=mxbai,002">mxbai-embed-large-v1 - Mixedbread</a>) and was trained on 700M pairs + 30M triplets with a specialized contrastive loss. It produces 1024-dimensional embeddings and is versatile across domains (tested via MTEB benchmark) (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=mxbai,demonstrates%20its%20versatility%20and%20robustness">mxbai-embed-large-v1 - Mixedbread</a>). This model is a strong default for our RAG pipeline given its high quality and moderate size.</li> <li><strong>Context Length &amp; Speed:</strong> <code>mxbai-embed-large</code> can handle reasonably long inputs (512 tokens recommended (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=Layers%20Embedding%20Dimension%20Recommended%20Sequence,Language%2024%201024%20512%20English">mxbai-embed-large-v1 - Mixedbread</a>)). For very large Markdown files, we may chunk before embedding to avoid truncation. Its 335M size is feasible to run on CPU (with MKL/BLAS optimizations) or modest GPU, but for large volumes of text, batching and possibly 8-bit quantization can be used to speed up embedding generation.</li> <li><strong>Alternative Embedding Models:</strong> By 2025, new open-source embedding models have emerged that might offer better performance:</li> <li><em>Stella (400M &amp; 1.5B):</em> An open model by Dun Zhang that tops MTEB’s retrieval leaderboard (commercial-use allowed). Notably, the 1.5B version provides only marginal gains over the 400M version (<a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,to%20pay%20for%20more%20throughput">The Best Embedding Models for Information Retrieval in 2025 | DataStax</a>), making the smaller Stella-400M very attractive for local use. Stella is praised for its excellent out-of-the-box accuracy (<a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,Perhaps%20the">The Best Embedding Models for Information Retrieval in 2025 | DataStax</a>).</li> <li><em>BGE (BAAI’s model):</em> Models like <strong>bge-large</strong> or <strong>bge-m3</strong> are multi-lingual, multi-domain embedding models (567M params) that perform well in semantic tasks. These appear in Ollama’s model list and offer versatility (e.g. BGE-M3 is noted for multi-granularity embeddings) (<a href="https://ollama.com/search?c=embedding#:~:text=%2A%20%20mxbai,very%20large%20sentence%20level%20datasets">Embedding models · Ollama Search</a>).</li> <li><em>Snowflake’s Arctic Embeds:</em> A suite of models (22M–335M and a newer 568M v2) optimized by Snowflake for high performance (<a href="https://ollama.com/search?c=embedding#:~:text=822,model%20that%20can%20be%20used">Embedding models · Ollama Search</a>) (<a href="https://ollama.com/search?c=embedding#:~:text=Updated%208%20months%20ago%20,14">Embedding models · Ollama Search</a>). These include multilingual support in v2 (<a href="https://ollama.com/search?c=embedding#:~:text=Updated%208%20months%20ago%20,14">Embedding models · Ollama Search</a>).</li> <li><em>Modern BERT Embed:</em> A 2025-era model from Answer AI/LightOn that aimed to improve BERT-based embeddings. However, evaluations showed it underperformed expectations relative to others (<a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,to%20pay%20for%20more%20throughput">The Best Embedding Models for Information Retrieval in 2025 | DataStax</a>).</li> <li><em>Nomic’s <code>nomic-embed-text</code>:</em> A high-performing model with a large token window, popular via Ollama (<a href="https://ollama.com/search?c=embedding#:~:text=Popular%20Newest">Embedding models · Ollama Search</a>). It is efficient (21M pulls indicating popularity) and may allow embedding longer documents in one shot.</li> </ul> <p>In practice, <strong>mxbai-embed-large</strong> remains a top choice for English text due to its proven balance of accuracy and size. We keep the architecture flexible so the embedding model can be swapped if a new model (e.g. “Voyage-3-embed” or other future release) surpasses it. The embedding generation is done through a local inference pipeline. We can run the model using the Hugging Face Transformers pipeline in Python or via Ollama if the model is packaged there (Ollama supports pulling these embeddings models directly (<a href="https://ollama.com/search?c=embedding#:~:text=%2A%20%20nomic,9K%20Pulls%2016%20Tags">Embedding models · Ollama Search</a>)). For example, one could run <code>ollama pull mxbai-embed-large</code> and have an embedding server locally. </p> <p><strong>Table: Comparison of Candidate Embedding Models (2024–2025)</strong></p> <table> <thead> <tr> <th>Model</th> <th>Dimensions</th> <th>Size (params)</th> <th>Notable Features</th> <th>License</th> </tr> </thead> <tbody> <tr> <td><strong>mxbai-embed-large</strong></td> <td>1024</td> <td>335M</td> <td>Top-tier accuracy, English-only, supports binary embeddings for compression (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=mxbai,of%20the%20performance">mxbai-embed-large-v1 - Mixedbread</a>)</td> <td>Apache-2.0 (?)</td> </tr> <tr> <td><strong>Stella-400M</strong></td> <td>1024</td> <td>400M</td> <td>MTEB retrieval leader (open-source), almost on par with 1.5B model ([The Best Embedding Models for Information Retrieval in 2025</td> <td>DataStax](<a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,to%20pay%20for%20more%20throughput">https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=3,to%20pay%20for%20more%20throughput</a>))</td> </tr> <tr> <td><strong>BGE-m3 (BAAI)</strong></td> <td>768 or 1024</td> <td>567M</td> <td>Multi-lingual &amp; multi-domain; good zero-shot performance</td> <td>Apache-2.0</td> </tr> <tr> <td><strong>Snowflake Arctic v2</strong></td> <td>768</td> <td>568M</td> <td>Multilingual, enterprise-optimized, various sizes available</td> <td>Apache-2.0</td> </tr> <tr> <td><strong>ModernBERT Large</strong></td> <td>1024</td> <td>~1B</td> <td>Next-gen BERT-based model by LightOn (underwhelming in 2025 tests ([The Best Embedding Models for Information Retrieval in 2025</td> <td>DataStax](<a href="https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=Zhang%20released%20a%20whitepaper%20in,to%20pay%20for%20more%20throughput">https://www.datastax.com/blog/best-embedding-models-information-retrieval-2025#:~:text=Zhang%20released%20a%20whitepaper%20in,to%20pay%20for%20more%20throughput</a>)))</td> </tr> <tr> <td><strong>nomic-embed-text</strong></td> <td>768</td> <td>- (transformer)</td> <td>High performance, large context window, very popular for local setups (<a href="https://ollama.com/search?c=embedding#:~:text=%2A%20%20nomic,9K%20Pulls%2016%20Tags">Embedding models · Ollama Search</a>)</td> <td>MIT</td> </tr> </tbody> </table> <p><em>Note:</em> All above models are <strong>offline</strong> and can be run locally. We ensure whichever model is used is loaded at startup and kept in memory for throughput. The embedding step is typically fast (&lt;100ms per chunk on GPU for these model sizes, or a bit slower on CPU). If using Apple M-series, models can run via coreml or 4-bit quantization for speed, albeit with slight accuracy trade-offs.</p> <p>After embedding, each text chunk (with its metadata) and vector goes into the vector database for fast similarity search, described next.</p> <h2 id=vector-database-local-deployment-options>Vector Database: Local Deployment Options<a class=headerlink href=#vector-database-local-deployment-options title="Permanent link">&para;</a></h2> <p>A <strong>vector database</strong> stores embeddings and supports similarity search (k-NN) efficiently. For a local-first system, the DB must run on a laptop without heavy resource needs or cloud services. We evaluate three popular options – <strong>Milvus (Lite)</strong>, <strong>Chroma</strong>, and <strong>Postgres (pgvector)</strong> – focusing on their capabilities, limitations, and ease of use for a developer:</p> <h3 id=milvus-with-milvus-lite>Milvus (with Milvus Lite)<a class=headerlink href=#milvus-with-milvus-lite title="Permanent link">&para;</a></h3> <p>Milvus is an open-source, purpose-built vector database designed for high-performance at scale (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Chroma%20vector%20database%20is%20a,latency%20applications">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). It supports a wide range of indexing algorithms (IVF, HNSW, DiskANN, etc.) and can handle billion-scale vectors with low latency on distributed clusters (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Another%20notable%20difference%20between%20Milvus,SPARSE_INVERTED_INDEX%2C%20SPARSE_WAND%2C%20CAGRA%2C%20GPU_IVF_FLAT%2C%20and">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). For our local setup, we leverage <strong>Milvus Lite</strong>, a lightweight in-process version introduced in Milvus 2.4.x that is tailored for laptops and small data sizes (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20is%20the%20lightweight,core%20components%20of%20Milvus%20Lite">milvus-lite · PyPI</a>) (<a href="https://pypi.org/project/milvus-lite/#:~:text=Image">milvus-lite · PyPI</a>).</p> <ul> <li><strong>Capabilities:</strong> Milvus (full) offers extensive index choices and tuning: e.g., HNSW for high recall, IVF for memory/disk trade-offs, GPU-accelerated indexes, and even hybrid search combining vector and scalar filters (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Another%20notable%20difference%20between%20Milvus,SPARSE_INVERTED_INDEX%2C%20SPARSE_WAND%2C%20CAGRA%2C%20GPU_IVF_FLAT%2C%20and">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L176%20Milvus%20and,the%20inverted%20index%20with%20tantivy">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). It ensures high recall and performance even as data scales to millions or more vectors. Milvus supports metadata filtering and hybrid queries (vector + keyword filtering) which can be useful to narrow results by file tags or sections (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L176%20Milvus%20and,substantial%20boost%20in%20prefiltering%20speed">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>).</li> <li><strong>Milvus Lite for Local Use:</strong> Milvus Lite runs as an embedded library (via <code>pymilvus</code> &gt;=2.4.2) without needing a separate server. This drastically lowers the setup complexity – the vector store is just a local file (e.g. <code>milvus_demo.db</code>) used via the same Milvus API (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20can%20be%20imported,pip%20install%20pymilvus">milvus-lite · PyPI</a>) (<a href="https://pypi.org/project/milvus-lite/#:~:text=Usage">milvus-lite · PyPI</a>). It supports &lt;1 million vectors comfortably in-memory on a single machine (<a href="https://pypi.org/project/milvus-lite/#:~:text=Image">milvus-lite · PyPI</a>), aligning with the scale of a personal vault. Importantly, Milvus Lite supports Apple Silicon (M1/M2) and Linux ARM out-of-the-box (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20currently%20supports%20the,following%20environments">milvus-lite · PyPI</a>), ensuring compatibility with both MacBooks and typical Linux dev machines.</li> <li><strong>Resource Requirements:</strong> A Milvus Lite instance embedded in a Python process will use only the resources needed for its indexes. This could be a few hundred MBs of RAM for, say, 100k vectors with HNSW (exact usage depends on vector dimension and index settings). It has no background daemon or extra services – unlike full Milvus which requires etcd or Pulsar – making it lightweight. Milvus is optimized in C++ and can use multiple CPU cores; with GPU, the full Milvus can offload ANN search computations to CUDA (Milvus Lite might not include GPU support, but one can always run a separate Milvus server with GPU if needed).</li> <li><strong>Limitations:</strong> The flexibility comes with complexity. While basic use is straightforward, fine-tuning indexes or dealing with schema changes in Milvus may have a learning curve for newcomers (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=Weaknesses%3A%20Less%20focus%20on%20semantic,and%20configure%20compared%20to%20Weaviate">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). However, for our moderate scale, we can likely stick with default HNSW index (high recall) and not worry about advanced tuning. Another consideration is that Milvus is focused purely on vector similarity; semantic filtering or re-ranking beyond k-NN would have to be handled at the application layer (e.g., by an LLM or cross-encoder re-ranker), whereas a system like Weaviate has built-in semantic search modules (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=Weaviate%20%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>) (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=algorithms%20and%20distance%20metrics%2C%20allowing,for%20customization">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). In our case, the LLM will handle final answer synthesis, so this is acceptable.</li> </ul> <p>Overall, Milvus Lite provides <strong>production-grade retrieval performance</strong> locally, with the same API as full Milvus if we ever scale up (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20uses%20the%20same,scale%20production">milvus-lite · PyPI</a>). This makes it a strong choice if our vector count grows or we require the fastest possible searches.</p> <h3 id=chroma>Chroma<a class=headerlink href=#chroma title="Permanent link">&para;</a></h3> <p>Chroma DB is a <strong>lightweight embedded vector store</strong> aimed at simplicity and developer-friendliness (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Chroma%20vector%20database%20is%20a,latency%20applications">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). It can be installed via pip (<code>chromadb</code>), and runs within the Python process (using DuckDB or SQLite under the hood for persistence). Chroma prioritizes ease of use – minimal configuration, an intuitive API, and tight integration with LangChain out-of-the-box.</p> <ul> <li><strong>Capabilities:</strong> Chroma provides HNSW-based ANN search (Cosine or Euclidean distance) with optional embeddings persistency on disk. It supports metadata filtering on queries, and recently introduced an HTTP server mode if needed. For a typical use (&lt;1M embeddings), Chroma’s single-node design is sufficient (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L103%20needs,or%20even%20trillions%20of%20vector">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Conversely%2C%20while%20prioritizing%20simplicity%20and,for%20applications%20with%20increasing%20demands">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). It is very easy to integrate: LangChain’s <code>Chroma</code> vector store can be initialized with a collection name and will handle storage in a local SQL database transparently.</li> <li><strong>Resource Use:</strong> As a pure-Python library with a C++ index (Faiss or HNSW in-memory index), Chroma is lightweight. It keeps the entire index in memory, so memory usage scales with number of vectors (~ a few hundred bytes per vector plus metadata overhead for HNSW). The <strong>practical limit is around 1 million vectors</strong> on a single machine before performance or memory becomes a concern (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L103%20needs,or%20even%20trillions%20of%20vector">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Conversely%2C%20while%20prioritizing%20simplicity%20and,for%20applications%20with%20increasing%20demands">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). This aligns with Milvus Lite’s sweet spot as well. Chroma does not spawn extra processes; it will create a DuckDB (or SQLite) file on disk to store the vectors and metadata for persistence.</li> <li><strong>Ease of Use:</strong> This is Chroma’s strong suit – no setup required, just <code>pip install chromadb</code>. It’s a good default for local prototypes. Chroma’s Pythonic API and LangChain integration mean a developer can go from raw text to a queryable index in a few lines of code. This makes it very <strong>developer-friendly for a local app</strong>.</li> <li><strong>Limitations:</strong> The simplicity comes at the cost of advanced features. Chroma currently uses a single index type (HNSW) and is limited to a single node (no sharding or replication) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L185%20GPU_IVF_PQ,algorithm%20for%20its%20KNN%20search">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). There’s no role-based access or multi-user security built in (not needed for a single-user dev app) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L171%20On%20the,applications%20with%20more%20complex%20requirements">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). While it supports basic filtering and CRUD on vectors, it lacks the rich configuration of Milvus. If our dataset remains moderately sized and our focus is quick development, these limitations are acceptable. Chroma may also have slower performance than Milvus on very large datasets or high concurrent query loads, but in a local scenario queries are single-user and sporadic.</li> </ul> <p>In summary, <strong>Chroma offers the simplest path</strong>: it’s essentially plug-and-play for a Python application and would work well as long as our vector count is in the thousands or low hundreds of thousands (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=While%20both%20databases%20proficiently%20manage,metrics%2C%20positioning%20Milvus%20as%20a">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L103%20needs,or%20even%20trillions%20of%20vector">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>).</p> <h3 id=postgres-pgvector>Postgres + pgvector<a class=headerlink href=#postgres-pgvector title="Permanent link">&para;</a></h3> <p>PostgreSQL with the <code>pgvector</code> extension introduces vector search capability into the familiar Postgres database. This option is attractive if we want to reuse a single database for both relational and vector data, or if we prefer the robustness of PostgreSQL for persistence.</p> <ul> <li><strong>Capabilities:</strong> pgvector allows storing vector embeddings as a column type and provides indexed approximate search (via IVF indexes) or exact search using built-in indexing. It benefits from all of Postgres’s features – transactions, durability, scalability via replication – and can combine vector queries with SQL filters easily. This means we can do hybrid queries (e.g., find similar vectors among documents where <code>category = 'API'</code>) with standard SQL syntax.</li> <li><strong>Ease of Deployment:</strong> If a developer already has Postgres running (or is comfortable with Docker), adding pgvector is straightforward (<code>CREATE EXTENSION pgvector</code>). Many developers find this easier than running a separate specialized DB. For a local setup, running Postgres might be heavier than Chroma, but not by much if Postgres is already used for other tasks. There are also lightweight Postgres variants (like Timescale or TDB) that integrate pgvector.</li> <li><strong>Performance and Scale:</strong> For <strong>moderate-sized data</strong>, pgvector performs well (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=pgvector%20%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94,sized%20datasets">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). It is known to handle tens of thousands to low millions of vectors fine, especially if using approximate indexes. However, compared to Milvus or Chroma, Postgres may struggle as vector count grows very large or if queries have to scan a lot of data. The <strong>trade-off is speed vs familiarity</strong>: Milvus is optimized in C++ for vector math, whereas Postgres has overhead from being a general database. In high-throughput or million-scale scenarios, pgvector will be slower and use more memory (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=decent%20performance%20for%20moderate">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). For our use (likely a few thousand Q&amp;A pairs or document chunks), pgvector search (with an IVF index) would likely be sub-100ms, which is acceptable.</li> <li><strong>Limitations:</strong> Postgres lacks out-of-the-box support for advanced ANN algorithms beyond IVF (which it added recently) and flat scan. It also doesn’t have built-in clustering or sharding for vectors specifically (beyond what Postgres provides generally). So while you <strong>can</strong> scale it, you might lose some of the performance benefits of a purpose-built DB at large scale (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=r%2FLocalLLaMA%20www,Relatively%20newer%20compared%20to">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). Additionally, running Postgres just for vectors is arguably more complex than using Chroma which was built for exactly that purpose. If not already needed, introducing a SQL database might add operational overhead (managing a service, tuning Postgres memory, etc.). On the flip side, it <strong>reuses known components</strong> – backup, indexing, etc. – which can be a plus for maintainability. </li> </ul> <p>In our scenario, pgvector is a viable option if we desire strong persistence guarantees and if we might integrate the data with other structured data. But if we purely need a vector store for unstructured docs, dedicated solutions (Milvus/Chroma) may be more efficient.</p> <h3 id=comparison-of-vector-db-options>Comparison of Vector DB Options<a class=headerlink href=#comparison-of-vector-db-options title="Permanent link">&para;</a></h3> <p>The table below summarizes the three options in terms of key considerations for a local developer setup:</p> <table> <thead> <tr> <th><strong>Criteria</strong></th> <th><strong>Milvus Lite (Milvus 2.4)</strong></th> <th><strong>Chroma DB</strong></th> <th><strong>Postgres pgvector</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Setup &amp; Integration</strong></td> <td>Embedded in Python via <code>pymilvus</code> (no separate server when using Lite) (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20can%20be%20imported,pip%20install%20pymilvus">milvus-lite · PyPI</a>). LangChain integration available (<code>Milvus.from_documents</code> etc.) ([Milvus</td> <td>️ LangChain](<a href="https://python.langchain.com/v0.1/docs/integrations/vectorstores/milvus/#:~:text=vector_db%20%3D%20Milvus,19530">https://python.langchain.com/v0.1/docs/integrations/vectorstores/milvus/#:~:text=vector_db%20%3D%20Milvus,19530</a>)).</td> <td>Pure Python library (<code>pip install chromadb</code>). LangChain integration native. Easiest setup (no external service).</td> </tr> <tr> <td><strong>Performance</strong></td> <td>High-performance ANN search, supports billions of vectors on full server (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Chroma%20vector%20database%20is%20a,latency%20applications">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). Lite mode good up to ~1M vectors (<a href="https://pypi.org/project/milvus-lite/#:~:text=Image">milvus-lite · PyPI</a>) with high recall. Many index types (HNSW, IVF, etc.) for tuning (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Another%20notable%20difference%20between%20Milvus,SPARSE_INVERTED_INDEX%2C%20SPARSE_WAND%2C%20CAGRA%2C%20GPU_IVF_FLAT%2C%20and">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). Can use GPU.</td> <td>Good performance for up to ~1M vectors (HNSW) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L103%20needs,or%20even%20trillions%20of%20vector">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Conversely%2C%20while%20prioritizing%20simplicity%20and,for%20applications%20with%20increasing%20demands">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). In-memory index gives low latency for moderate data. Beyond 1M, may hit single-node limits.</td> <td>Decent performance for tens of thousands of vectors; can use IVF index for faster search. May be ~20-30% slower than Milvus for large datasets (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=r%2FLocalLLaMA%20www,Relatively%20newer%20compared%20to">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>). Suitable for moderate scale, but not designed for very high QPS vector-only workloads.</td> </tr> <tr> <td><strong>Features</strong></td> <td>Rich features: dynamic index selection, advanced filtering, hybrid search (vector + scalar) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L176%20Milvus%20and,substantial%20boost%20in%20prefiltering%20speed">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>), time-travel consistency, etc. Mature ecosystem and documentation (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1e63m16/vector_database_pgvector_vs_milvus_vs_weaviate/#:~:text=Milvus%20%E2%80%94%E2%80%94%E2%80%94%E2%80%94%20Strengths%3A%20High%20performance%3A,distance%20metrics%2C%20allowing%20for%20customization">Vector database : pgvector vs milvus vs weaviate. : r/LocalLLaMA</a>).</td> <td>Simplicity-focused feature set: HNSW index only (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=match%20at%20L185%20GPU_IVF_PQ,algorithm%20for%20its%20KNN%20search">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>), metadata filtering, persistence to disk. Lacks user authentication or sharding (single-node only) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Conversely%2C%20while%20prioritizing%20simplicity%20and,for%20applications%20with%20increasing%20demands">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>).</td> <td>Leverages SQL features: can do complex filtering/join with vector search in one query. Strong persistence (ACID transactions). Fewer ANN algorithms (flat or IVF). No built-in vector-specific clustering – relies on Postgres scaling.</td> </tr> <tr> <td><strong>Resource Needs</strong></td> <td><em>Lite:</em> runs in-process, memory depends on data (roughly a few hundred bytes per vector plus index). Very low idle footprint. <em>Full:</em> requires etcd and more RAM/CPU (not ideal for laptop). Lite supports ARM (Apple M) natively (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20currently%20supports%20the,following%20environments">milvus-lite · PyPI</a>).</td> <td>Very low overhead. Runs in-process, uses DuckDB/SQLite file. Memory usage proportional to vector count (HNSW graph). CPU usage efficient for moderate data; multi-threaded search.</td> <td>Postgres server will use a constant memory overhead (e.g. 100MB+ even if few data) and CPU even when idle. Optimizing Postgres (shared buffers, etc.) may be needed. On Apple M-series, Postgres runs natively via Homebrew.</td> </tr> <tr> <td><strong>Developer Experience</strong></td> <td>Medium – some learning curve to understand Milvus concepts and API. Once set up, same code scales from Lite to cluster. Good if future growth expected (<a href="https://pypi.org/project/milvus-lite/#:~:text=Milvus%20Lite%20uses%20the%20same,scale%20production">milvus-lite · PyPI</a>).</td> <td>Easiest – designed for developers. Minimal configuration, intuitive API. Great for quick iteration and local debugging.</td> <td>High – Familiar SQL interface. Can use standard Postgres tools (PSQL, DBeaver) to inspect data. But adds SQL overhead for those not used to it. Combines vector search with normal DB operations if needed.</td> </tr> </tbody> </table> <p><strong>Decision</strong>: Given the fully offline, single-user context and anticipated data size, <strong>Milvus Lite</strong> and <strong>Chroma</strong> are the leading candidates. Chroma offers <strong>zero-friction setup</strong> and is likely sufficient if the vault has, say, a few thousand markdown chunks. Milvus Lite offers more headroom and advanced indexing if we push towards hundreds of thousands of vectors or want to experiment with index trade-offs. Both integrate with LangChain easily. If we value simplicity, Chroma might be chosen initially, with an option to migrate to Milvus Lite if needed. </p> <p>We will proceed assuming one of these is used (the architecture doesn’t change significantly between them). For completeness, if a team already uses Postgres in their stack, pgvector could be slotted in, but in a local-first scenario it’s an extra component unless already running.</p> <h2 id=langchain-rag-pipeline-design>LangChain RAG Pipeline Design<a class=headerlink href=#langchain-rag-pipeline-design title="Permanent link">&para;</a></h2> <p>With documents indexed into the vector store, the <strong>query-time pipeline</strong> handles incoming questions by retrieving relevant context and generating answers. We leverage <strong>LangChain’s RAG components</strong> to construct this pipeline in a modular way. The pipeline consists of:</p> <ol> <li><strong>User Query Intake</strong> – A question from the user (via CLI, API, or OpenWebUI chat) is received.</li> <li><strong>Retriever</strong> – The query is embedded and similar documents are fetched from the vector store.</li> <li><strong>(optional) Reranking or Filtering</strong> – The retrieved chunks may be filtered for relevance or diversity (to avoid redundant answers). This can involve dropping low similarity results or using Maximal Marginal Relevance (MMR) to ensure a variety of topics in the top results.</li> <li><strong>LLM Answer Generation</strong> – The question and retrieved context are fed into a local LLM (via Ollama) using a prompt template that encourages using the provided info to answer.</li> <li><strong>Response Formatting</strong> – The LLM’s output is returned to the user. In a chat UI, this would appear as the assistant’s answer, potentially with citations or code blocks as provided by the model.</li> </ol> <p>Let’s detail some of these stages and the LangChain design patterns used:</p> <h3 id=retrieval-step-embedding-query-and-similarity-search>Retrieval Step: Embedding Query and Similarity Search<a class=headerlink href=#retrieval-step-embedding-query-and-similarity-search title="Permanent link">&para;</a></h3> <p>When a user asks a question, we embed the query text using the same embedding model used for documents. This yields a query vector in the same vector space as our document embeddings. We then use the vector database’s k-NN search to find the top <em>k</em> most similar chunks (by cosine similarity or Euclidean distance).</p> <ul> <li>Using LangChain, we encapsulate this in a <strong>Retriever</strong> object. For instance, if using Chroma, <code>Chroma.as_retriever(search_type="mmr", k=5)</code> could be used to get a retriever with MMR, or for Milvus we might use a <code>VectorStoreRetriever</code> with <code>search_k</code> and <code>fetch_k</code> parameters tuned.</li> <li>We typically retrieve a handful of chunks (e.g., 3–5) for the LLM to consider. The exact number can be tuned based on the average size of chunks and the LLM’s context limit. With a modern local LLM (which often have 4K or more tokens context), including ~4 chunks of a few hundred tokens each is reasonable.</li> </ul> <p><strong>Design considerations for retrieval:</strong></p> <ul> <li><strong>Chunk Metadata and Filtering:</strong> Because each chunk has metadata (source file, section, etc.), we can apply filters. For example, if the query includes a tag like “#API”, we could filter to only search documents tagged as API-related. LangChain’s retriever interface supports metadata filters (which underlying stores like Milvus or Chroma apply).</li> <li><strong>Diversity (MMR):</strong> Instead of plain top-k similarity, using Maximal Marginal Relevance can improve the diversity of retrieved contexts. This prevents getting multiple very similar chunks that all say the same thing. LangChain’s retrievers support MMR, which we can enable for broad queries that might span multiple documents.</li> <li><strong>Cross-Encoder Re-ranking:</strong> For maximum accuracy, one could re-rank the top 10–20 retrieved chunks using a cross-encoder (a BERT-based model that scores query–chunk relevance) before picking the final few to pass to the LLM. This is a technique to improve precision at the cost of extra computation. Given we aim for fully local solution, an efficient cross-encoder model like MiniLM or E5 could be used if needed. However, as an initial design, we might skip this and rely on the embedding model’s semantic search quality (especially if using a strong model like mxbai or Stella which are quite accurate).</li> </ul> <h3 id=prompt-construction-and-llm-query>Prompt Construction and LLM Query<a class=headerlink href=#prompt-construction-and-llm-query title="Permanent link">&para;</a></h3> <p>Once we have the relevant text chunks, we construct the prompt for the LLM. A common pattern is the <strong>“Stuffing” approach</strong>: concatenate the retrieved chunks and the question into a single prompt that asks the LLM to answer using the provided context. For example:</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>You are an assistant answering questions about the project’s documentation. 
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>Use the following context to answer the question. If the context does not have the answer, say you don&#39;t know.
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>Context:
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>&lt;&lt;DOC 1 TITLE&gt;&gt;:
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>DOC 1 CONTENT...
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>&lt;&lt;DOC 2 TITLE&gt;&gt;:
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>DOC 2 CONTENT...
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>Question: &lt;&lt;user question&gt;&gt;
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>Answer:
</span></code></pre></div> <p>The exact prompt can be tuned, but we ensure to include citations markers or section titles so that the LLM can refer to them. Some RAG systems wrap each document in special tokens or XML (OpenWebUI uses <code>&lt;context&gt;&lt;/context&gt;</code> tags around knowledge (<a href="https://medium.com/@kelvincampelo/how-ive-optimized-document-interactions-with-open-webui-and-rag-a-comprehensive-guide-65d1221729eb#:~:text=Use%20the%20following%20context%20as,context%3E%20%5Bcontext">How I’ve Optimized Document Interactions with Open WebUI and RAG: A Comprehensive Guide | by Kelvin Campelo | Medium</a>)) – this can help the model distinguish context from question.</p> <p>We also can include a <strong>system message</strong> (if using a chat-model format) that instructs the model on how to behave (e.g., be concise, cite sources). LangChain’s <code>RetrievalQA</code> chain or <code>ConversationalRetrievalChain</code> automates some of this prompt assembly. By 2025, LangChain offers robust support for custom prompt templates in retrieval-augmented QA. We will likely use a custom prompt to ensure the format fits our use (especially if we want markdown output with citations).</p> <p>After forming the prompt, we invoke the <strong>local LLM</strong> through Ollama. This is done via a LangChain LLM wrapper. For example, using the <code>langchain_ollama</code> integration, we initialize a <code>ChatOllama</code> model instance (<a href="https://python.langchain.com/v0.2/docs/tutorials/local_rag/#:~:text=from%20langchain_ollama%20import%20ChatOllama">Build a Local RAG Application | ️ LangChain</a>) pointing to our desired model (like <code>llama3.1:8b</code> or <code>deepseek-r1:Q4_0</code> if available). This wrapper will handle sending the prompt to the Ollama backend (which serves models on <code>localhost:11434</code> by default) (<a href="https://python.langchain.com/v0.2/docs/tutorials/local_rag/#:~:text=,localhost%3A11434">Build a Local RAG Application | ️ LangChain</a>) and receiving the generated answer.</p> <p><strong>Local LLM considerations:</strong></p> <ul> <li><strong>Model Choices:</strong> We plan to support multiple local models. For general queries, Meta’s <strong>LLaMA 3</strong> (the hypothetical next-gen LLaMA, here presumably version 3.1) is a strong foundation model. For specialized needs, <strong>Phi-4</strong> by Microsoft (14B parameters) excels at complex reasoning and math (<a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090#:~:text=Today%20we%20are%20introducing%20Phi,Foundry%20and%20on%20Hugging%20Face">Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub</a>), and <strong>DeepSeek-R1</strong> (which is an open model focusing on reasoning/code, comparable to OpenAI’s top models (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1#:~:text=reasoning%20performance%2C%20we%20introduce%20DeepSeek,art%20results%20for%20dense%20models">deepseek-ai/DeepSeek-R1 · Hugging Face</a>)). Each of these can be obtained as GGUF or similar format and served via Ollama. We can allow the user to choose the model per query or per session in the UI (OpenWebUI makes it easy to switch models mid-chat).</li> <li><strong>Prompt Tuning per Model:</strong> Different models may require different prompt formats. For instance, LLaMA-based models often use a system prompt like <code>&lt;s&gt;[INST] ...</code> unless using a chat wrapper that handles formatting. The LangChain <code>ChatModel</code> abstraction can hide these differences. We will test the prompt on each target model to ensure the formatting is correct. (LangChain provides prompt templates for known families – e.g., it notes inclusion of special tokens might be needed (<a href="https://python.langchain.com/v0.2/docs/tutorials/local_rag/#:~:text=Note%3A%20This%20guide%20uses%20a,an%20example%20for%20LLaMA%202">Build a Local RAG Application | ️ LangChain</a>), but the <code>ChatOllama</code> wrapper likely normalizes this).</li> <li><strong>Context Window:</strong> We ensure the total tokens (documents + question + instructions) fit in the model’s context. Many 2025 models have 4k to 16k token contexts. If using a 8k context model, and our docs are large, we may truncate or retrieve fewer chunks to stay within limits. Alternatively, some RAG setups implement iterative answering: e.g., use a <strong>Refine Chain</strong> where the LLM reads one chunk at a time and builds an answer incrementally (<a href="https://medium.com/decodingml/design-a-rag-langchain-application-leveraging-the-3-pipeline-architecture-46bcc3cb3500#:~:text=RAG%20LangChain%20app%20using%20the,by%20LLMs%20%26%20vector%20DBs">RAG LangChain app using the 3-pipeline design | Decoding ML</a>). This is useful if context window is small. However, given modern models and our moderate chunk sizes, a single-shot prompt with top 3–5 chunks should suffice (“Stuff” method).</li> <li><strong>Computation Performance:</strong> Running a 7B-14B parameter model on a laptop is feasible. On an M2 MacBook, a 7B model (4-bit quantized) can generate ~10 tokens/sec. For a typical answer (~100 tokens) this is under 15 seconds, which is acceptable. Larger models like 30B may be slower (~2-4 tokens/sec), so we might stick to 7B–13B models for responsiveness. If a GPU (like an NVIDIA 3080) is available, FP16 inference can be much faster, making even 30B models viable. The architecture allows swapping the model depending on hardware – e.g., on a desktop with a 24GB GPU, one could run a 34B DeepSeek-R1 for higher quality. On a Macbook with no GPU, one might use LLaMA 3 7B or Phi-4 14B in 4-bit mode.</li> </ul> <p>LangChain’s chain abstraction will tie it together. For example, using a <code>RetrievalQA</code> chain, we set <code>retriever</code> to our vector store retriever and <code>llm</code> to our ChatOllama model. The chain then handles the overall logic: embed query, retrieve docs, format prompt, get LLM answer, and even include source references if our prompt instructs it to.</p> <h3 id=end-to-end-query-flow>End-to-End Query Flow<a class=headerlink href=#end-to-end-query-flow title="Permanent link">&para;</a></h3> <p>Putting it all together, the <strong>end-to-end flow</strong> for a query is:</p> <ol> <li><strong>User</strong> (e.g. via OpenWebUI chat or a CLI) asks: “How do I set up the API client in this project?”</li> <li><strong>RAG Pipeline</strong>:</li> <li>The query is embedded by (for example) mxbai-embed-large, yielding a 1024-d vector.</li> <li>Vector DB (Milvus/Chroma) is queried (k=5, MMR reordering) and returns, say, 3 relevant text chunks from <code>API_Guide.md</code> and <code>Quickstart.md</code>.</li> <li>The pipeline prepares a prompt: <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>[SYSTEM] You are a helpful assistant for the project docs.
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>[CONTEXT] 
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>API_Guide.md:
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>&quot;... snippet about API client initialization...&quot;
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>Quickstart.md:
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a>&quot;... snippet showing an example usage of the API client...&quot;
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>[USER] How do I set up the API client in this project?
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>[ASSISTANT]
</span></code></pre></div></li> <li>The local LLM (Phi-4, for instance) receives this prompt and generates an answer, e.g. explaining the steps to set up the client, possibly quoting code from the context. It might produce an answer with citations like “(see API_Guide.md)”.</li> <li><strong>User receives answer.</strong> The answer is accurate and based on local docs, with no external calls made.</li> </ol> <p>Throughout this, <strong>LangChain’s framework</strong> provides observability and modularity. We can log the retrieved docs, model’s output, etc., to a file for debugging (LangChain callbacks or tracing can be used, though we’d use them offline, e.g., LangSmith locally if needed).</p> <p>If the user asks a follow-up question in a chat, we can reuse the chain. If we want to support <strong>conversational memory</strong> (so the assistant remembers previous Q&amp;A), LangChain’s <code>ConversationalRetrievalChain</code> can maintain chat history context. This essentially treats prior conversation as additional context (or uses a summary of it). OpenWebUI by default shows the entire conversation to the model, so another strategy is to let the UI handle the history and our pipeline just augment the latest question. In practice, for multi-turn Q&amp;A about documents, a common approach is to combine the last user question with a summary of relevant past info to retrieve again. This is an area for extension, but not core to the initial design.</p> <h2 id=integration-with-openwebui>Integration with OpenWebUI<a class=headerlink href=#integration-with-openwebui title="Permanent link">&para;</a></h2> <p><strong>OpenWebUI</strong> is a popular self-hosted chat interface that supports multiple LLM backends (including Ollama) and features like knowledge base RAG integration (<a href="https://www.both.org/?p=8282#:~:text=OpenWebUI%20offers%20a%20robust%2C%20feature,thirty%20developers%20working%20on%20it">Open WebUI: A Powerful, Open Source Interface for LLM – Both.org</a>). Integrating our RAG system with OpenWebUI can provide a rich UI for the user without custom frontend coding. There are two ways to integrate:</p> <ol> <li><strong>Use OpenWebUI’s Built-in RAG (Knowledge Bases):</strong> OpenWebUI allows creating “Knowledge Bases” by uploading documents (it has a UI for adding Markdown/PDFs, etc.) (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=1"> Open WebUI RAG Tutorial | Open WebUI</a>) (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=,Create%20a%20Knowledge%20Base"> Open WebUI RAG Tutorial | Open WebUI</a>). Internally, OpenWebUI will embed these (it likely uses HuggingFace sentence transformers or similar, possibly ones we specify) and store them (possibly in a local SQLite or DuckDB with FAISS index). A custom model can then be created in OpenWebUI that attaches this knowledge base (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=match%20at%20L125%20Create%20a,Model%20with%20the%20Knowledge%20Base"> Open WebUI RAG Tutorial | Open WebUI</a>). When the user asks a question to that model, OpenWebUI will handle retrieval and will pass the context to the model automatically. Essentially, it implements a RAG pipeline under the hood. This approach means we could <strong>bypass our LangChain pipeline for query time</strong> and let OpenWebUI do it. However, its ingestion might not be dynamic (you’d have to re-upload or trigger a re-index manually when docs change), and we have less control over embedding models or retrieval settings.</li> <li><strong>Custom Pipeline Integration:</strong> We can instead connect our LangChain pipeline to OpenWebUI by treating it as a <strong>tool or API</strong>. For example, OpenWebUI supports an OpenAI-compatible REST API mode and even pipelines. We could run a local FastAPI server that exposes an endpoint: when a request comes in (with the user query), our server uses the LangChain RAG chain to produce an answer (with citations) and returns it. Then configure OpenWebUI to use that as a “model” (it would treat it like an OpenAI chat completion API). In essence, OpenWebUI becomes just the frontend, and our LangChain pipeline is the backend answering. This gives us full control over retrieval settings, embedding model choice, etc., at the cost of a bit more setup (running a separate API server).</li> </ol> <p>Given that OpenWebUI now has official <strong>RAG support with knowledge bases</strong> (and tutorials for it (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=In%20this%20tutorial%2C%20you%20will,an%20example%20for%20this%20setup"> Open WebUI RAG Tutorial | Open WebUI</a>) (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=Step,as%20Knowledge%20Base"> Open WebUI RAG Tutorial | Open WebUI</a>)), we might leverage it for simplicity. We could periodically push our vault documents into OpenWebUI’s knowledge base via its CLI or API (perhaps there’s an API to add docs, or we use the UI occasionally to sync). But for a seamless “hot reload” of docs, a custom integration might be superior.</p> <p>OpenWebUI’s advantage is the user experience: it supports chat history, Markdown rendering (important for code snippets in answers), and multi-model selection easily (<a href="https://www.both.org/?p=8282#:~:text=The%20documentation%20states%20that%20one,nuances%20of%20this%20amazing%20software">Open WebUI: A Powerful, Open Source Interface for LLM – Both.org</a>) (<a href="https://davidmac.pro/posts/2024-11-15-ai-start-ollama-openwebui/#:~:text=Start%20chatting">Dave does AI #1 - Self-hosted AI using Ollama + Open WebUI</a>). It’s also offline and lightweight (web app running on localhost). Since our focus is on architecture, we ensure that <strong>the core RAG system is decoupled</strong> – it can function via command line or tests without OpenWebUI. The UI integration is an added layer. </p> <p><strong>Plan for integration:</strong> Start with our LangChain pipeline accessible through a simple interface (could be a CLI or small Flask app). Then, if using OpenWebUI, create a custom “tool” or model in OpenWebUI that queries this interface. For instance, define a pseudo-model in OpenWebUI that on each user message calls our API to get a response (this might be achieved through OpenWebUI’s “OpenAPI Tool Servers” feature (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=,24"> Open WebUI RAG Tutorial | Open WebUI</a>) or a pipeline script). Another approach is to use OpenWebUI’s new <strong>Pipelines</strong> functionality (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text="> Open WebUI RAG Tutorial | Open WebUI</a>), where you might be able to insert a custom Python function in the generation pipeline. If possible, we could insert a hook that intercepts the user message, runs our retrieval, and prepends the context to the prompt, then continues to the LLM. Community discussions (e.g., Reddit posts about RAG in OpenWebUI) indicate users have successfully connected custom knowledge sources (<a href="https://www.reddit.com/r/OpenWebUI/comments/1ir8yl4/focused_retrieval_on_knowledge_documents/#:~:text=Reddit%20www,embedded%20into%20a%20vector%20db">Focused Retrieval on Knowledge Documents : r/OpenWebUI - Reddit</a>) (<a href="https://medium.com/@kelvincampelo/how-ive-optimized-document-interactions-with-open-webui-and-rag-a-comprehensive-guide-65d1221729eb#:~:text=RAG%20medium,ChatGPT%20to%20ask%20about%20documents">How I've Optimized Document Interactions with Open WebUI and RAG</a>).</p> <p>In summary, the integration will allow a user to open a browser to OpenWebUI, select (for example) a model called “LocalDocs-GPT”, and chat with it. The answers will come from the LangChain RAG backend, but to the user it feels like a normal chat with an AI assistant that “knows” their documents. This meets the goal of a user-friendly, offline QA system for the developer’s documentation.</p> <h2 id=performance-and-resource-considerations>Performance and Resource Considerations<a class=headerlink href=#performance-and-resource-considerations title="Permanent link">&para;</a></h2> <p>Designing for <strong>developer hardware</strong> means we must be mindful of CPU/GPU usage, memory, and responsiveness:</p> <ul> <li><strong>Embedding Throughput:</strong> Using a large embedding model (300M+ params) to index potentially thousands of chunks can be time-consuming. We mitigate this by doing initial ingestion in batch (which is one-time and can be done when the vault is first indexed). For ongoing updates, the volume is presumably low (commits of a few docs at a time). We can further speed up embedding by using batch inference (embedding multiple texts at once if the model and library support it) or by switching to a slightly smaller model if necessary (e.g. the <strong>all-MiniLM</strong> models are very fast but at some accuracy cost). In practice, mxbai-embed-large is reported to be quite efficient for its size, and on an M1 chip it might take ~50-100 milliseconds per chunk. This is acceptable for &lt;=10k chunks (a few minutes total). If the vault is extremely large (say hundreds of MBs of text), one might do initial embedding on a more powerful machine or overnight. But since the use-case is developer notes/docs, we anticipate manageable data sizes.</li> <li><strong>Vector DB Memory:</strong> We will configure the vector store index to balance memory and speed. HNSW (used by both Milvus and Chroma by default) has a controllable memory footprint (via M and ef parameters). We might use a slightly lower M (graph connectivity) if memory is tight, at the cost of some recall drop. That said, for a few thousand vectors, the memory use is trivial. For 100k vectors, HNSW might use a few hundred MB of RAM. Milvus Lite storing data to disk (<code>milvus_demo.db</code> file) ensures persistence without consuming RAM for all vectors at once (it likely memory-maps data as needed). Chroma uses DuckDB which will spill to disk as well. So memory should not be a limiting factor. On a 16GB RAM laptop, dedicating even 1-2GB to the vector index is fine.</li> <li><strong>LLM Model Memory/VRAM:</strong> Running a 7B parameter model in 4-bit quantization uses about ~4GB of RAM (or VRAM). A 13-14B model uses ~8GB in 4-bit. This is within reach for an 8GB VRAM GPU or 16GB system RAM (with swap possibly for Mac). For better performance on Mac, using the GPU via Metal acceleration is possible with smaller models (CoreML versions of LLaMA 2 exist; by 2025 possibly LLaMA 3 too). If using an NVIDIA GPU, we can load the model in VRAM fully for faster inference. The design allows configuring the model size/precision to fit the machine. We will provide instructions for using 4-bit quantized models via Ollama for those on CPU-only systems. Ollama itself handles model loading and can use Apple’s neural engine for acceleration where possible. </li> <li><strong>Multi-threading and Concurrency:</strong> Since this is a single-user setup, we don’t expect concurrent queries. If multiple queries did happen, both the vector DB and LLM could become bottlenecks. Milvus/Chroma can handle concurrent searches well on multiple threads, but a single LLM on one machine can usually do one response at a time (unless running multiple model instances). If in future one wanted a multi-user setup, they might need to run separate processes or threads for the LLM calls (and ensure the hardware can handle it). Our focus is single-user, which simplifies things.</li> <li> <p><strong>Latency:</strong> The steps in the pipeline (embedding the query, vector search, LLM generation) all add some latency. Embedding the query is fast (~20ms). Vector search in a small index is ~10ms. The LLM generation is the dominant cost (a few seconds). End-to-end latency is thus mostly the LLM’s doing. We aim to keep that reasonable by choosing models and context sizes appropriately. If a user asks extremely long questions or if the retrieved context is very large, generation will be slower (more tokens to process). We will document best practices: e.g., if a user asks a extremely detailed multi-part question, it might be better to break it down. But generally, expect answers within 5–15 seconds on typical hardware, which is acceptable for an interactive assistant.</p> </li> <li> <p><strong>Accuracy vs Model Size:</strong> If the smaller local models sometimes falter in reasoning or correctness, one could swap in a bigger model (like DeepSeek-R1 distilled 32B) for complex questions. The modular design (using Ollama and LangChain) makes this a configuration choice rather than an architectural one. It’s worth noting that open models like DeepSeek-R1 and Phi-4 are closing the gap with larger proprietary models (<a href="https://medium.com/data-science-in-your-pocket/deepseek-r1-best-open-source-reasoning-llm-outperforms-openai-o1-b79869392945#:~:text=DeepSeek,o1%20across%20key%20benchmarks">DeepSeek-R1: Best Open-Source Reasoning LLM Outperforms ...</a>) (<a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090#:~:text=Phi,frontier%20of%20size%20vs%20quality">Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub</a>), so we anticipate high-quality answers, especially since the model can directly look up the truth from documentation (which mitigates knowledge gaps and hallucinations).</p> </li> </ul> <h2 id=implementation-references>Implementation References<a class=headerlink href=#implementation-references title="Permanent link">&para;</a></h2> <p>To ensure this architecture is implementable with today’s tools, refer to these resources and examples: - <strong>LangChain Local RAG Tutorial (2024):</strong> Demonstrates running LLaMA 3 locally with Ollama and a local embedding model (<a href="https://python.langchain.com/v0.2/docs/tutorials/local_rag/#:~:text=This%20guide%20will%20show%20how,208%20if%20you%20prefer">Build a Local RAG Application | ️ LangChain</a>) (<a href="https://python.langchain.com/v0.2/docs/tutorials/local_rag/#:~:text=,localhost%3A11434">Build a Local RAG Application | ️ LangChain</a>). It provides a blueprint for setting up the Ollama backend and retrieving from a vector store. - <strong>Milvus Lite RAG Example:</strong> Milvus documentation and examples show how to use Milvus Lite within a Python app for RAG (<a href="https://pypi.org/project/milvus-lite/#:~:text=Examples">milvus-lite · PyPI</a>). A specific example is Milvus’s bootcamp demo “build_RAG_with_milvus.ipynb” on GitHub, integrating with LangChain. - <strong>OpenWebUI RAG Tutorial:</strong> The community-contributed tutorial on OpenWebUI’s site (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=In%20this%20tutorial%2C%20you%20will,an%20example%20for%20this%20setup"> Open WebUI RAG Tutorial | Open WebUI</a>) (<a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial/#:~:text=Step,as%20Knowledge%20Base"> Open WebUI RAG Tutorial | Open WebUI</a>) is a step-by-step guide to load a set of Markdown files as a knowledge base and query them in the UI. This can be used as a starting point to configure our system in OpenWebUI if we choose that route. - <strong>Ollama Documentation:</strong> For installing and managing models with Ollama – e.g., how to quantize models, the command to pull specific versions, etc. (Ollama’s model catalog shows available models like mxbai-embed-large and others (<a href="https://ollama.com/search?c=embedding#:~:text=%2A%20%20nomic,9K%20Pulls%2016%20Tags">Embedding models · Ollama Search</a>)). - <strong>Mixedbread’s Embedding Blog:</strong> Mixedbread’s blog post “Open Source Strikes Bread – New Fluffy Embedding Model” (referenced on their site (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=API%20Reference%20EmbeddingsModel%20Reference%20mxbai,New%20Fluffy%20Embedding%20Model">mxbai-embed-large-v1 - Mixedbread</a>)) likely details the performance of mxbai-embed-large and how to best use it (e.g., prompting techniques for embeddings, as hinted by the use of an optional prompt for domain (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=Adding%20a%20domain,the%20embedding%20will%20be%20used">mxbai-embed-large-v1 - Mixedbread</a>)). - <strong>Microsoft Phi-4 Technical Report:</strong> Provides insight into the capabilities of Phi-4 14B, which might guide how to leverage its strengths (especially if the docs have math or require step-by-step reasoning) (<a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090#:~:text=Today%20we%20are%20introducing%20Phi,Foundry%20and%20on%20Hugging%20Face">Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub</a>) (<a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090#:~:text=Phi,frontier%20of%20size%20vs%20quality">Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub</a>). - <strong>DeepSeek-R1 Paper:</strong> Describes the reasoning prowess of DeepSeek-R1 (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1#:~:text=reasoning%20performance%2C%20we%20introduce%20DeepSeek,art%20results%20for%20dense%20models">deepseek-ai/DeepSeek-R1 · Hugging Face</a>). It also mentions distilled variants based on Llama and Qwen (some of which might be easier to run locally). These references help justify using these models and can guide fine-tuning or prompting if we ever refine the LLM on our domain.</p> <p>By following this design and utilizing the mentioned tools, a developer can implement a <strong>robust RAG system</strong> that runs entirely locally, scales to their needs, and provides quick, accurate answers from their Markdown knowledge base. This empowers an “offline ChatGPT” experience tailored to one’s own documentation – preserving privacy and leveraging the latest open-source AI advances (circa 2025).</p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h2> <p>The proposed architecture combines <strong>modern embedding models</strong>, a <strong>high-performance local vector store</strong>, and <strong>powerful local LLMs</strong> to achieve a self-contained RAG setup. We prioritized components that are <strong>community-maintained and cutting-edge</strong> in 2024–2025, ensuring the system remains relevant and high-quality: - <em>Dynamic ingestion</em> keeps the knowledge updated from a Git vault in real-time. - <em>State-of-the-art embeddings</em> (mxbai-embed-large or its successors) ensure the retrieval step is semantically accurate (<a href="https://www.mixedbread.com/docs/embeddings/mxbai-embed-large-v1#:~:text=mxbai,002">mxbai-embed-large-v1 - Mixedbread</a>). - <em>Milvus Lite/Chroma</em> offer fast similarity search on device, with Milvus providing scalability if needed (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=Chroma%20vector%20database%20is%20a,latency%20applications">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>) (<a href="https://zilliz.com/blog/milvus-vs-chroma#:~:text=needs,or%20even%20trillions%20of%20vector">Comparing Vector Databases: Milvus vs. Chroma DB - Zilliz blog</a>). - <em>Local LLMs via Ollama</em> provide the brains for answering questions, with options like LLaMA 3, Phi-4, and DeepSeek-R1 pushing the envelope of what’s possible without any cloud services (<a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090#:~:text=Today%20we%20are%20introducing%20Phi,Foundry%20and%20on%20Hugging%20Face">Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning | Microsoft Community Hub</a>) (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1#:~:text=reasoning%20performance%2C%20we%20introduce%20DeepSeek,art%20results%20for%20dense%20models">deepseek-ai/DeepSeek-R1 · Hugging Face</a>). - <em>Integration with OpenWebUI</em> delivers a polished user interface, showing that local AI assistants can be both powerful and user-friendly (<a href="https://www.both.org/?p=8282#:~:text=OpenWebUI%20offers%20a%20robust%2C%20feature,thirty%20developers%20working%20on%20it">Open WebUI: A Powerful, Open Source Interface for LLM – Both.org</a>).</p> <p>In essence, this design enables developers to <strong>harness their private documentation with AI assistance</strong> entirely offline. It aligns with the growing trend of privacy-preserving, local AI deployments and leverages community best practices (LangChain patterns, open models, vector DB benchmarks) to ensure it’s both <strong>practical and cutting-edge</strong>. With this system in place, a developer could query “How do I deploy our app on Kubernetes?” and get an immediate, accurate answer sourced from their own docs – all without an internet connection. Such capability underscores the potential of retrieval-augmented generation when thoughtfully applied in a local-first context. </p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="April 11, 2025 05:21:57">April 11, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="April 11, 2025 05:21:57">April 11, 2025</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../architecture-draft/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Architecture Design"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Architecture Design </div> </div> </a> <a href=../vector-database/ class="md-footer__link md-footer__link--next" aria-label="Next: Vector Database"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Vector Database </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> </div> <div class=md-social> <a href=https://github.com/usrbinkat target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to analyze traffic and understand where our visitors are coming from.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">Accept</button> <label class=md-button for=__settings>Manage settings</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.footer", "navigation.path", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "search.share", "content.tabs.link", "content.code.annotation", "content.code.copy", "content.action.edit", "content.action.view", "header.autohide"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "0.1.0", "provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../javascripts/extra.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>
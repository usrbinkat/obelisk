# Migration Reference: ChromaDB → Milvus + LiteLLM

> **Version**: 0.1.0-alpha  
> **Migration Date**: January 2025  
> **Status**: ✅ Complete

## Quick Reference: Key Changes

### Removed Components
- **ChromaDB**: All references removed
- **Direct Ollama API calls**: Now routed through LiteLLM
- **`/v1/litellm` endpoint**: Unified under `/v1/chat/completions`
- **`CHROMA_*` environment variables**: No longer used

### Added Components
- **Milvus vector database**: Production-ready vector storage
- **etcd**: Milvus metadata storage
- **MinIO**: Milvus object storage
- **init-service container**: Token generation and model registration
- **LiteLLM proxy**: Unified LLM API
- **Provider factory pattern**: Clean abstraction layer

### Changed Components

| Component | Before | After |
|-----------|--------|-------|
| Vector DB | ChromaDB | Milvus 2.5.10 |
| Embeddings | mxbai-embed-large (1024 dims) | text-embedding-3-large (3072 dims) |
| LLM API | Direct Ollama calls | LiteLLM proxy (unified) |
| API Endpoint | Multiple endpoints | Single `/v1/chat/completions` |
| Authentication | None/Basic | Bearer tokens (sk-prefixed) |
| Provider Selection | Hardcoded | Factory pattern with override |

## Configuration Changes

### Old Configuration
```bash
# ChromaDB
CHROMA_HOST=chromadb
CHROMA_PORT=8000
CHROMA_COLLECTION=obelisk_docs

# Direct Ollama
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=llama3
```

### New Configuration
```bash
# Milvus
MILVUS_HOST=milvus
MILVUS_PORT=19530
MILVUS_COLLECTION=obelisk_rag

# LiteLLM
LITELLM_API_BASE=http://litellm:4000
LITELLM_API_KEY=<generated-token>
MODEL_PROVIDER=litellm
FORCE_LITELLM_PROXY=true

# Models
EMBEDDING_MODEL=text-embedding-3-large
LLM_MODEL=gpt-4o
```

## API Changes

### Before: Multiple Endpoints
```bash
# Direct Ollama
POST /api/generate

# Separate LiteLLM  
POST /v1/litellm/completions

# OpenAI compatibility
POST /v1/chat/completions
```

### After: Unified Endpoint
```bash
# All requests go here
POST /v1/chat/completions

# Provider override via header
X-Provider-Override: ollama  # Optional, for hardware tuning
```

## Docker Service Changes

### Added Services
```yaml
etcd:
  image: bitnami/etcd:3.5.21
  # Milvus metadata storage

minio:
  image: minio/minio:latest
  # Milvus object storage

milvus:
  image: milvusdb/milvus:v2.5.10
  depends_on:
    - etcd
    - minio

litellm_db:
  image: postgres:latest
  # LiteLLM configuration storage

litellm:
  image: ghcr.io/berriai/litellm:latest
  depends_on:
    - litellm_db
    - ollama

init-service:
  build: ./deployments/docker/images/init
  restart: "no"  # Runs once
  # Generates tokens and registers models
```

### Service Startup Order
1. **Infrastructure**: etcd, minio, litellm_db, ollama, tika
2. **Vector Database**: milvus (depends on etcd, minio)
3. **LLM Proxy**: litellm (depends on litellm_db, ollama)
4. **Initialization**: init-service (configures everything)
5. **Applications**: obelisk, obelisk-rag
6. **Frontend**: open-webui

## Code Migration Examples

### Vector Storage

**Before (ChromaDB)**:
```python
from langchain_chroma import Chroma

self.vector_store = Chroma(
    collection_name=config.chroma_collection,
    embedding_function=self.embedding_function,
    persist_directory=config.chroma_persist_directory
)
```

**After (Milvus)**:
```python
from pymilvus import connections, Collection, CollectionSchema

connections.connect(
    alias="default",
    host=self.config.milvus_host,
    port=self.config.milvus_port
)

self.collection = Collection(
    name=config.milvus_collection,
    schema=schema,
    consistency_level="Strong"
)
```

### LLM Calls

**Before (Direct Ollama)**:
```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(
    base_url=config.ollama_url,
    model=config.ollama_model
)
response = llm.invoke(prompt)
```

**After (Provider Factory)**:
```python
from src.obelisk.rag.common.providers import ProviderFactory, ProviderType

provider = ProviderFactory.create(
    ProviderType.LITELLM,
    config
)
response = await provider.completion(request)
```

## Authentication Changes

### Token Generation
```bash
# Tokens are auto-generated by init-service
docker logs init-service | grep "Generated API token"

# Or retrieve from container
docker exec litellm cat /app/tokens/api_tokens.env | grep LITELLM_API_TOKEN
```

### Using Tokens
```bash
# Set token
export LITELLM_TOKEN=$(docker compose exec -T litellm grep LITELLM_API_TOKEN /app/tokens/api_tokens.env | cut -d= -f2)

# Use in requests
curl -H "Authorization: Bearer $LITELLM_TOKEN" ...
```

## Performance Improvements

### Embedding Quality
- **Before**: 1024 dimensions (mxbai-embed-large)
- **After**: 3072 dimensions (text-embedding-3-large)
- **Result**: Better semantic matching

### Vector Search
- **Before**: ChromaDB default indexing
- **After**: HNSW index (M=16, efConstruction=256)
- **Result**: Faster similarity search at scale

### API Latency
- **Before**: Direct calls to various endpoints
- **After**: Unified proxy with retry logic
- **Result**: Better reliability and fallback handling

## Breaking Changes

1. **No backward compatibility** - This is greenfield development
2. **All ChromaDB data must be re-indexed** in Milvus
3. **API endpoints changed** - Update all client code
4. **Authentication required** - Add Bearer tokens
5. **Environment variables renamed** - Update .env files

## Rollback Plan

Not applicable - this is greenfield development with no existing users.

## Validation Commands

```bash
# Verify no ChromaDB references
grep -r "ChromaDB\|chroma" src/ --include="*.py" | wc -l  # Should be 0

# Check Milvus is working
curl http://localhost:19530/v1/vector/collections

# Verify LiteLLM routing
curl -H "Authorization: Bearer $LITELLM_TOKEN" \
  http://localhost:4000/v1/models

# Test unified endpoint
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Authorization: Bearer $LITELLM_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o", "messages": [{"role": "user", "content": "test"}]}'
```